<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep-Learning on Luke Salamone&#39;s Blog</title>
    <link>https://lukesalamone.github.io/tags/deep-learning/</link>
    <description>Recent content in Deep-Learning on Luke Salamone&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 06 Jun 2024 16:30:27 -0700</lastBuildDate>
    <atom:link href="https://lukesalamone.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What are Sparse Autoencoders?</title>
      <link>https://lukesalamone.github.io/posts/sparse-autoencoder/</link>
      <pubDate>Thu, 06 Jun 2024 16:30:27 -0700</pubDate>
      <guid>https://lukesalamone.github.io/posts/sparse-autoencoder/</guid>
      <description>&lt;script type=&#34;text/javascript&#34; async src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;&lt;/script&gt;&#xA;&lt;script type=&#34;text/x-mathjax-config&#34;&gt;&#xA;MathJax.Hub.Config({&#xA;  tex2jax: {&#xA;    inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],&#xA;    displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\[&#39;,&#39;\]&#39;]],&#xA;    processEscapes: true,&#xA;    processEnvironments: true,&#xA;    skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;],&#xA;    TeX: {&#xA;      equationNumbers: {&#xA;        autoNumber: &#34;AMS&#34;&#xA;      },&#xA;      extensions: [&#34;AMSmath.js&#34;, &#34;AMSsymbols.js&#34;]&#xA;    }&#xA;  }&#xA;});&#xA;&lt;/script&gt;&#xA;&lt;script type=&#34;text/x-mathjax-config&#34;&gt;&#xA;  MathJax.Hub.Queue(function() {&#xA;    // Fix &lt;code&gt; tags after MathJax finishes running. This is a&#xA;    // hack to overcome a shortcoming of Markdown. Discussion at&#xA;    // https://github.com/mojombo/jekyll/issues/199&#xA;    var all = MathJax.Hub.getAllJax(), i;&#xA;    for(i = 0; i &lt; all.length; i += 1) {&#xA;        all[i].SourceElement().parentNode.className += &#39; has-jax&#39;;&#xA;    }&#xA;});&#xA;&lt;/script&gt;&#xA;&lt;p&gt;&lt;strong&gt;TLDR: A sparse autoencoder is just a regular autoencoder that encourages sparsity with an L1 penalty or KL divergence loss rather than using a low-dimensional bottleneck.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Summary: Deep &amp; Cross Net v2</title>
      <link>https://lukesalamone.github.io/posts/deep-cross-net-v2/</link>
      <pubDate>Mon, 02 Oct 2023 12:39:18 -0700</pubDate>
      <guid>https://lukesalamone.github.io/posts/deep-cross-net-v2/</guid>
      <description>&lt;p&gt;Paper link: &lt;a href=&#34;https://arxiv.org/pdf/2008.13535&#34;&gt;https://arxiv.org/pdf/2008.13535&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Learning to rank is an important problem in many machine-learning products such as search, recommendation, and advertising. Originally, many machine learning systems used simple logistic regression models, but it quickly became apparent that combining two or more features together was &lt;a href=&#34;https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf&#34;&gt;even better&lt;/a&gt;. This is called feature crossing.&lt;/p&gt;&#xA;&lt;p&gt;A lot of research and engineering work has gone into learning useful feature crosses. The fundamental problem is that although higher-order feature crosses can be more informative, they are also more sparse, and the number of high order features grows exponentially. Some attempts to address this have been:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
