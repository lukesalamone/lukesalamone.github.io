<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep-Learning on Luke Salamone&#39;s Blog</title>
    <link>https://lukesalamone.github.io/tags/deep-learning/</link>
    <description>Recent content in Deep-Learning on Luke Salamone&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 06 Jun 2024 16:30:27 -0700</lastBuildDate>
    <atom:link href="https://lukesalamone.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What are Sparse Autoencoders?</title>
      <link>https://lukesalamone.github.io/posts/sparse-autoencoder/</link>
      <pubDate>Thu, 06 Jun 2024 16:30:27 -0700</pubDate>
      <guid>https://lukesalamone.github.io/posts/sparse-autoencoder/</guid>
      <description>TLDR: A sparse autoencoder is just a regular autoencoder that encourages sparsity with an L1 penalty or KL divergence loss rather than using a low-dimensional bottleneck.&#xA;If you understood all of those words above, you may be interested in the OpenAI paper which used sparse autoencoders to interpret features from GPT-4.&#xA;If not, I&amp;rsquo;ll try to break it down.&#xA;What is an autoencoder? An autoencoder is a machine learning architecture which contains two functions: an encoder and a decoder.</description>
    </item>
    <item>
      <title>Summary: Deep &amp; Cross Net v2</title>
      <link>https://lukesalamone.github.io/posts/deep-cross-net-v2/</link>
      <pubDate>Mon, 02 Oct 2023 12:39:18 -0700</pubDate>
      <guid>https://lukesalamone.github.io/posts/deep-cross-net-v2/</guid>
      <description>Paper link: https://arxiv.org/pdf/2008.13535&#xA;Learning to rank is an important problem in many machine-learning products such as search, recommendation, and advertising. Originally, many machine learning systems used simple logistic regression models, but it quickly became apparent that combining two or more features together was even better. This is called feature crossing.&#xA;A lot of research and engineering work has gone into learning useful feature crosses. The fundamental problem is that although higher-order feature crosses can be more informative, they are also more sparse, and the number of high order features grows exponentially.</description>
    </item>
  </channel>
</rss>
