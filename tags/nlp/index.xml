<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nlp on Luke Salamone&#39;s Blog</title>
    <link>https://lukesalamone.github.io/tags/nlp/</link>
    <description>Recent content in Nlp on Luke Salamone&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 09 Nov 2022 02:42:51 -0700</lastBuildDate>
    <atom:link href="https://lukesalamone.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Rolling My Own Blog Search</title>
      <link>https://lukesalamone.github.io/posts/rolling-my-own-blog-search/</link>
      <pubDate>Wed, 09 Nov 2022 02:42:51 -0700</pubDate>
      <guid>https://lukesalamone.github.io/posts/rolling-my-own-blog-search/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve found myself hitting ctrl+f on this blog enough that I figured it&amp;rsquo;s about time to add some search functionality to it. While there are certainly prefab solutions out there, this task is simple enough and fairly instructive. I had a few requirements, though:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The search needs to be fast, useful, and aesthetically pleasing.&lt;/li&gt;&#xA;&lt;li&gt;Search in the browser. Standing up a server is a lot of extra work. It&amp;rsquo;s also overkill since I only have about 30 articles so far.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;semantic-search&#34;&gt;Semantic search&lt;/h2&gt;&#xA;&lt;p&gt;I did some experiments with small neural networks deployed using ONNX but ultimately they didn&amp;rsquo;t seem to be a good fit for this blog. The search experience was not quite as snappy as I&amp;rsquo;d have liked it to be, and while I was able to get the model under 10MB, it still added a good amount of bloat to the page size. Further, it wasn&amp;rsquo;t clear to me that the search results were significantly better, and in some cases they were worse. In any case, the advantages were not enough to justify the added page size.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Paper Summary: COMET (Knowledge Graph Construction)</title>
      <link>https://lukesalamone.github.io/posts/knowledge-graph-construction/</link>
      <pubDate>Tue, 17 May 2022 17:47:25 +0700</pubDate>
      <guid>https://lukesalamone.github.io/posts/knowledge-graph-construction/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;../../img/comet_knowledge_generation.png&#34;&#xA;    alt=&#34;Selected {subject, relation, object} tuples generated by COMET&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;p&gt;Selected {subject, relation, object} tuples generated by COMET&lt;/p&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;Paper link: &lt;a href=&#34;https://arxiv.org/abs/1906.05317&#34;&gt;https://arxiv.org/abs/1906.05317&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;This paper describes COMET, a method of generating knowledge bases automatically. Previous work largely focused on encyclopedic knowledge, which has well-defined relationships. This paper, however, focuses on commonsense knowledge. In this paper the authors introduce a ‚Äúcommonsense transformer‚Äù which trains on a knowledge base consisting of tuples and a pre-trained language model. Their trained model generates new nodes in the knowledge graph and completes phrases based on edges in the existing graph.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Paper Summary: Defending Against Neural Fake News</title>
      <link>https://lukesalamone.github.io/posts/grover-paper-summary/</link>
      <pubDate>Sun, 19 Sep 2021 20:13:09 -0500</pubDate>
      <guid>https://lukesalamone.github.io/posts/grover-paper-summary/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.12616&#34;&gt;&lt;em&gt;Defending Against Neural Fake News&lt;/em&gt;&lt;/a&gt; by Zellers et al. presents a model for controllable text generation called Grover. This model can be used to create highly believable computer-generated news articles. The authors present this paper as a method of detecting and preventing the spread of fake news. They claim their model is 92% accurate at detecting fake news stories, partially due to artifacts that generators include in the generated text.&lt;/p&gt;</description>
    </item>
    <item>
      <title>BERT vs GPT-2 Performance</title>
      <link>https://lukesalamone.github.io/posts/bert-vs-gpt2/</link>
      <pubDate>Mon, 21 Jun 2021 01:04:42 -0500</pubDate>
      <guid>https://lukesalamone.github.io/posts/bert-vs-gpt2/</guid>
      <description>&lt;p&gt;There are quite a few BERT vs GPT-2 breakdowns online, mostly focusing on the architectural differences between the two models. However, I am more interested in the performance differences between the two models, specifically their predictive capabilities. This blog post outlines the results of my experiments.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lukesalamone/gpt2-vs-bert&#34;&gt;The code used in this experiment can be found on my Github&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;bert&#34;&gt;BERT&lt;/h2&gt;&#xA;&lt;p&gt;The &lt;a href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;&gt;Devlin et al. model&lt;/a&gt; was released in November 2018. It is a transformer-based language model pretrained on masked input (also known as the &lt;em&gt;cloze&lt;/em&gt; task). During pretraining, 15% of tokens are hidden from the model, and it is trained to predict the masked tokens. As a result, I was able to evaluate its ability to correctly predict a masked token at a random position in a fixed-size input.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How does GPT-2 Tokenize Text?</title>
      <link>https://lukesalamone.github.io/posts/gpt2-tokenization/</link>
      <pubDate>Thu, 17 Jun 2021 19:30:48 -0500</pubDate>
      <guid>https://lukesalamone.github.io/posts/gpt2-tokenization/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s explore how GPT-2 tokenizes text.&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-is-tokenization&#34;&gt;What is tokenization?&lt;/h2&gt;&#xA;&lt;p&gt;It&amp;rsquo;s important to understand that GPT-2 doesn&amp;rsquo;t work with strings directly. Instead, it needs to tokenize the input string, which is essentially a process for converting the string into a list of numbers, or &amp;ldquo;tokens&amp;rdquo;. It is these tokens which are passed into the model during training or for inference. As a concrete example, let&amp;rsquo;s look at a few sample sentences:&lt;/p&gt;</description>
    </item>
    <item>
      <title>What Are Attention Masks?</title>
      <link>https://lukesalamone.github.io/posts/what-are-attention-masks/</link>
      <pubDate>Tue, 15 Jun 2021 19:09:36 -0500</pubDate>
      <guid>https://lukesalamone.github.io/posts/what-are-attention-masks/</guid>
      <description>&lt;p&gt;TLDR: Attention masks allow us to send a batch into the transformer even when the examples in the batch have varying lengths. We do this by padding all sequences to the same length, then using the &amp;ldquo;attention_mask&amp;rdquo; tensor to identify which tokens are padding.&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;../../img/attention_mask.png&#34;&#xA;    alt=&#34;Here we use a batch with three samples padded from the left since we want to predict the next token on the right. (Padding on the right would probably predict another pad.)&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;p&gt;Here we use a batch with three samples padded from the left since we want to predict the next token on the right. (Padding on the right would probably predict another pad.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Train and Run a Simple Language Model</title>
      <link>https://lukesalamone.github.io/posts/running-simple-language-model/</link>
      <pubDate>Fri, 16 Apr 2021 21:08:53 -0500</pubDate>
      <guid>https://lukesalamone.github.io/posts/running-simple-language-model/</guid>
      <description>&lt;p&gt;This article will show how to run a simple language model, KenLM. It&amp;rsquo;s not as powerful as transformer-based models like BERT or GPT-3, but depending on what you&amp;rsquo;re trying to accomplish it may be more than enough. This tutorial should take you about 15 minutes, including the time to run the scripts.&lt;/p&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s work backwards from where we&amp;rsquo;re trying to get to. When you&amp;rsquo;ve finished, you should be able to run the following script:&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is Temperature in NLP?üê≠</title>
      <link>https://lukesalamone.github.io/posts/what-is-temperature/</link>
      <pubDate>Fri, 02 Apr 2021 00:50:38 -0500</pubDate>
      <guid>https://lukesalamone.github.io/posts/what-is-temperature/</guid>
      <description>&lt;script type=&#34;text/x-mathjax-config&#34;&gt;&#xA;MathJax.Hub.Config({&#xA;  tex2jax: {&#xA;    inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],&#xA;    displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\[&#39;,&#39;\]&#39;]],&#xA;    processEscapes: true,&#xA;    processEnvironments: true,&#xA;    skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;],&#xA;    TeX: {&#xA;      equationNumbers: {&#xA;        autoNumber: &#34;AMS&#34;&#xA;      },&#xA;      extensions: [&#34;AMSmath.js&#34;, &#34;AMSsymbols.js&#34;]&#xA;    }&#xA;  }&#xA;});&#xA;&lt;/script&gt;&#xA;&lt;script type=&#34;text/x-mathjax-config&#34;&gt;&#xA;  MathJax.Hub.Queue(function() {&#xA;    // Fix &lt;code&gt; tags after MathJax finishes running. This is a&#xA;    // hack to overcome a shortcoming of Markdown. Discussion at&#xA;    // https://github.com/mojombo/jekyll/issues/199&#xA;    var all = MathJax.Hub.getAllJax(), i;&#xA;    for(i = 0; i &lt; all.length; i += 1) {&#xA;        all[i].SourceElement().parentNode.className += &#39; has-jax&#39;;&#xA;    }&#xA;});&#xA;&lt;/script&gt;&#xA;&lt;p&gt;Temperature is a parameter used in natural language processing models to increase or decrease the &amp;ldquo;confidence&amp;rdquo; a model has in its most likely response.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is Perplexity?</title>
      <link>https://lukesalamone.github.io/posts/perplexity/</link>
      <pubDate>Thu, 01 Apr 2021 12:14:49 -0500</pubDate>
      <guid>https://lukesalamone.github.io/posts/perplexity/</guid>
      <description>&lt;script type=&#34;text/javascript&#34; async src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;&lt;/script&gt;&#xA;&lt;script type=&#34;text/x-mathjax-config&#34;&gt;&#xA;MathJax.Hub.Config({&#xA;  tex2jax: {&#xA;    inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]],&#xA;    displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\[&#39;,&#39;\]&#39;]],&#xA;    processEscapes: true,&#xA;    processEnvironments: true,&#xA;    skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;],&#xA;    TeX: {&#xA;      equationNumbers: {&#xA;        autoNumber: &#34;AMS&#34;&#xA;      },&#xA;      extensions: [&#34;AMSmath.js&#34;, &#34;AMSsymbols.js&#34;]&#xA;    }&#xA;  }&#xA;});&#xA;&lt;/script&gt;&#xA;&lt;script type=&#34;text/x-mathjax-config&#34;&gt;&#xA;  MathJax.Hub.Queue(function() {&#xA;    var all = MathJax.Hub.getAllJax(), i;&#xA;    for(i = 0; i &lt; all.length; i += 1) {&#xA;        all[i].SourceElement().parentNode.className += &#39; has-jax&#39;;&#xA;    }&#xA;});&#xA;&lt;/script&gt;&#xA;&lt;p&gt;&lt;strong&gt;TLDR: NLP metric ranging from 1 to infinity. Lower is better.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;In natural language processing, perplexity is the most common metric used to measure the performance of a language model. To calculate perplexity, we use the following formula:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
