<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine-Learning on Luke Salamone&#39;s Blog</title>
    <link>https://lukesalamone.github.io/tags/machine-learning/</link>
    <description>Recent content in Machine-Learning on Luke Salamone&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 27 Jun 2024 18:40:06 -0700</lastBuildDate>
    <atom:link href="https://lukesalamone.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Very Large Datasets in PyTorch</title>
      <link>https://lukesalamone.github.io/posts/very-large-datasets/</link>
      <pubDate>Thu, 27 Jun 2024 18:40:06 -0700</pubDate>
      <guid>https://lukesalamone.github.io/posts/very-large-datasets/</guid>
      <description>In God we trust. All others must bring data. ~ W. Edwards Deming&#xA;Datasets that fit in memory For simple machine learning problems, your PyTorch dataset class probably looks something like this:&#xA;class SimpleDataset(Dataset): def __init__(self, features, targets): self.features = [] for feature in features: self.features.append(self._feature_transform(feature)) self.targets = targets def _feature_transform(self, feature): # Optional feature transformation function which # converts each feature into its input representation # for the model.</description>
    </item>
    <item>
      <title>How does HNSW work?</title>
      <link>https://lukesalamone.github.io/posts/how-does-hnsw-work/</link>
      <pubDate>Mon, 20 May 2024 13:38:01 -0700</pubDate>
      <guid>https://lukesalamone.github.io/posts/how-does-hnsw-work/</guid>
      <description>Suppose we have a vector database with a billion items in it (the haystack). And suppose we are looking for K vectors, the needles which maximize some similarity function. (In the case of cosine similarity or euclidean distance, we may be maximizing 1-distance(x,y).) And also suppose that we&amp;rsquo;d like to do this quickly.&#xA;Naive and semi-naive approaches One approach might be to compare every vector and take the argmax. In that case, for vectors of length D our runtime will be 1 billion x D.</description>
    </item>
    <item>
      <title>Vectorized K-Means Clustering</title>
      <link>https://lukesalamone.github.io/posts/vectorized-kmeans/</link>
      <pubDate>Sun, 04 Feb 2024 23:39:29 -0700</pubDate>
      <guid>https://lukesalamone.github.io/posts/vectorized-kmeans/</guid>
      <description>K-means clustering (previous discussion) is an unsupervised learning algorithm which assigns points to one of K different clusters based on the distance of that point to a centroid. The points may represent physical locations, or embeddings in high-dimensional vector space.&#xA;ðŸŒŸCheck out the demo (in two dimensions) below. Centroids are colored white.ðŸŒŸ&#xA;start Note that the points are changing color only, not moving.&#xA;General algorithm The basic K-means algorithm is fairly simple and has two steps, repeated until convergence (i.</description>
    </item>
    <item>
      <title>Paper Summary: Dual-Encoders in Ranking</title>
      <link>https://lukesalamone.github.io/posts/dual-encoders-ranking/</link>
      <pubDate>Sat, 17 Dec 2022 16:53:47 -0800</pubDate>
      <guid>https://lukesalamone.github.io/posts/dual-encoders-ranking/</guid>
      <description>In Defense of Dual-Encoders for Neural Ranking by Menon et. al. discusses the question of why dual-encoder (DE) models, also called Bi-Encoders elsewhere, don&amp;rsquo;t match the performance of cross-attention (CA) models. The authors investigate what is actually going on, and demonstrate some improved performance over baseline DE models with a new model distillation method.&#xA;Background Search requires an automatic way to find the most relevant documents to a query. There are bag-of-word approaches to this task (for example BM25) and neural approaches.</description>
    </item>
    <item>
      <title>How to Create a Custom Pytorch Dataloader</title>
      <link>https://lukesalamone.github.io/posts/pytorch-dataloader/</link>
      <pubDate>Thu, 28 Apr 2022 18:22:07 -0500</pubDate>
      <guid>https://lukesalamone.github.io/posts/pytorch-dataloader/</guid>
      <description>First, create a custom dataset class.&#xA;from torch.utils.data import Dataset, DataLoader class CustomDataset(Dataset): def __init__(self, features, labels): assert len(features) == len(labels) self.features = features self.labels = labels def __len__(self): return len(self.features) def __getitem__(self, idx): return self.features[idx], self.labels[idx] Next, create a custom dataloader where we specify the batch size.&#xA;features, labels = load_data() # features &amp;amp; labels must have equal lengths # e.g. features = [[1,2,3],[4,5,6]] # labels = [7,8] dataset = CustomDataset(features, labels) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) Finally, iterate over the dataloader during training.</description>
    </item>
    <item>
      <title>How does K-means clustering work?</title>
      <link>https://lukesalamone.github.io/posts/kmeans-clustering/</link>
      <pubDate>Wed, 07 Oct 2020 17:39:22 -0700</pubDate>
      <guid>https://lukesalamone.github.io/posts/kmeans-clustering/</guid>
      <description>K-means clustering (not to be confused with K-nearest neighbors) is an unsupervised learning algorithm used for grouping similar points together into clusters.&#xA;start Algorithm The basic K-means algorithm is fairly simple and has two steps, repeated until convergence:&#xA;assign points to cluster corresponding to closest centroid update centroid locations to the mean of all points assigned to the associated cluster The algorithm converges when the centroids stop moving, i.e. no points can be switched between clusters to a closer centroid.</description>
    </item>
  </channel>
</rss>
