<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="keywords" content=", nlp, gpt2" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://lukesalamone.github.io/posts/gpt2-tokenization/" />

<meta property="og:title" content=""/>
<meta property="og:image" content=""/>
<meta property="og:description" content=""/>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-9GBJCJFKED"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-9GBJCJFKED');
</script>


    <title>
        
            How does GPT-2 Tokenize Text? :: Luke Salamone&#39;s Blog 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="../../main.8a6383efac6dd742c5d60f7b7fd685e292562affcc27ebd03293f62544a84aed.css">






<meta itemprop="name" content="How does GPT-2 Tokenize Text?">
<meta itemprop="description" content="Let&rsquo;s explore how GPT-2 tokenizes text.
What is tokenization? It&rsquo;s important to understand that GPT-2 doesn&rsquo;t work with strings directly. Instead, it needs to tokenize the input string, which is essentially a process for converting the string into a list of numbers, or &ldquo;tokens&rdquo;. It is these tokens which are passed into the model during training or for inference. As a concrete example, let&rsquo;s look at a few sample sentences:"><meta itemprop="datePublished" content="2021-06-17T19:30:48-05:00" />
<meta itemprop="dateModified" content="2021-06-17T19:30:48-05:00" />
<meta itemprop="wordCount" content="828"><meta itemprop="image" content="https://lukesalamone.github.io"/>
<meta itemprop="keywords" content="nlp,gpt2," />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://lukesalamone.github.io"/>

<meta name="twitter:title" content="How does GPT-2 Tokenize Text?"/>
<meta name="twitter:description" content="Let&rsquo;s explore how GPT-2 tokenizes text.
What is tokenization? It&rsquo;s important to understand that GPT-2 doesn&rsquo;t work with strings directly. Instead, it needs to tokenize the input string, which is essentially a process for converting the string into a list of numbers, or &ldquo;tokens&rdquo;. It is these tokens which are passed into the model during training or for inference. As a concrete example, let&rsquo;s look at a few sample sentences:"/>



    <meta property="og:title" content="How does GPT-2 Tokenize Text?" />
<meta property="og:description" content="Let&rsquo;s explore how GPT-2 tokenizes text.
What is tokenization? It&rsquo;s important to understand that GPT-2 doesn&rsquo;t work with strings directly. Instead, it needs to tokenize the input string, which is essentially a process for converting the string into a list of numbers, or &ldquo;tokens&rdquo;. It is these tokens which are passed into the model during training or for inference. As a concrete example, let&rsquo;s look at a few sample sentences:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lukesalamone.github.io/posts/gpt2-tokenization/" /><meta property="og:image" content="https://lukesalamone.github.io"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-06-17T19:30:48-05:00" />
<meta property="article:modified_time" content="2021-06-17T19:30:48-05:00" /><meta property="og:site_name" content="Luke Salamone&#39;s Blog" />







    <meta property="article:published_time" content="2021-06-17 19:30:48 -0500 -0500" />








    </head>

    <body class="dark-theme">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="../../" style="text-decoration: none;">
  <div class="logo">
    <div class="logo-holder-dark">
      <img src="../../img/logo.png" alt="Blog Home" />
    </div>
    <div class="logo-holder-light">
      <img src="../../img/logo2.png" alt="Blog Home" />
    </div>
  </div>
</a>


        <span class="header__right">
            
            <div id="header_search">
              <input type="text" placeholder="Search here" />
            </div>
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        4 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="https://lukesalamone.github.io/posts/gpt2-tokenization/">How does GPT-2 Tokenize Text?</a>
      </h1>

      

      <div class="post-content">
        

<p>Let&rsquo;s explore how GPT-2 tokenizes text.</p>

<h2 id="what-is-tokenization">What is tokenization?</h2>

<p>It&rsquo;s important to understand that GPT-2 doesn&rsquo;t work with strings directly. Instead, it needs to tokenize the input string, which is essentially a process for converting the string into a list of numbers, or &ldquo;tokens&rdquo;. It is these tokens which are passed into the model during training or for inference. As a concrete example, let&rsquo;s look at a few sample sentences:</p>

<pre><code class="language-python">tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokens1 = tokenizer('I love my dog')
</code></pre>

<p>When we look at <code>tokens1</code> we see there are 4 tokens:</p>

<pre><code class="language-python">{'input_ids': [40, 1842, 616, 3290], 'attention_mask': [1, 1, 1, 1]}
</code></pre>

<p>Here what we care about is the <code>'input_ids'</code> list. We can ignore the <code>'attention_mask'</code> for now. We can convert the tokens in <code>[40, 1842, 616, 3290]</code> back into strings using <code>tokenizer.decode</code>:</p>

<pre><code class="language-python">tokens1 = tokens1['input_ids']

[tokenizer.decode(x) for x in tokens1]
# prints ['I', ' love', ' my', ' dog']

[tokenizer.decode(x).strip().lower() for x in tokens1]
# prints ['i', 'love', 'my', 'dog']
</code></pre>

<p>This process allows us to recover the tokens as strings from the tokenizer. For dictionary lookups, we&rsquo;ll also lowercase the strings and remove the whitespace from them.</p>

<p>Now, let&rsquo;s see what happens when we do the same thing with more complex words:</p>

<pre><code class="language-python">tokens2 = tokenizer('My favorite color is chartreuse')['token_ids']
[tokenizer.decode(x).strip().lower() for x in tokens2]
# prints ['my', 'favorite', 'color', 'is', 'chart', 're', 'use']
</code></pre>

<p>Because &ldquo;chartreuse&rdquo; isn&rsquo;t in GPT-2&rsquo;s vocabulary, it is tokenized as &ldquo;chart&rdquo;, &ldquo;re&rdquo; and &ldquo;use&rdquo;.</p>

<h3 id="about-that-attention-mask">About that attention mask</h3>

<p>For brevity I glossed over what <code>attention_mask</code> does above. If you&rsquo;re interested in attention masks, <a href="../../posts/what-are-attention-masks">I have a blog post on that very topic</a>!</p>

<h2 id="english-words">English words</h2>

<p>Now it would be interesting to see how many tokens in GPT-2&rsquo;s vocabulary are actually English words. This is an imprecise metric since it depends heavily on which dictionary we use. (There is no single authoritative source of all English words.) I&rsquo;ll use several dictionaries and compare the results.</p>

<h3 id="enchant">Enchant</h3>

<p><a href="https://pyenchant.github.io/pyenchant/tutorial.html">PyEnchant</a> contains a python module <code>enchant</code> which we can use to check if a word is spelled correctly. It can also make spelling suggestions for incorrectly spelled words:</p>

<pre><code class="language-python">import enchant
d = enchant.request_dict(&quot;en_US&quot;)
d.check('Hello')
# prints True

d.check('Helo')
# prints False
</code></pre>

<h3 id="nltk-words">NLTK words</h3>

<p>The popular NLP library <a href="https://www.nltk.org/">NLTK</a> also contains a word list, accessible through its <code>corpus</code> module.</p>

<pre><code class="language-python">from nltk.corpus import words

nltk_words = set(words.words())
len(nltk_words)
# prints 235892
</code></pre>

<h3 id="english-350k">English 350k</h3>

<p>This list of words was taken from this <a href="https://github.com/dwyl/english-words/blob/master/words_alpha.txt">github repository</a>. It is a convenient list of lowercased words containing only letters. It seems to be the biggest of the word lists.</p>

<h3 id="lemmatization">Lemmatization</h3>

<p>We can bump our numbers up slightly through <a href="https://en.wikipedia.org/wiki/Lemmatisation">lemmatization</a>:</p>

<blockquote>
<p>In many languages, words appear in several inflected forms. For example, in English, the verb &lsquo;to walk&rsquo; may appear as &lsquo;walk&rsquo;, &lsquo;walked&rsquo;, &lsquo;walks&rsquo; or &lsquo;walking&rsquo;. The base form, &lsquo;walk&rsquo;, that one might look up in a dictionary, is called the lemma for the word. The association of the base form with a part of speech is often called a lexeme of the word.</p>
</blockquote>

<p>For our lemmatizer we will use <code>WordNetLemmatizer</code> from <code>nltk.stem.wordnet</code>.</p>

<h2 id="testing-gpt-2-tokens">Testing GPT-2 tokens</h2>

<p>So of the tokens which GPT-2 uses, how many are English words? We can break this down metric by the dictionary used.</p>

<table>
<thead>
<tr>
<th>Dictionary</th>
<th>% Words</th>
</tr>
</thead>

<tbody>
<tr>
<td>English370k †</td>
<td>72.92%</td>
</tr>

<tr>
<td>English370k</td>
<td>72.59%</td>
</tr>

<tr>
<td>Enchant †</td>
<td>60.48%</td>
</tr>

<tr>
<td>Enchant</td>
<td>60.17%</td>
</tr>

<tr>
<td>NLTK words †</td>
<td>57.07%</td>
</tr>

<tr>
<td>NLTK words</td>
<td>48.27%</td>
</tr>
</tbody>
</table>

<p><em>† indicates words were lemmatized</em></p>

<p>So the English370k word list seems to capture the most tokens from the three dictionaries. Also note the mild impact of lemmatization: although it may bump some of the percentages up a bit, it&rsquo;s not enough for one dictionary to outperform another.</p>

<div id="pie"><canvas></canvas></div>

<p>Looking at the tokens which aren&rsquo;t in the dictionary, around 73% of them are non-word alphabetical strings. The final 27% is accounted for by symbols, numbers, and non-ascii character sequences (unicode characters from languages like Arabic, Korean, and Chinese). If we remove these, we end up with about 10k tokens containing only letters, which is around 21% of GPT-2&rsquo;s total vocabulary. I&rsquo;ve included this list in a <a href="https://gist.github.com/lukesalamone/22ce6f362db3bdd09eda3cc5cbf5576f">github gist</a> (duplicates removed).</p>

<h2 id="now-what">Now what?</h2>

<p>Looking at these non-word alphabetical strings, it&rsquo;s interesting to see how the Internet (as GPT-2 saw it) was encoded. Then again, it also contains a lot of proper nouns which wouldn&rsquo;t be in a normal dictionary like &ldquo;starbucks&rdquo;.</p>

<p>Other tokens are clearly vestiges of the scraping process which was used to gather text which GPT-2 trained on. Tokens like &ldquo;rawdownloadcloneembedreportprint&rdquo;, &ldquo;buyableinstoreandonline&rdquo;, &ldquo;randomredditorwithno&rdquo;, and &ldquo;itemthumbnailimage&rdquo; contain next to zero semantic value and the vocabulary space would probably have been better served with more meaningful tokens.</p>

<p>The following are the longest non-dictionary tokens found in GPT-2&rsquo;s vocabulary:</p>

<table>
<thead>
<tr>
<th>Token ID</th>
<th>String</th>
</tr>
</thead>

<tbody>
<tr>
<td>39177</td>
<td>ItemThumbnailImage</td>
</tr>

<tr>
<td>30210</td>
<td>guiActiveUnfocused</td>
</tr>

<tr>
<td>39755</td>
<td>isSpecialOrderable</td>
</tr>

<tr>
<td>31576</td>
<td>externalActionCode</td>
</tr>

<tr>
<td>39753</td>
<td>quickShipAvailable</td>
</tr>

<tr>
<td>39757</td>
<td>channelAvailability</td>
</tr>

<tr>
<td>36174</td>
<td>RandomRedditorWithNo</td>
</tr>

<tr>
<td>30899</td>
<td>cloneembedreportprint</td>
</tr>

<tr>
<td>40242</td>
<td>BuyableInstoreAndOnline</td>
</tr>

<tr>
<td>30906</td>
<td>rawdownloadcloneembedreportprint</td>
</tr>
</tbody>
</table>

<p>We may also be able to measure performance of GPT-2 on certain tasks based on how many of the tokens were dictionary words. It might be true, for example, that sentences with higher proportions of dictionary word tokens would perform better on sentence completion tasks.</p>

<script src="../../js/chart.min.js" type="text/javascript"></script>
<script src="../../js/gpt2-tokens-pie.js" type="text/javascript"></script>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://lukesalamone.github.io/tags/nlp/">nlp</a></span>
        <span class="tag"><a href="https://lukesalamone.github.io/tags/gpt2/">gpt2</a></span>
        
    </p>

      

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        828 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2021-06-17 17:30
        

        
          
        
      </p>
    </div>

    
      <div class="pagination">
        <div class="pagination__title">
          <span class="pagination__title-h">Read other posts</span>
          <hr />
        </div>

        <div class="pagination__buttons">
          
            <span class="button previous">
              <a href="https://lukesalamone.github.io/posts/bert-vs-gpt2/">
                <span class="button__icon">←</span>
                <span class="button__text">BERT vs GPT-2 Performance</span>
              </a>
            </span>
          

          
            <span class="button next">
              <a href="https://lukesalamone.github.io/posts/what-are-attention-masks/">
                <span class="button__text">What Are Attention Masks?</span>
                <span class="button__icon">→</span>
              </a>
            </span>
          
        </div>
      </div>
    


    
  </main>
  <div id="shadow-search" class="posts">
    <div class="posts-group">
      <div class="post-year">2022</div>
      <ul class="posts-list"></ul>
    </div>
  </div>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2022</span>
            <span><a href="https://lukesalamone.com/" target="_blank">lukesalamone.com</a></span>
            <span>&nbsp;|&nbsp;</span>
            <span><a href="https://github.com/lukesalamone/" target="_blank">github</a></span>
        </div>
    </div>
</footer>

            
        </div>

        




<script type="text/javascript" src="../../bundle.min.6e3c1600024e56e8bc744e9d2307abeb07531c35d8a501e933992234d69d5ac4b89d2a70dbeb88af234d9f7a031494ce781ffeb155e6027abcfd4f10ce80a3f8.js" integrity="sha512-bjwWAAJOVui8dE6dIwer6wdTHDXYpQHpM5kiNNadWsS4nSpw2&#43;uIryNNn3oDFJTOeB/&#43;sVXmAnq8/U8QzoCj&#43;A=="></script>



    </body>
</html>
