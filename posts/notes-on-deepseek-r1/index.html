<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="keywords" content=", llm, reinforcement-learning, fine-tuning" />
<meta name="google-site-verification" content="sPwRw-1pCCEcm2EbqDJiJzJEnze1K7FkQzY9xLreBJQ" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://lukesalamone.github.io/posts/notes-on-deepseek-r1/" />

<meta property="og:title" content=""/>
<meta property="og:image" content=""/>
<meta property="og:description" content=""/>

<script>
    if(location.hostname !== 'localhost') {
        
            fetch('https://d1tkdcmshl91xi.cloudfront.net?p=' + window.location.href.match(/\/([^/]+)\/$/)[1]);
        
    }
</script>

<script data-goatcounter="https://qw6ut244wbxe3rf2bvu5.goatcounter.com/count" async src="../../js/count.js"></script>


    <title>
        
            Notes on Deepseek R1 :: Luke Salamone&#39;s Blog 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="../../main.e929a2b129440353e8b5e39dd12696947040fd7c9a9639e600bca78a3fadd11b.css">




    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="shortcut icon" href="../../favicon.ico">
    <meta name="msapplication-TileColor" content="#2d89ef">
    <meta name="theme-color" content="#ffffff">




  <meta itemprop="name" content="Notes on Deepseek R1">
  <meta itemprop="description" content="DeepSeek R1 is a large language model which employs test-time compute to generate a response. Unlike many decoder-based models in the past which simply continue the given text (and may be fine-tuned for conversation), R1 generates reasoning tokens before the final answer is given. According to the researchers, its performance is on par with OpenAI’s O1 model.
Terminology First, I will briefly describe some terminology related to training techniques:
Supervised fine-tuning (SFT) is a process which uses input/output pairs to directly fine-tune a model. In a reinforcement learning setting, SFT can help to mitigate cold start issues by providing initial policy behavior prior to RL training. The downside of SFT is that the input/output pairs can be expensive to acquire.">
  <meta itemprop="datePublished" content="2025-01-28T08:35:55-08:00">
  <meta itemprop="dateModified" content="2025-01-28T08:35:55-08:00">
  <meta itemprop="wordCount" content="1022">
  <meta itemprop="image" content="https://lukesalamone.github.io/">
  <meta itemprop="keywords" content="Llm,Reinforcement-Learning,Fine-Tuning">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://lukesalamone.github.io/">
  <meta name="twitter:title" content="Notes on Deepseek R1">
  <meta name="twitter:description" content="DeepSeek R1 is a large language model which employs test-time compute to generate a response. Unlike many decoder-based models in the past which simply continue the given text (and may be fine-tuned for conversation), R1 generates reasoning tokens before the final answer is given. According to the researchers, its performance is on par with OpenAI’s O1 model.
Terminology First, I will briefly describe some terminology related to training techniques:
Supervised fine-tuning (SFT) is a process which uses input/output pairs to directly fine-tune a model. In a reinforcement learning setting, SFT can help to mitigate cold start issues by providing initial policy behavior prior to RL training. The downside of SFT is that the input/output pairs can be expensive to acquire.">



    <meta property="og:url" content="https://lukesalamone.github.io/posts/notes-on-deepseek-r1/">
  <meta property="og:site_name" content="Luke Salamone&#39;s Blog">
  <meta property="og:title" content="Notes on Deepseek R1">
  <meta property="og:description" content="DeepSeek R1 is a large language model which employs test-time compute to generate a response. Unlike many decoder-based models in the past which simply continue the given text (and may be fine-tuned for conversation), R1 generates reasoning tokens before the final answer is given. According to the researchers, its performance is on par with OpenAI’s O1 model.
Terminology First, I will briefly describe some terminology related to training techniques:
Supervised fine-tuning (SFT) is a process which uses input/output pairs to directly fine-tune a model. In a reinforcement learning setting, SFT can help to mitigate cold start issues by providing initial policy behavior prior to RL training. The downside of SFT is that the input/output pairs can be expensive to acquire.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-01-28T08:35:55-08:00">
    <meta property="article:modified_time" content="2025-01-28T08:35:55-08:00">
    <meta property="article:tag" content="Llm">
    <meta property="article:tag" content="Reinforcement-Learning">
    <meta property="article:tag" content="Fine-Tuning">
    <meta property="og:image" content="https://lukesalamone.github.io/">






    <meta property="article:published_time" content="2025-01-28 08:35:55 -0800 -0800" />








    </head>

    <body class="dark-theme">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="../../" style="text-decoration: none;">
  <div class="logo">
    <div class="logo-holder-dark">
      <img src="../../img/logo.png" alt="Blog Home" />
    </div>
    <div class="logo-holder-light">
      <img src="../../img/logo2.png" alt="Blog Home" />
    </div>
  </div>
</a>


        <span class="header__right">
            
            <div id="header_search">
              <input type="text" placeholder="Search here" />
            </div>
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        5 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="https://lukesalamone.github.io/posts/notes-on-deepseek-r1/">Notes on Deepseek R1</a>
      </h1>

      

      <div class="post-content">
        <p>DeepSeek R1 is a large language model which employs test-time compute to generate a response. Unlike many decoder-based models in the past which simply continue the given text (and may be fine-tuned for conversation), R1 generates reasoning tokens before the final answer is given. According to the researchers, its performance is on par with OpenAI&rsquo;s O1 model.</p>
<h2 id="terminology">Terminology</h2>
<p>First, I will briefly describe some terminology related to training techniques:</p>
<ul>
<li>
<p><strong>Supervised fine-tuning (SFT)</strong> is a process which uses input/output pairs to directly fine-tune a model. In a reinforcement learning setting, SFT can help to mitigate cold start issues by providing initial policy behavior prior to RL training. The downside of SFT is that the input/output pairs can be expensive to acquire.</p>
</li>
<li>
<p><strong>Preference fine-tuning</strong> involves training a model using preferences between different outputs for the same input. Usually these preferences are human-generated. Relative quality assessments are easier for humans to make, but are still a valuable training signal.</p>
</li>
<li>
<p><strong>Rejection sampling fine-tuning (RFT)</strong> is a method for generating synthetic training data. First, a model produces k diverse candidate completions for a prompt using temperature sampling. Next, a reward model ranks or selects the best completions for training data.</p>
<ul>
<li>In the simplest version, this technique simply generates k <code>(reasoning, answer)</code> pairs for each prompt using a nonzero temperature (e.g. 0.7) and filters out incorrect answers. <a href="https://arxiv.org/pdf/2308.01825">Paper</a></li>
</ul>
</li>
<li>
<p><strong>Large scale reinforcement learning (RL)</strong>, as DeepSeek refers to it, is a training method which which assigns synthetic labels to a response based on easily-verified outcomes from reasoning problems such as correctness and formatting.</p>
<ul>
<li>This is in contrast to a supervised learning approach, which would typically enforce strict token-level matching.</li>
</ul>
</li>
</ul>
<p>Here are a few terms related to model architecture:</p>
<ul>
<li>
<p><strong>Mixture of Experts (MoE)</strong> is an architecture which uses a &ldquo;router&rdquo; to select from a pool of candidate &ldquo;expert&rdquo; networks.</p>
<ul>
<li>In a typical decoder transformer such as GPT-2 is simply a tall stack of decoder blocks, where 100% of the weights are activated for each token. In this paradigm, making the model bigger either means making bigger blocks or a taller stack of them, which both increase inference costs.</li>
<li>However, replacing simple decoder blocks with MoE blocks allows models to include more parameters without significantly increasing inference costs, since only a fraction of experts are active at any one time.</li>
</ul>
</li>
<li>
<p><strong>Multi-head Latent Attention (MLA)</strong> is an alternative to traditional multihead attention which improves inference efficiency by using a low-rank approximation for the KV cache.</p>
</li>
</ul>
<h2 id="paper-links">Paper links</h2>
<p>The release of R1 follows other papers from DeepSeek which go into extensive depth regarding many of the primatives discussed above. They&rsquo;re worth a read:</p>
<ul>
<li>
<p><a href="https://arxiv.org/pdf/2405.04434">DeepSeek-V2</a> introduces Multi-head Latent Attention (MLA) and several Mixture of Experts (MoE) optimizations to improve load balancing at the device level and expert level.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2402.03300">DeepSeekMath</a> discusses Rejection Sampling Fine-Tuning (RFT) and compares Direct Preference Optimization (DPO), Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO).</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2412.19437v1">DeepSeek-V3</a> describes a multi-token training objective, improvements to MoE load balancing, SFT, RL for fine-tuning, and framework optimizations related to training efficiency.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2501.12948">DeepSeek-R1</a> discusses training techniques (large-scale RL, SFT, and RFT) for training R1-Zero and R1.</p>
</li>
</ul>
<h2 id="how-deepseek-r1-zero-was-trained">How DeepSeek R1-Zero was trained</h2>
<p>The researchers apply RL directly to the base model (DeepSeek v3) without any cold start data to generate DeepSeek R1-Zero. However, the researchers note poor formatting/readability and the mixing multiple languages in the reasoning outputs.</p>
<p>In the training process, the researchers reused the Group Relative Policy Optimization (GRPO) algorithm which they introduced in <a href="https://arxiv.org/pdf/2402.03300">their DeepSeekMath paper</a>. This algorithm requires less computational overhead since it does not require a critic, and is stable as it measures output performance relative to the group. Responses received rewards based on accuracy and correct formatting.</p>
<p>R1-Zero was required to adhere to the format <code>&lt;think&gt;&lt;/think&gt;&lt;answer&gt;&lt;/answer&gt;</code> when responding.</p>
<h2 id="how-deepseek-r1-was-trained">How DeepSeek R1 was trained</h2>
<p>To address the usability issues in R1 Zero, the researchers expanded on R1 Zero with a four step process:</p>
<ol>
<li>Supervised fine-tuning with a small number of R1-Zero outputs, cleaned by human annotators</li>
<li>Large scale RL, in the same way as R1 Zero. The language mixing issue returned, but the researchers mitigated it with a language consistency reward during training. This reward did result in a slight drop in performance, however.</li>
<li>Supervised fine-tuning data collected via rejection sampling. These prompts cover both reasoning and non-reasoning domains.
<ul>
<li>For reasoning domains (600k examples), only the correct response was retained.</li>
<li>For non-reasoning domains (200k samples) like writing, factual QA, translation, etc. the researchers reused techniques from DeepSeek v3: synthetic responses cleaned up with human annotators.</li>
</ul>
</li>
<li>Further RL refinement for &ldquo;helpfullness and harmlessness&rdquo;:
<ul>
<li>For reasoning tasks, the researchers used rule-based rewards, as in R1-Zero.</li>
<li>For non-reasoning tasks, the researchers used reward models to capture preferences.</li>
<li>To optimize helpfulness, only the final summary was evaluated.</li>
<li>To optimize harmlessness, both the reasoning tokens and final summary were evaluated.</li>
</ul>
</li>
</ol>
<h2 id="further-distillation">Further Distillation</h2>
<p>Using the 800k inputs from step 3 above, the researchers generated outputs from R1. They then performed supervised fine-tuning on Qwen (1.5B, 7B, 14B, 32B) and Llama (8B and 70B).</p>
<h2 id="discussion">Discussion</h2>
<ul>
<li>In their experiments, the researchers performed large-scale RL training on Qwen-32B-Base and compared with a 32B model distilled from DeepSeek R1-Zero, which was also trained using RL only. The distilled model performed significantly better.
<ul>
<li>This demonstrates that distillation performs significantly better than large-scale RL training from scratch.</li>
<li>This also means that RL by itself cannot surpass larger models.</li>
</ul>
</li>
<li>The researchers also tried using a Process Reward Model (PRM). However, identifying fine-grained reasoning steps proved difficult to implement in practice, is susceptible to reward hacking, and complicates the overall training pipeline.
<ul>
<li>The researchers do note that PRM may be useful for reranking top-N responses from the model or assisting in guided search.</li>
</ul>
</li>
<li>Intuitively, Monte Carlo Tree Search (MCTS) seemed like a promising route for improving reasoning. MCTS involves learning a model to search a tree of possibilities, such as a game tree in chess or go.
<ul>
<li>Unlike in turn-based games where there are a discrete set of possible actions, reasoning problems have a continuous search space.</li>
<li>Additionally, MCTS depends on an accurate value model for exploring the tree, so a poor value model will hurt performance. MCTS may be useful at inference time, however.</li>
</ul>
</li>
</ul>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://lukesalamone.github.io/tags/llm/">llm</a></span>
        <span class="tag"><a href="https://lukesalamone.github.io/tags/reinforcement-learning/">reinforcement-learning</a></span>
        <span class="tag"><a href="https://lukesalamone.github.io/tags/fine-tuning/">fine-tuning</a></span>
        
    </p>

      

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        1022 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2025-01-29 00:35
        

        
          
        
      </p>
    </div>

    
      <div class="pagination">
        <div class="pagination__title">
          <span class="pagination__title-h">Read other posts</span>
          <hr />
        </div>

        <div class="pagination__buttons">
          

          
            <span class="button next">
              <a href="https://lukesalamone.github.io/posts/space-is-really-big/">
                <span class="button__text">Space Is Really Big</span>
                <span class="button__icon">→</span>
              </a>
            </span>
          
        </div>
      </div>
    

  </main>
  <div id="shadow-search" class="posts">
    <div class="posts-group">
      <div class="post-year">2022</div>
      <ul class="posts-list"></ul>
    </div>
  </div>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2025 Luke Salamone</span>&nbsp;|&nbsp;
            <span><a href="https://lukesalamone.com/" target="_blank">lukesalamone.com</a>&nbsp;|&nbsp;<a href="https://github.com/lukesalamone/" target="_blank">github</a></span>
        </div>
    </div>
</footer>

            
        </div>

        




<script type="text/javascript" src="../../bundle.min.e5fbf64306713fe8df1194a658201f5e861815365f67b3c6fc9094ed28d21c92aeaff20b7f4c12c25ba7efabe31291ffd1074e33cd60cf8ff8d8ffef7c028d73.js" integrity="sha512-5fv2QwZxP&#43;jfEZSmWCAfXoYYFTZfZ7PG/JCU7SjSHJKur/ILf0wSwlun76vjEpH/0QdOM81gz4/42P/vfAKNcw=="></script>



    </body>
</html>
