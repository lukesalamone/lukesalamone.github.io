<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="keywords" content=", deep-learning, llm" />
<meta name="google-site-verification" content="sPwRw-1pCCEcm2EbqDJiJzJEnze1K7FkQzY9xLreBJQ" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://lukesalamone.github.io/posts/sparse-autoencoder/" />

<meta property="og:title" content=""/>
<meta property="og:image" content=""/>
<meta property="og:description" content=""/>

<script>
    if(location.hostname !== 'localhost') {
        
            fetch('https://d1tkdcmshl91xi.cloudfront.net?p=' + window.location.href.match(/\/([^/]+)\/$/)[1]);
        
    }
</script>

<script data-goatcounter="https://qw6ut244wbxe3rf2bvu5.goatcounter.com/count" async src="../../js/count.js"></script>


    <title>
        
            What are Sparse Autoencoders? :: Luke Salamone&#39;s Blog 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="../../main.e929a2b129440353e8b5e39dd12696947040fd7c9a9639e600bca78a3fadd11b.css">




    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="shortcut icon" href="../../favicon.ico">
    <meta name="msapplication-TileColor" content="#2d89ef">
    <meta name="theme-color" content="#ffffff">




  <meta itemprop="name" content="What are Sparse Autoencoders?">
  <meta itemprop="description" content="TLDR: A sparse autoencoder is just a regular autoencoder that encourages sparsity with an L1 penalty or KL divergence loss rather than using a low-dimensional bottleneck.">
  <meta itemprop="datePublished" content="2024-06-06T16:30:27-07:00">
  <meta itemprop="dateModified" content="2024-06-06T16:30:27-07:00">
  <meta itemprop="wordCount" content="330">
  <meta itemprop="image" content="https://lukesalamone.github.io/">
  <meta itemprop="keywords" content="Deep-Learning,Llm">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://lukesalamone.github.io/">
  <meta name="twitter:title" content="What are Sparse Autoencoders?">
  <meta name="twitter:description" content="TLDR: A sparse autoencoder is just a regular autoencoder that encourages sparsity with an L1 penalty or KL divergence loss rather than using a low-dimensional bottleneck.">



    <meta property="og:url" content="https://lukesalamone.github.io/posts/sparse-autoencoder/">
  <meta property="og:site_name" content="Luke Salamone&#39;s Blog">
  <meta property="og:title" content="What are Sparse Autoencoders?">
  <meta property="og:description" content="TLDR: A sparse autoencoder is just a regular autoencoder that encourages sparsity with an L1 penalty or KL divergence loss rather than using a low-dimensional bottleneck.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-06-06T16:30:27-07:00">
    <meta property="article:modified_time" content="2024-06-06T16:30:27-07:00">
    <meta property="article:tag" content="Deep-Learning">
    <meta property="article:tag" content="Llm">
    <meta property="og:image" content="https://lukesalamone.github.io/">






    <meta property="article:published_time" content="2024-06-06 16:30:27 -0700 -0700" />








    </head>

    <body class="dark-theme">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="../../" style="text-decoration: none;">
  <div class="logo">
    <div class="logo-holder-dark">
      <img src="../../img/logo.png" alt="Blog Home" />
    </div>
    <div class="logo-holder-light">
      <img src="../../img/logo2.png" alt="Blog Home" />
    </div>
  </div>
</a>


        <span class="header__right">
            
            <div id="header_search">
              <input type="text" placeholder="Search here" />
            </div>
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        2 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="https://lukesalamone.github.io/posts/sparse-autoencoder/">What are Sparse Autoencoders?</a>
      </h1>

      

      <div class="post-content">
        <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      },
      extensions: ["AMSmath.js", "AMSsymbols.js"]
    }
  }
});
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<p><strong>TLDR: A sparse autoencoder is just a regular autoencoder that encourages sparsity with an L1 penalty or KL divergence loss rather than using a low-dimensional bottleneck.</strong></p>
<p>If you understood all of those words above, you may be interested in the <a href="https://cdn.openai.com/papers/sparse-autoencoders.pdf">OpenAI paper</a> which used sparse autoencoders to interpret features from GPT-4.</p>
<p>If not, I&rsquo;ll try to break it down.</p>
<h2 id="what-is-an-autoencoder">What is an autoencoder?</h2>
<p>An autoencoder is a machine learning architecture which contains two functions: an <code>encoder</code> and a <code>decoder</code>. The <code>encoder</code> learns to create an efficient representation of the input, similar to lossy file compression. The <code>decoder</code> learns to reconstruct the original input from the efficient representation.</p>
<p>I have a couple of other pretty good explanations on this. In the first post, I described <a href="../../posts/build-an-autoencoder/">how to build an autoencoder</a>. Later, I discussed their potential <a href="../../posts/learning-the-haystack/#autoencoders">applications in vector retrieval</a>.</p>
<h2 id="what-is-sparsity">What is sparsity?</h2>
<p>Sparsity just means that most of the values in a vector are zero.</p>
<h2 id="what-is-an-l1-penalty">What is an L1 penalty?</h2>
<p>L1 stands for penalizing the $ L^1 $ norm of a vector. In other words, the sum of the absolute values of the values in the vector:</p>
<p style="text-align: center; font-size: 1.5em">
$$
\text{L1 norm} = \sum_{i=1}^N \vert x_i \vert
$$
</p>
<h2 id="what-is-kl-divergence">What is KL divergence?</h2>
<p><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a> is a measure of the &ldquo;distance&rdquo; one probability distribution has from another:</p>
<p style="text-align: center; font-size: 1.5em">
$$
\sum_{x \in X} P(x) \log \left( \frac{P(x)}{Q(x)} \right)
$$
</p>
<p>Note that KL divergence is not symmetrical, so distance isn&rsquo;t an entirely accurate description. If you&rsquo;re looking for a symmetrical measure of distribution similarity, use Jensen-Shannon divergence. Additionally, $ X $ doesn&rsquo;t have to be a continuous variable, although it often is.</p>
<p>In the demo below you can see that as the probability distributions overlap more and more, the KL divergence decreases.</p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.7.2/Chart.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
<script src="../../js/kl_divergence.js"></script>
<p><canvas id="myChart" style="background-color: #0000"></canvas></p>
<div style="text-align:center">
  <span>mean:
  <input type="range" min="300" max="600" value="400" class="slider" id="mean">
  <span id="mean_label">40.0</span>
</div>
<div style="text-align:center">
  <span>variance:
  <input type="range" min="100" max="500" value="200" class="slider" id="variance">
  <span id="variance_label">20.0</span>
</div>
<div style="text-align:center">
  <span id="kl_divergence_label"></span>
</div>
<h2 id="what-is-a-kl-divergence-loss">What is a KL divergence loss?</h2>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html">KL divergence loss</a> tries to force a model&rsquo;s predicted distribution to match a target distribution. Common applications of KL divergence loss are t-SNE dimensionality reduction and generative models like variational autoencoders and generative adversarial networks.</p>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://lukesalamone.github.io/tags/deep-learning/">deep-learning</a></span>
        <span class="tag"><a href="https://lukesalamone.github.io/tags/llm/">llm</a></span>
        
    </p>

      

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        330 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2024-06-07 07:30
        

        
          
        
      </p>
    </div>

    
      <div class="pagination">
        <div class="pagination__title">
          <span class="pagination__title-h">Read other posts</span>
          <hr />
        </div>

        <div class="pagination__buttons">
          
            <span class="button previous">
              <a href="https://lukesalamone.github.io/posts/most-ramified-chess-position-2023/">
                <span class="button__icon">←</span>
                <span class="button__text">The Most Ramified Chess Position of 2023</span>
              </a>
            </span>
          

          
            <span class="button next">
              <a href="https://lukesalamone.github.io/posts/how-does-hnsw-work/">
                <span class="button__text">How does HNSW work?</span>
                <span class="button__icon">→</span>
              </a>
            </span>
          
        </div>
      </div>
    

  </main>
  <div id="shadow-search" class="posts">
    <div class="posts-group">
      <div class="post-year">2022</div>
      <ul class="posts-list"></ul>
    </div>
  </div>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2025 Luke Salamone</span>&nbsp;|&nbsp;
            <span><a href="https://lukesalamone.com/" target="_blank">lukesalamone.com</a>&nbsp;|&nbsp;<a href="https://github.com/lukesalamone/" target="_blank">github</a></span>
        </div>
    </div>
</footer>

            
        </div>

        




<script type="text/javascript" src="../../bundle.min.e5fbf64306713fe8df1194a658201f5e861815365f67b3c6fc9094ed28d21c92aeaff20b7f4c12c25ba7efabe31291ffd1074e33cd60cf8ff8d8ffef7c028d73.js" integrity="sha512-5fv2QwZxP&#43;jfEZSmWCAfXoYYFTZfZ7PG/JCU7SjSHJKur/ILf0wSwlun76vjEpH/0QdOM81gz4/42P/vfAKNcw=="></script>



    </body>
</html>
