<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Luke Salamone&#39;s Blog</title>
        <link>https://lukesalamone.github.io/posts/</link>
        <description>Recent content in Posts on Luke Salamone&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Wed, 16 Nov 2022 15:24:15 -0500</lastBuildDate>
        <atom:link href="https://lukesalamone.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>A Few Notes on the Transformer</title>
            <link>https://lukesalamone.github.io/posts/self-attention/</link>
            <pubDate>Wed, 16 Nov 2022 15:24:15 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/self-attention/</guid>
            <description>A self-attention block depicted as a neural network.
  In this post I will describe the attention mechanism, commonly used in transformers, a popular neural language architecture. Most of the most well-known large language models of late are based on the transformer architecture. Attention was first described in Attention is All You Need by Vaswani et al.
What is attention? At a high level, attention is a mechanism for neural networks to boost portions of an input which are relevant and ignore those which aren&amp;rsquo;t.</description>
            <content type="html"><![CDATA[

<figure><img src="/img/self-attention.png"
         alt="A self-attention block depicted as a neural network."/><figcaption>
            <p>A self-attention block depicted as a neural network.</p>
        </figcaption>
</figure>


<p>In this post I will describe the attention mechanism, commonly used in transformers, a popular neural language architecture. Most of the most well-known large language models of late are based on the transformer architecture. Attention was first described in <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention is All You Need</a> by Vaswani et al.</p>

<h2 id="what-is-attention">What is attention?</h2>

<p>At a high level, attention is a mechanism for neural networks to boost portions of an input which are relevant and ignore those which aren&rsquo;t. In language models, attention is used as a way for the model to learn which portions of a sentence are relevant to each word.</p>

<h2 id="what-is-attention-for">What is attention for?</h2>

<p>Let&rsquo;s use an example:</p>

<blockquote>
<p>I am sitting at the library with my friend.</p>
</blockquote>

<p>It should be pretty clear that not all words in this sentence are equally important. What words are relevant to &ldquo;I&rdquo;? Probably &ldquo;sitting&rdquo;, &ldquo;library&rdquo;, and &ldquo;friend&rdquo;. Likewise, &ldquo;the&rdquo; might only be relevant to &ldquo;library&rdquo;. Attention provides a way for a model to increase and decrease the importance of each word.</p>

<p>Since the value of each token in the sequence is dependent on other tokens, this method of generating word embeddings is very different from more classical methods like <a href="https://arxiv.org/pdf/1301.3781.pdf">Word2Vec</a> and <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>. There is no one fixed vector for a given word.</p>

<p>And this makes sense. Many words in English are homonyms, and have identical spellings for distinct meanings. For example, &ldquo;rock&rdquo; is a genre of music but also can mean a stone. <a href="https://www.npr.org/2011/05/30/136796448/has-run-run-amok-it-has-645-meanings-so-far">The word &ldquo;run&rdquo; has 645 meanings and has recently replaced &ldquo;set&rdquo; as the word with the most definitions.</a> It would not make sense for all of these homonyms to have the same vector.</p>

<h2 id="an-interactive-example">An interactive example</h2>

<p>You can hover over each word to see the relative importances of each word in the sentence to the hovered word.</p>

<p><div id="attention_demo"></div>
<link rel="stylesheet" href="/css/attention-demo.css" />
<script src="/js/attention_demo.js"></script></p>

<h2 id="how-does-self-attention-work">How does self-attention work?</h2>

<p><figure><img src="/img/scaled-dot-prod-attention.png"
         alt="The Vaswani paper describes scaled dot product attention, which involves normalizing by the square root of the input dimension."/><figcaption>
            <p>The Vaswani paper describes scaled dot product attention, which involves normalizing by the square root of the input dimension.</p>
        </figcaption>
</figure>

This is the part where Vaswani delves into a database analogy with <strong>keys</strong>, <strong>queries</strong>, and <strong>values</strong>. Most online resources try to salvage this analogy. Personally, I always found this a bit confusing. What you need to know is that keys, values, and queries correspond to 3 matrices M<sub>k</sub>, M<sub>q</sub>, and M<sub>v</sub>, which are used in a dot product with the original input vectors.</p>

<p>In linear algebra terms this means multiplying the 1xd input vector by a matrix of size dxd. In neural network terms, this means passing the input vector through a full-connected layer. After M<sub>k</sub> and M<sub>q</sub> are multiplied, they are normalized by the square root of d<sub>k</sub>, a constant representing the dimension of the input vector.</p>

<h2 id="can-we-attend-to-multiple-parts-of-a-sentence">Can we attend to multiple parts of a sentence?</h2>

<figure><img src="/img/multi_head_attention.png"
         alt="Multi-headed attention means performing attention n times in parallel inside of an encoder block."/><figcaption>
            <p>Multi-headed attention means performing attention n times in parallel inside of an encoder block.</p>
        </figcaption>
</figure>


<p>Yes, that is called multi-headed attention. Its architecture is very similar, using  additional M<sub>k</sub>, M<sub>q</sub>, and M<sub>v</sub> matrices for each additional &ldquo;attention head&rdquo;. In the Vaswani paper they used 8 heads.</p>

<h2 id="how-do-transformers-compare-with-other-architectures-e-g-rnn-cnn">How do transformers compare with other architectures (e.g. RNN/CNN)?</h2>

<figure><img src="/img/attention_performance.png"
         alt="When the input sequence length n is lower than the input dimensionality d, self-attention is faster than recurrent neural networks. Self-attention is also easily parallelizable."/><figcaption>
            <p>When the input sequence length n is lower than the input dimensionality d, self-attention is faster than recurrent neural networks. Self-attention is also easily parallelizable.</p>
        </figcaption>
</figure>


<p>Generally speaking, RNNs are able to memorize but not parallelize, and CNNs are able to parallelize but not memorize. Transformers are able to do both.</p>

<p>The Vaswani paper outlines three main benefits:</p>

<ol>
<li><p>Computational complexity per layer. Self-attention layers are faster than recurrent layers when the input sequence length is smaller than the input vector dimensionality.</p></li>

<li><p>The opportunity to parallelize calculations. Each head in multi-headed attention can be computed separately in an encoder layer.</p></li>

<li><p>Easier to learn long-range dependencies. For many English sentences, especially fairly complex ones found in more scientific writings, the full context of a word cannot be learned from its immediate neighbors. Sometimes it can&rsquo;t even be found in the same sentence. However, even though most language models prior to the transformer had theoretically infinite input sequence lengths, in practice it was quite difficult for them to learn long-range dependencies. Because a transformer sees its whole input simultaneously, Vaswani argues, it is able to more easily learn those dependencies.</p></li>
</ol>

<h2 id="is-that-all">Is that all?</h2>

<p>No. In part two I will describe the encoder and decoder blocks, as well as the self-supervised training process.</p>
]]></content>
        </item>
        
        <item>
            <title>A new type of chess tournament</title>
            <link>https://lukesalamone.github.io/posts/qualitative-analysis-chess/</link>
            <pubDate>Sat, 08 Oct 2022 15:17:36 -0700</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/qualitative-analysis-chess/</guid>
            <description>This is part 2 of a paper I wrote for Ken Forbus&amp;rsquo; Qualitative Reasoning course, adapted for this blog. You can find a printable version of the paper here and part 1 here.
In the previous post I discussed the history of chess engines and why they don&amp;rsquo;t &amp;ldquo;think&amp;rdquo; like we think. Trading interpretability for computation cycles ultimately led to the engines we have today, fairly alien in nature and perhaps less pedagogically useful because of it.</description>
            <content type="html"><![CDATA[

<p><strong><em>This is part 2 of a paper I wrote for <a href="https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/forbus-ken.html">Ken Forbus</a>&rsquo; Qualitative Reasoning course, adapted for this blog. You can find a printable version of the paper <a href="/files/anthropomorphic-chess-evaluation-via-qualitative-analysis.pdf">here</a> and part 1 <a href="/posts/chess-engine-history">here</a>.</em></strong></p>

<p>In the previous post I discussed the history of chess engines and why they don&rsquo;t &ldquo;think&rdquo; like we think. Trading interpretability for computation cycles ultimately led to the engines we have today, fairly alien in nature and perhaps less pedagogically useful because of it. At the time, though, the goal was to beat human grandmasters by any means necessary, a great engineering feat that the field had been working on for decades.</p>

<p>This post contains two related proposals. The first is a chess engine tournament, unique in the type of engine which will be permitted to enter and likely to succeed. Importantly, the vast majority of engines currently holding the highest performance ratings will likely not be effective.</p>

<p>The second proposal is the outlines of a chess engine that is likely to be successful in this tournament, taking advantage of the highly qualitative nature of chess position evaluation. Although it is unlikely to perform as strongly against top-performing engines, there are several distinct advantages of such an engine. In short, there is likely to be a great deal of educational value as well as financial incentive driving the construction of highly successful qualitative chess engines.</p>

<h1 id="qualitative-chess-analysis">Qualitative chess analysis</h1>

<p>Qualitatively, there are many aspects to a chess game that may be captured. Let&rsquo;s take a look at the way a grandmaster analyzes a position. It will become quite apparent that at the highest levels, the qualitative aspects of position analysis dominate over quantitative aspects (i.e. the number and value of each piece).</p>

<p>In the selected lecture, Grandmaster Varuzhan Akobian details a game he played previously. At a key moment of the game, Akobian sacrificed his rook for a key pawn in the center of the board. The resulting position is reproduced for convenience in Figure 1.</p>

<figure><img src="/img/human_analysis_fig4.png"
         alt="Figure 1. Quantitative analysis would posit that black is winning due to his extra rook for white’s knight and pawn. However, qualitative analysis provides a more complete picture of black’s predicament."/><figcaption>
            <p>Figure 1. Quantitative analysis would posit that black is winning due to his extra rook for white’s knight and pawn. However, qualitative analysis provides a more complete picture of black’s predicament.</p>
        </figcaption>
</figure>


<p><a href="https://youtu.be/h80Mu4N6oYI?t=1515">His analysis starts at 25:15 in the video</a>:</p>

<blockquote>
<p>I would like you to spend a minute or two just to give me the evaluation of this position. It may not seem that clear because I’m down the exchange. [Novice chess players are taught that chess pieces have quantitative values, which may come into consideration when exchanging one piece for another. These values are measured in terms of pawns. Knights and bishops are generally understood to be worth 3 pawns, rooks are worth 5, and queens are worth 9.] I have a knight and a pawn for a rook. Rook is valued 5, knight and a pawn is 4. It may seem like I’m down a pawn here. But what do you think is the proper evaluation of this position?</p>

<p>&hellip;Basically white is very active. There are a few other things we can mention about white’s position, that it’s very strong. What else is very strong? White’s king is very safe, he cannot attack me. But how about the black king? Do you think the black king is very safe? [No.] For example, I could put my queen here [the e4 square] then I have a battery! Remember when we have a queen and a bishop on the same diagonal we call that a battery. And suddenly if I can deflect this queen [black queen on the g7 square] I will just go queen takes pawn, checkmate!</p>

<p>His dark square bishop is basically trapped behind his own pawns so it’s ineffective&hellip; . My bishop is very active&hellip; . And one more thing that you can mention. Passed pawn, exactly! And it’s a very strong passed pawn because with a knight on d6 very quickly [the pawn] will turn into [a queen].</p>

<p>&hellip; How much advantage does white have here? Big advantage, slight advantage, maybe winning? &hellip; We’re not going to use Houdini [a chess engine], Houdini will probably say black is slightly worse. But in practical play, I would be very comfortable to play this against anybody, and pretty comfortable I can win this position for white.</p>
</blockquote>

<p>Note that quantitative analysis is almost entirely absent from GM Akobian’s evaluation. Towards the beginning he mentions that he has sacrificed his rook for a knight and a pawn, and consequently is at a material deficit. However, he quickly discards this shallow evaluation, going so far as to label his subsequent qualitative evaluation as the “proper” evaluation.</p>

<p>GM Akobian goes on to mention several other qualitative features of the position which are difficult to assign quantitative value to. Firstly, the activity of his pieces means that it is much easier to play the position as white because his pieces are on better squares, including some deep in black’s half of the board. The lack of activity is mentioned later on, noting that black’s bishop is essentially trapped behind his own pieces.</p>

<p>King safety is another difficult thing to quantify. In the given position, it is difficult to find a way that black can even check the white king. Moving the d8 rook to b1 will take 2 moves, and even then the b1 square is guarded by the bishop on d3. So the white king is indeed quite safe. In contrast black king is quite vulnerable, guarded mainly by the black queen, who is herself vulnerable to deflection or direct attack. (Deflection is a chess tactic which involves “distracting” an opponent’s piece which plays an important defensive role. For example, a piece which is defending two pieces simultaneously may be deflected by capturing one of the defended pieces.)</p>

<p>GM Akobian emphasizes the weakness of black’s king by sketching out a simple game-winning checkmate plan: arrange the bishop and queen in a battery which attacks the h7 pawn, deflect the black queen, and deliver checkmate with the queen by taking the h7 pawn. Although it is not immediately clear how to implement the plan, this type of simple plan creates a well-defined long-term “threat” that black must contend with.</p>

<p>Another threat he mentions is encompassed by white’s passed pawn on c5. This pawn may become a queen, which would become an insurmountable advantage for white. Therefore, this threat is another long-term vulnerability for black. (A passed pawn is a pawn which cannot be stopped or attacked by an opponent’s pawns. This occurs when there are no opponent pawns in the “file” (vertical column) of the pawn, as well as the file to the left and the right, if applicable. For example, a pawn in the C file is a passed pawn if there are no opponent pawns in the B, C or D files. A pawn in the H file is passed if there are no opponent pawns in the G or H files.)</p>

<p>Finally, note that GM Akobian does not assign a quantitative value to the board position, but rather a “very comfortable to win” assessment. Very little of this analysis involves quantities, but rather qualitative situations which must be dealt with. Consequently, it seems that qualitative reasoning is an ideal tool which a chess engine might use.</p>

<h2 id="qualitative-analysis-is-more-human">Qualitative analysis is more human</h2>

<p>The nature of expert-level perception described by GM Akobian was studied directly in a <a href="https://drive.google.com/file/d/1gvAMm39EVVN9bPnWVOdrAXU_MwDqkf0Z/view?usp=sharing">1973 paper</a> by Chase and Simon. Participants at three different levels of chess ability (a master-level player, an experienced club-level player, and a novice) were asked to complete two chess-related cognitive tasks. The first was a perception task, requiring him to reproduce a chess position on an adjacent board as quickly as possible, with the model board in plain view. The second task was a memory task, requiring participants to reproduce a position from memory after viewing it for only 5 seconds.</p>

<p>Importantly, the perceptual study attended to chess players’ tendencies to “chunk” the board position as they reproduced positions, tending to remember groups of interrelated pieces. These pieces tended to have relationships which the authors characterized in five ways: a piece attacks another, a piece defends another, two pieces are adjacent, two pieces are the same color, two pieces are the same type.</p>

<p>The results of the study found that “the C, S, and null relations are low because subjects are placing pieces which usually have multiple relations. Thus, from the within-glance relations, it appears that subjects are noticing the pawn structure, clusters of pieces of the same color, and attack and defense relations over small spatial distances.” In other words, it seems likely that human players use the qualitative relationships between pieces to remember the board.</p>

<h1 id="a-qualitative-chess-engine">A qualitative chess engine</h1>

<p>It is unlikely that a qualitative chess engine will be able to entirely do away with the basic structural algorithm involved in chess calculations, i.e. minimax. We would like our qualitative engine to calculate in a way most similar to humans, and thus will require some level of ply depth to the calculations. However, a qualitative engine will have a much stronger sense of the “flow” of the game, and will thus explore fewer branches. Rather than considering each position as discrete, a qualitative engine should note how each move guides the evolution of the chess board position.</p>

<p>It is important to note that a qualitative chess engine may not be the most computationally efficient, a factor which was the primary motivation during the period of time when top chess engines needed to be run on supercomputers and every ounce of performance needed to be squeezed out of the machine. A qualitative engine should instead favor explainability over performance whenever possible. Instead, an engine should produce a good explanation of which moves were considered and why a particular move was chosen.</p>

<p>More modern qualitative research can improve upon Wilkins’ knowledge-based PARADISE approach. It is important to recognize that his knowledge base is quite similar in nature to the FAC component of the retrieval model <a href="https://groups.psych.northwestern.edu/gentner/papers/GentnerForbus91.pdf">presented</a> by Forbus et al. in 1995, but does not take advantage of the performance speedups presented there. Because of the high number of positional examples <a href="https://database.lichess.org/#puzzles">available online</a>, there is a huge opportunity for a performant analogical retrieval system at present. The MAC/FAC retrieval system could pay huge performance dividends in retrieval if applied to this problem.</p>

<p>Specifically, the Lichess database referenced above contains 1,737,489 chess “puzzles” as of the time of writing. A chess puzzle is simply a chess board position in which players are encouraged to find the best move. Each puzzle relates to one or more chess “themes” (e.g. “mate in 1”, “pin”, “discovered attack”, etc.), analogous to Wilkins’ concepts outlined above. Each puzzle also includes the best move to make in the position. Some research will need to be done to derive meaning from this best move, relating it by analogy to the current position being evaluated by the engine.</p>

<p>Qualitative spatial calculi may also be used to construct more psychologically plausible models of chess positions than simply noting which pieces occupy which squares, seeking to emulate the models suggested by Chase and Simon. Chess pieces have intricate relationships which can be captured, and which change whenever a piece moves to another square. Importantly, however, not all relationships are affected by the movement of a single chess piece, suggesting that performance gains may be realized by recomputing only those relationships which have changed.</p>

<p>It is likely that low-level piece relationships may give rise to higher level relationships and tactics. For example, the concept of “capturing the defender” arises from the concept of attacking a piece A which defends B, which works when pieces A and B are attacked by pieces C and D of opposing colors. And in the case of Figure 2 below, a defender may be “deflected” to win the piece it is defending. Defensive relationships may be thought of in a chain or directed graph, with each piece defending another and the safety of a piece being considered in relation to its connection to a defensive group.</p>

<figure><img src="/img/human_analysis_fig5.png"
         alt="Figure 2. White to move. From this image, many basic piece relationships are apparent with only 8 pieces left on the board. Importantly, the black king is defending the black queen. The black queen is also being attacked by the white queen, and is attacking the white queen. White’s queen is undefended, a state sometimes referred to as hanging. These relationships reveal the opportunity for a tactic. White can simultaneously move his rook to attack black’s king (danger levels) and take advantage of the defensive connection between the black king and queen with the move rook to b8. This forces black to move his king, removing the defense of his queen. In this position, black’s queen can be freely captured by white’s queen. Due to the mobility advantages of a queen over a rook, this is a favorable move sequence for white."/><figcaption>
            <p>Figure 2. White to move. From this image, many basic piece relationships are apparent with only 8 pieces left on the board. Importantly, the black king is defending the black queen. The black queen is also being attacked by the white queen, and is attacking the white queen. White’s queen is undefended, a state sometimes referred to as hanging. These relationships reveal the opportunity for a tactic. White can simultaneously move his rook to attack black’s king (danger levels) and take advantage of the defensive connection between the black king and queen with the move rook to b8. This forces black to move his king, removing the defense of his queen. In this position, black’s queen can be freely captured by white’s queen. Due to the mobility advantages of a queen over a rook, this is a favorable move sequence for white.</p>
        </figcaption>
</figure>


<h2 id="applications-of-a-qualitative-chess-engine">Applications of a qualitative chess engine</h2>

<p>There are many benefits to reopening the pursuit of qualitative reasoning in chess. The first and most clear value proposition is that qualitative reasoning is likely to serve as a more plausible model for how humans think about the game. This is evidenced by the fact that as Chase and Simon found, chess players do not “see” the whole board at once, but rather in chunks of interrelated pieces. Even if the details of human mental models differ slightly from the implementation of a qualitative reasoning engine, it will be able to provide a traceable account of its decision-making process, an important step towards explainability.</p>

<p>As we saw in <a href="/posts/chess-engine-history">part 1</a>, current top chess engines reason about chess in ways that are quite contrary to human intuition. Stockfish uses full-width search, considering each move in each position without prejudice and assigning numerical values to each position. As we saw from the analysis from GM Akobian, qualitative evaluations are far more meaningful to humans.</p>

<p>ther chess engines approach chess in an even more alien way. Specifically, it is unlikely that any engine which makes heavy use of neural evaluation functions will model human-derived organic strategies in ways which chess players will recognize. At the far end of this spectrum is the fully neural Maia chess engine, but even Lc0’s Monte-Carlo tree search precludes consideration for cognitive plausibility.</p>

<p>Qualitative chess engines which are able to better reproduce the types of chess reasoning used by top human chess players are also likely to serve as better pedagogical tools for those interested in studying chess. This applies at every level, from beginner to grandmaster. The skill level of such a chess engine would be quite easily tunable simply by disabling more advanced knowledge from the knowledge base.</p>

<p>This is a far more natural method of “handicapping” than the search depth limitations used in current chess engines. Each piece of knowledge becomes a tunable parameter to the engine. As students learn concepts, the corresponding representations in the knowledge base could be enabled, allowing for gradual learning in a far more accessible way. In fact, it is likely true that a qualitative chess engine could outperform human grandmasters (who often teach chess to others) in this respect.</p>

<p>Finally, it is likely that a qualitative engine would become a key component of a first line of defense against cheating in chess. Most cheating is performed by using assistance from a chess engine during online games with unsuspecting opponents. Consulting a functionally omniscient computer program can thus provide a cheater with a theoretically insurmountable advantage.</p>

<p>In an <a href="https://youtu.be/bmIFdrUVHXw?t=2280">interview</a> with the Perpetual Chess Podcast, Chris Callahan of the popular chess website LiChess.org stated that the majority of employees of the website work primarily to detect cheaters and yet the problem still persists. By exploiting the difference between conventional full-width engines like Stockfish and a qualitative evaluation, those working to detect cheaters will be better equipped to detect “suspicious” moves. However, qualitative chess engines are unlikely to be able to completely replace human moderation.</p>

<h1 id="nerf-the-engine">Nerf the Engine!</h1>

<p>A computer chess tournament be held between chess engines can encourage the type of reasoning and gameplay which resembles human games. However, because we are not interested in the best overall chess engine, but one which can reason like a human might, the rules of the tournament will be adjusted in several key ways to discourage brute-force computational methods. We already know that calculating millions of positions can find the optimal move. But what happens when an engine is limited to e.g. 1000 positions?</p>

<p>Because we expect few entrants in early iterations of this special tournament, engineering an automatic enforcement mechanism for the limitations stipulated in this document are likely to be unnecessary. Engine compliance may simply be verified through manual inspection. Future iterations may include further safeguards, potentially separating the position evaluation function and directly counting the number of invocations while arbitrating the tournament to directly verify compliance.</p>

<h2 id="position-limitation">Position limitation</h2>

<p>Firstly, competing chess engines will be limited in the number of board positions they can evaluate during any one move. Because human grandmasters evaluate around 100 positions before making a move, the tournament arbitration system will artificially impose this limitation on all competing engines.</p>

<p>This cap immediately creates an issue for full-width chess engines because of chess’ high branching factor. Were an engine to evaluate each possible move, it would perform quite poorly in board positions with many possible plies and replies available, rarely reaching a depth of more than 2 or 3. As a result, any engine which naively assesses a chess board would perform quite poorly in this setup.</p>

<p>The practical upshot of the position limitation is that the engine will be incentivized to gather as much relevant information about a position as possible rather than optimizing for the maximum number of positions.</p>

<h2 id="position-saliency">Position saliency</h2>

<p>Additionally, engines will be required to implement scheduling logic which takes the time remaining into consideration. While this creates the immediate problem of how an engine should allocate its time, it creates the ancillary challenge of evaluating a position’s quiescence. Positions which are “quiet” and have few forcing moves require less evaluation than positions in which there are many non-forcing moves.</p>

<p>This requirement immediately motivates a qualitative chess engine to recognize the futility of falling prey to the Horizon Effect. The Horizon Effect causes engines to waste many position calculations pursuing delaying moves which amount to hopeless rabbit trails. Instead, an engine should recognize that quiescence has to do with the <em>relationships between pieces</em>. Humans understand this and can quite quickly see the futility of a move and terminate their search. A qualitative analysis which takes this factor into consideration will be able to save a great deal of position calculations, behaving more like a human player.</p>

<h1 id="conclusion">Conclusion</h1>

<p>Given that computers have achieved and sustained superhuman capabilities in the domain of chess, the next frontier is not in building increasingly strong engines, but harnessing the present computational power to reason about the game in ways that humans do. Qualitative reasoning can provide novel and intuitive ways to reason about previously seen moves and think about the game.</p>
]]></content>
        </item>
        
        <item>
            <title>The Chess Engine&#39;s Final Horizon</title>
            <link>https://lukesalamone.github.io/posts/chess-engine-history/</link>
            <pubDate>Fri, 07 Oct 2022 20:17:21 -0700</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/chess-engine-history/</guid>
            <description>This is part 1 of a paper I wrote for Ken Forbus&amp;rsquo; Qualitative Reasoning course, adapted for this blog. You can find a printable version of the paper here and part 2 here.
Computers that play chess, otherwise known as chess engines, have existed since at least the late 1940s. Because the game was said to require the perfect combination of planning, strategy, psychology, and calculation, chess was once thought to be an activity directly correlated with intelligence, and that only a truly intelligent computer should be able to defeat humans.</description>
            <content type="html"><![CDATA[

<p><strong><em>This is part 1 of a paper I wrote for <a href="https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/forbus-ken.html">Ken Forbus</a>&rsquo; Qualitative Reasoning course, adapted for this blog. You can find a printable version of the paper <a href="/files/anthropomorphic-chess-evaluation-via-qualitative-analysis.pdf">here</a> and part 2 <a href="/posts/qualitative-analysis-chess">here</a>.</em></strong></p>

<p>Computers that play chess, otherwise known as chess engines, have existed <a href="https://www.youtube.com/watch?v=wrxdWkjmhKg">since at least the late 1940s</a>. Because the game was said to require the perfect combination of planning, strategy, psychology, and calculation, chess was once thought to be an activity directly correlated with intelligence, and that only a truly intelligent computer should be able to defeat humans. However, as a recent chess.com <a href="https://drive.google.com/file/d/11IokKgTVSXdpYEzAuyViIleSZ_2wl0ag/view">report</a> explains, computers are now far stronger than humans:</p>

<blockquote>
<p>Human chess and computer chess are different, even at the highest levels. The best humans play at an Elo rating of 2800. “Stockfish,” the most powerful chess engine, has an estimated rating of more than 3500. In a theoretical match between World Champion Magnus Carlsen vs. Stockfish, we estimate that it is most likely that Magnus Carlsen would lose every single game—no wins and no draws.</p>
</blockquote>

<p>The supremacy of Machine over Man should be marked as a great triumph in artificial intelligence, and in a broad sense it is. However, I would argue that playing skill should not be the only goal, and a truly useful engine should be interpretable as well. We&rsquo;ve already &ldquo;solved&rdquo; the problem of playing chess at an extremely high level. Yet for historical reasons, interpretability was sacrificed for the sake of speed and playing strength. However, these tradeoffs may not make as much sense today. <a href="/posts/qualitative-analysis-chess">Part 2</a> discusses opportunities and clear benefits for reevaluating those decisions moving forward.</p>

<h1 id="background">Background</h1>

<p>The general algorithm for performing search in zero-sum perfect information games like chess is known as minimax. The algorithm attempts to find the move which minimizes the opponent&rsquo;s maximum (i.e. best) move. Minimax visualizes the possible future state of the game as a tree, and the value of each node in the tree as a function of the nodes following from it. If you could compute the full game tree (in simple games like tik-tac-toe this is possible), you would be able to enumerate all possible terminal nodes, and propagate the result (1 for a win, -1 for a loss, 0 for a draw) back up the tree.</p>

<p>That&rsquo;s not possible in chess. The game tree is too big. Instead, the value of many of the nodes needs to be <em>estimated using heuristics</em> so that they can be propagated upwards. Therefore a fast and accurate static evaluation function is critical. (However, since the problem is recursive, the problem of creating a perfectly accurate static evaluator is as hard as evaluating that node&rsquo;s entire associated game tree.)</p>

<p>Minimax is often augmented with “alpha-beta pruning” to reduce the number of positions which will be evaluated. This effectively cuts the <a href="https://imgur.com/wRZINqS">computational complexity exponent in half</a> by removing from consideration those branches which cannot affect the final result.</p>

<p>Other challenges in minimax evaluation exist as well. In particular, a phenomenon dubbed the “Horizon Effect” is a peculiar failure mode of minimax searches. The Horizon Effect was <a href="https://drive.google.com/file/d/1XHhMQUgZHhD8Be7Klrl7_RMHQm8B9Zlc/view">first described</a> by grandmaster and computer scientist Hans Berliner in 1975. His illustration of the problem (his Figure 1.3) is reproduced in Figure 1. Algorithms that do not account for the Horizon Effect will try to “push” bad outcomes of their search beyond their search horizon, instead opting to make hopeless moves which only serve to delay the inevitable. As humans we intuitively understand this illusion.</p>

<figure><img src="/img/human_analysis_fig1.png"
         alt="Figure 1. White to move. Here, white’s bishop on a4 is doomed, attacked by black’s pawn on b5. White could delay the inevitable by moving his bishop to b3, but then black simply seals the bishop’s fate with pawn to c4. In that position, white does not have time to save his bishop, and it will be captured no matter what on the next move by the pawn on c4. Due to the Horizon Effect, at a limited depth white will not recognize this and will play hopeless moves like pawn from e4 to e5, temporarily attacking the knight but easily parried by capturing with the pawn on d6. This phenomenon is deemed the “Horizon Effect” because by pushing negative outcomes beyond the “horizon” of calculation depth, the engine is able to trick itself into believing that the problem doesn’t exist. A true case of “see no evil”."/><figcaption>
            <p>Figure 1. White to move. Here, white’s bishop on a4 is doomed, attacked by black’s pawn on b5. White could delay the inevitable by moving his bishop to b3, but then black simply seals the bishop’s fate with pawn to c4. In that position, white does not have time to save his bishop, and it will be captured no matter what on the next move by the pawn on c4. Due to the Horizon Effect, at a limited depth white will not recognize this and will play hopeless moves like pawn from e4 to e5, temporarily attacking the knight but easily parried by capturing with the pawn on d6. This phenomenon is deemed the “Horizon Effect” because by pushing negative outcomes beyond the “horizon” of calculation depth, the engine is able to trick itself into believing that the problem doesn’t exist. A true case of “see no evil”.</p>
        </figcaption>
</figure>


<h2 id="branching-factor">Branching Factor</h2>

<p>In any given position, there may be many possible moves. By some estimates, chess has an average branching factor of around 30, with other estimates putting the number around 40. It isn’t easy to find a concrete value for the branching factor of chess. One source claims (without citation) that the branching factor may be <a href="https://courses.csail.mit.edu/6.034f/ai3/rest.pdf">up to 40</a> (see pg. 117). However, a more recent statistical <a href="https://chess.stackexchange.com/a/24325">analysis</a> of 2.5 million chess games put the real number closer to 31.</p>

<figure><img src="/img/avg_moves_available.png"
         alt="The branching factor depends on the stage of the game; middle games have far more available moves than end games. Because not all games are the same length, shorter games will tend to have higher average branching factors than longer ones. Barnes&amp;rsquo; graphic is reproduced above."/><figcaption>
            <p>The branching factor depends on the stage of the game; middle games have far more available moves than end games. Because not all games are the same length, shorter games will tend to have higher average branching factors than longer ones. Barnes&rsquo; graphic is reproduced above.</p>
        </figcaption>
</figure>


<p>In any case, it is possible to dramatically reduce the branching factor by employing “selective search”, which means excluding nodes from recursive tree search altogether. Reducing the number of possible moves that will be explored in any given position promises dramatic computational speedups, allowing the tree to be searched deeper at the expense of width. Given the slow speeds (by contemporary standards) of early computers, the allure of such a technique should be clear. It is also a more psychologically plausible way of playing, since most players quickly rule out ridiculous seeming moves and focus on the most promising ones.</p>

<h1 id="early-chess-engines">Early Chess Engines</h1>

<p>An early attempt to narrow the search tree was proposed by Berliner, who in 1975 devised CAPS-II which utilized a tree search algorithm with five total “reference levels” including ALPHA and BETA. His paper also was cognizant of the work of Chase and Simon from two years prior, recognizing the need for a bottom-up board representation. Unfortunately, his board representation was largely geometrical and included little in the way of qualitative relationships between pieces. Nevertheless, the resulting program was able to solve tactical puzzles in a quite impressive manner for the time.</p>

<p>Another example of the use of such a narrowed search tree is <a href="https://drive.google.com/file/d/1I54sVUe4Ybkln5hy5_MlZBnmrZBqbj2l/view?usp=sharing">PARADISE (PAttern Recognition Applied to DIrecting SEarch)</a>. This system used a knowledge base containing 200 “production rules” to match board patterns and determine the best move at any time. An example production rule is shown in Figure 2.</p>

<figure><img src="/img/human_analysis_fig2.png"
         alt="Figure 2. A sample production rule from the PARADISE knowledge base. This production rule detects and acts upon opponent’s trapped pieces. A trapped piece is identified as a non-pawn defensive piece which cannot move to another square without being captured and is not already attacked. Finally, the production rule describes the threat of this action (winning the piece) and how likely it is to succeed."/><figcaption>
            <p>Figure 2. A sample production rule from the PARADISE knowledge base. This production rule detects and acts upon opponent’s trapped pieces. A trapped piece is identified as a non-pawn defensive piece which cannot move to another square without being captured and is not already attacked. Finally, the production rule describes the threat of this action (winning the piece) and how likely it is to succeed.</p>
        </figcaption>
</figure>


<p>Because the tree search was narrower, the system was able to search to higher depths. Still, because of hardware limitations of the time, PARADISE was extremely slow, generating only 109 nodes in 20 minutes of search time. (Current chess engines evaluate millions of nodes per second.)</p>

<p>PARADISE executes static analysis on a position using its many production rules with the ultimate goal of creating and executing a plan. Matched production rules post concepts to the database in addition to information about the intentions of the concept and its likelihood of success.</p>

<p>The program is then able to use this information to form a plan, which is a set of actions for the side to move, along with corresponding plans for each defensive alternative. Because there may be many potential alternatives at each move, this plan includes many branches.</p>

<figure><img src="/img/human_analysis_fig3.png"
         alt="Figure 3. White to move, PARADISE has produced a plan which involves checking the black king by moving the knight to g5, then checking the black king by moving the rook to d7. Depending on black’s next move white will then try to either capture the queen on d4 or the rook on d7."/><figcaption>
            <p>Figure 3. White to move, PARADISE has produced a plan which involves checking the black king by moving the knight to g5, then checking the black king by moving the rook to d7. Depending on black’s next move white will then try to either capture the queen on d4 or the rook on d7.</p>
        </figcaption>
</figure>


<p>An example of such a plan is reproduced in Figure 3 above. (An interesting sidebar about this position is that based upon Stockfish analysis, this position results in inescapable checkmate for black within 15 moves. That is, even with optimal play, black will be checkmated in 15 or fewer moves. The winning sequence begins with white moving his queen to e5, exploiting the pin on the f6 pawn from the f1 rook.)</p>

<p>Searching only selective lines is more difficult to implement than full-width search. Further, selective searches may miss important continuations of a position, causing the computer to select an incorrect move. It was for this reason that Berliner himself, an original proponent for the application of strict logical rules in chess, decided to seek <a href="https://www.nytimes.com/2017/01/16/business/hans-berliner-master-chess-player-and-programmer-dies-at-87.html">brute-force search methods</a> instead.</p>

<h1 id="modern-chess-engines">Modern Chess Engines</h1>

<p>Stockfish is currently the #1 ranked chess engine in the world. Stockfish is open-source and performs a “full width” search on the game tree. Leaf nodes are evaluated using either a classical, hand-crafted evaluation heuristic, or more recently, a neural network evaluator called NNUE. The classical evaluation function uses a set of around 30 factors weighted empirically using a dedicated testing framework called Fishtest. Altogether, the project has <a href="https://github.com/official-stockfish/Stockfish">around 200 contributors</a>. Optimized for speed, Stockfish can evaluate around <a href="https://chessify.me/blog/nps-what-are-the-nodes-per-second-in-chess-engine-analysis">millions of nodes per second</a> on a typical 4-core computer.</p>

<figure><img src="/img/stockfish_starting_pos.png"
         alt="Stockfish analyzing the starting position on my laptop at around 1 million nodes per second."/><figcaption>
            <p>Stockfish analyzing the starting position on my laptop at around 1 million nodes per second.</p>
        </figcaption>
</figure>


<p>Neural networks may also be used more directly. <a href="https://lczero.org/dev/wiki/technical-explanation-of-leela-chess-zero/">Leela Chess Zero</a>, also known as Lc0 is another open-source chess engine, modeled after DeepMind’s AlphaZero chess engine. Lc0 uses Predictor + Upper Confidence Bound tree search (PUCT) to search its game tree. Leela evaluates new nodes by iteratively choosing moves from a probability distribution until it reaches an unexplored node. At that point, Leela&rsquo;s neural network estimates the node&rsquo;s value and propagates that value back up the tree. PUCT is very similar to Monte Carlo Tree Search, but with game “rollouts” replaced by neural network evaluations.</p>

<p>One other notable engine is <a href="https://arxiv.org/abs/2006.01855">Maia Chess</a>. Maia is interesting for two reasons. Unlike most other engines, Maia&rsquo;s objective is to model human behavior rather than to perform optimally. This is interesting because Maia is in many cases trained on suboptimal data. The other notable feature about Maia is the unusual manner in which moves are selected: instead of performing any type of tree search at all, the engine simply returns the neural network&rsquo;s static evaluation of the current board position.</p>

<p>Suffice it to say, however, that despite computation no longer being a significant limitation, prior qualitative approaches still have garnered relatively little attention.</p>

<h1 id="conclusion">Conclusion</h1>

<p>Engineers have historically sought to build the most powerful chess engine possible. With the singular goal of defeating human (and now computer) opponents, explainability was an expedient but understandable tradeoff to make. In the present day, however, there is no longer a question of whether humans or computers are superior chess players. It seems quite clear that the time has come to revisit some of the tradeoffs made in the past.</p>

<p>In <a href="/posts/qualitative-analysis-chess">part 2</a> I will discuss some clear benefits of a more interpretable chess engine, and some possible routes of getting there.</p>
]]></content>
        </item>
        
        <item>
            <title>Alphabet Chess</title>
            <link>https://lukesalamone.github.io/posts/alphabet-chess/</link>
            <pubDate>Fri, 10 Jun 2022 23:56:14 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/alphabet-chess/</guid>
            <description>Chess has been played in different forms since the seventh century, and in its modern form since the nineteenth century. Opening theory, i.e. the study of the best moves to begin the game with, has been developing since then.
The EGG Opening I was inspired the other day after watching a video by Eric Rosen called &amp;ldquo;Quadruple Egg&amp;rdquo;. Chess boards are usually notated from left to right with the letters A through H.</description>
            <content type="html"><![CDATA[

<p>Chess has been played in different forms since the seventh century, and in its modern form since the nineteenth century. Opening theory, i.e. the study of the best moves to begin the game with, has been developing since then.</p>

<h1 id="the-egg-opening">The EGG Opening</h1>

<p>I was inspired the other day after watching a <a href="https://www.youtube.com/watch?v=J6G3cP991Yc">video</a> by Eric Rosen called &ldquo;Quadruple Egg&rdquo;. Chess boards are usually notated from left to right with the letters A through H. (These columns are called &ldquo;files&rdquo;.) The &ldquo;egg&rdquo; opening, therefore, involves moving the E pawn, then the G pawn twice, spelling the word &ldquo;egg&rdquo;. It&rsquo;s an extremely unconventional opening, but how bad is it compared with unrestricted openings?</p>

<h1 id="what-are-the-best-i-e-least-bad-alphabetic-openings">What are the best (i.e. least bad) alphabetic openings?</h1>

<p>We can call the Egg Opening one version of an alphabetical chess opening: an opening where the piece moved must start on a file corresponding to the next letter in a given word. In this definition, the pieces don&rsquo;t have to be pawns.</p>

<p>This raises an interesting question: what are the best and worst alphabetical openings? Under engine evaluation, they will all be losing, because the opponent is not limited in which piece they can move. However, as we will see, some are much worse than others.</p>

<h2 id="four-letters">Four letters</h2>

<p>English doesn&rsquo;t have an official dictionary, so I chose <a href="https://gist.github.com/paulcc/3799331">this list</a> of four letter words. From there, we need to eliminate all words with letters after H. This leaves us with <a href="https://gist.github.com/lukesalamone/eab96ddde6eb326b8e339c41f5f52bda">37 words</a>. Don&rsquo;t ask me what they all mean:</p>

<pre><code>abbe, abed, aced, ache, aged, agha,
baba, babe, bach, bade, bead, beef,
cafe, cage, ceca, cede, chad, chef,
dace, dada, dead, deaf, deed,
each, edda, edge, egad,
face, fade, feed,
gaff, gaga, gage, geed, ghee,
head, heed
</code></pre>

<p>A simple way of measuring how bad each of these is would be to evaluate them against Stockfish to see how badly we&rsquo;re losing after playing these moves. In this simulation, we play as white each time.</p>

<p><script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.7.2/Chart.js"></script>
<script src="/js/heatmap.js"></script>
<script src="/js/alpha_chess.js"></script>
<canvas id="myChart" style="background-color: #0000"></canvas></p>

<p>This measurement approach has a problem, though: Stockfish is assuming that white can make any move, and playing black&rsquo;s moves accordingly. In reality, white&rsquo;s moves are extremely restricted. If black knew about this restriction, he would be much more aggressive.</p>

<p>Further, if black were to discover which word white is playing, he could punish white. And even if black doesn&rsquo;t know exactly which word white is playing, black could eliminate moves which definitely can&rsquo;t form English words. For example, if white&rsquo;s first move is H, the next move will definitely be E.</p>

<h1 id="alphabetical-chess">Alphabetical Chess</h1>

<p>What if both players were required to play their first four moves from dictionary words? And what if those words were pre-assigned but secret to each other? For example, if each player drew a card from a deck. In this game, if a player is unable to make a move with a piece of the correct letter, he loses.</p>

<p>There are a few interesting features of this chess variant:</p>

<ol>
<li>Your starting word will have a big impact on how the game will turn out.</li>
<li>Figuring out your opponent&rsquo;s word will give you a big advantage.</li>
<li>Checks and other attacks can be devastating. You may not be able to respond to the attack, and checks may end the game immediately.</li>
</ol>

<p>In that case, it isn&rsquo;t necessarily true that the best and worst words from above will still be the best and worst when pitted against another randomly chosen word. It&rsquo;s a rock-paper-scissors situation: depending on which word A you happen to get, there is another word B which best counters your word, and another word C which your word best counters.</p>

<p><link rel="stylesheet" href="/css/heatmap.css" />
<div id="heatmap" style="margin-bottom: 50px"></div></p>

<p>Here the y-axis shows the word which was played with the white pieces, and the x-axis shows the word played with black.</p>
]]></content>
        </item>
        
        <item>
            <title>How to Create a Custom Pytorch Dataloader</title>
            <link>https://lukesalamone.github.io/posts/pytorch-dataloader/</link>
            <pubDate>Thu, 28 Apr 2022 18:22:07 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/pytorch-dataloader/</guid>
            <description>First, create a custom dataset class.
from torch.utils.data import Dataset, DataLoader class CustomDataset(Dataset): def __init__(self, features, labels): assert len(features) == len(labels) self.features = features self.labels = labels def __len__(self): return len(self.features) def __getitem__(self, idx): return self.features[idx], self.labels[idx]  Next, create a custom dataloader where we specify the batch size.
features, labels = load_data() # features &amp;amp; labels must have equal lengths # e.g. features = [[1,2,3],[4,5,6]] # labels = [7,8] dataset = CustomDataset(features, labels) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)  Finally, iterate over the dataloader during training.</description>
            <content type="html"><![CDATA[<p>First, create a custom dataset class.</p>

<pre><code class="language-python">from torch.utils.data import Dataset, DataLoader

class CustomDataset(Dataset):
  def __init__(self, features, labels):

    assert len(features) == len(labels)
    self.features = features
    self.labels = labels

  def __len__(self):
    return len(self.features)

  def __getitem__(self, idx):
    return self.features[idx], self.labels[idx]
</code></pre>

<p>Next, create a custom dataloader where we specify the batch size.</p>

<pre><code class="language-python">features, labels = load_data()

# features &amp; labels must have equal lengths
# e.g. features = [[1,2,3],[4,5,6]]
#      labels = [7,8]

dataset = CustomDataset(features, labels)
dataloader = DataLoader(dataset,
                        batch_size=batch_size,
                        shuffle=True)
</code></pre>

<p>Finally, iterate over the dataloader during training.</p>

<pre><code class="language-python">for epoch in range(num_epochs):
  for x, y in train_dataloader:
    # do stuff
</code></pre>
]]></content>
        </item>
        
        <item>
            <title>How to Zip and Unzip a tar.gz File</title>
            <link>https://lukesalamone.github.io/posts/how-to-tar-untar-file/</link>
            <pubDate>Wed, 30 Mar 2022 20:05:26 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/how-to-tar-untar-file/</guid>
            <description>If you want to extract a tar archive
tar -xf archive.tar.gz  If you want to compress a directory
tar -czvf archive.tar.gz /path/to/directory  That&amp;rsquo;s all.</description>
            <content type="html"><![CDATA[<p>If you want to extract a tar archive</p>

<pre><code class="language-console">tar -xf archive.tar.gz
</code></pre>

<p>If you want to compress a directory</p>

<pre><code class="language-console">tar -czvf archive.tar.gz /path/to/directory
</code></pre>

<p>That&rsquo;s all.</p>
]]></content>
        </item>
        
        <item>
            <title>Paper Summary: Defending Against Neural Fake News</title>
            <link>https://lukesalamone.github.io/posts/grover-paper-summary/</link>
            <pubDate>Sun, 19 Sep 2021 20:13:09 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/grover-paper-summary/</guid>
            <description>Defending Against Neural Fake News by Zellers et al. presents a model for controllable text generation called Grover. This model can be used to create highly believable computer-generated news articles. The authors present this paper as a method of detecting and preventing the spread of fake news. They claim their model is 92% accurate at detecting fake news stories, partially due to artifacts that generators include in the generated text.</description>
            <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1905.12616"><em>Defending Against Neural Fake News</em></a> by Zellers et al. presents a model for controllable text generation called Grover. This model can be used to create highly believable computer-generated news articles. The authors present this paper as a method of detecting and preventing the spread of fake news. They claim their model is 92% accurate at detecting fake news stories, partially due to artifacts that generators include in the generated text.</p>

<p>Grover functions in an adversarial manner:<br />
 - an “adversary” generates synthetic stories<br />
 - a “discriminator” identifies fake stories</p>

<p>Current generative models are fairly good at creating realistic-looking text, but largely lack the ability to be controlled via tunable parameters. In contrast, Grover models news stories as distributions like (<em>domain</em>, <em>date</em>, <em>authors</em>, <em>headline</em>, <em>body</em>) to be sampled from. Adversaries specify domain, date, and headline, and the generator creates the body and author as well as a more appropriate headline.</p>

<figure><img src="/img/grover-prob-distribution.png"
         alt="Grover samples from a joint distribution of the parts of a news article."/><figcaption>
            <p>Grover samples from a joint distribution of the parts of a news article.</p>
        </figcaption>
</figure>


<p>Grover is built using a transformer architecture similar to BERT and GPT. Depending on the model size, the number of parameters varies from similar to GPT to on par with GPT-2. The authors use the RealNews corpus, resulting in 120 gigabytes of total file size. Training took 2 weeks on 256 TPU v3 cores.</p>

<figure><img src="/img/grover-vs-human.png"
         alt="Grover is actually better at writing synthetic articles than humans are according to authors."/><figcaption>
            <p>Grover is actually better at writing synthetic articles than humans are according to authors.</p>
        </figcaption>
</figure>


<p>The results of the authors&rsquo; subjective experiments show that humans have a hard time identifying Grover-written propaganda. Using Mechanical Turk, articles were rated according to stylistic consistency, content sensibility, and overall trustworthiness. Grover, on the other hand, turns out to be a fairly good discriminator.</p>

<p>The authors also tested GPT2, BERT, and FastText on the task of classifying news as human or synthetic. The researchers set up the experiment in an unpaired setting (give a human/synthetic classification for a single article) and a paired setting (determine which is the human and which is the synthetic between 2 articles). Unsurprisingly, the paired setting was far easier. Also unsurprisingly, the larger models perform better at discriminating when paired with smaller generators.</p>

<figure><img src="/img/grover-discrimination-results.png"
         alt="For generators of the same size, discrimination accuracy was around 90-91%."/><figcaption>
            <p>For generators of the same size, discrimination accuracy was around 90-91%.</p>
        </figcaption>
</figure>


<p>When comparing generator/discriminator pairs with the same numbers of parameters, classification accuracy was around 90-92%.
Grover tends to leave artifacts when generating text, and this fact may be part of the reason discriminators are so good at identifying synthetic text. For one, the authors identify “exposure bias” as one of the reasons. The fact that Grover is never trained on generated text, only on human-authored articles seems to contribute to this. Perplexity also tends to vary over the length of the generated article, and depending on the sampling variance may fall out of the distribution of human language.</p>
]]></content>
        </item>
        
        <item>
            <title>Connect Jupyter to Remote</title>
            <link>https://lukesalamone.github.io/posts/connect-jupyter-to-remote/</link>
            <pubDate>Tue, 07 Sep 2021 09:10:56 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/connect-jupyter-to-remote/</guid>
            <description>Here&amp;rsquo;s how to connect to a remote Jupyter notebook.
Create an ssh tunnel to your remote machine:
ssh -L 8080:localhost:8080 user@12.34.56.78  Start Jupyter on that machine in headless mode:
jupyter notebook --no-browser --port=8080  Use a browser to open one of the urls that Jupyter presents:
http://localhost:8080/?token=xyz</description>
            <content type="html"><![CDATA[<p>Here&rsquo;s how to connect to a remote Jupyter notebook.</p>

<p>Create an ssh tunnel to your remote machine:</p>

<pre><code>ssh -L 8080:localhost:8080 user@12.34.56.78
</code></pre>

<p>Start Jupyter on that machine in headless mode:</p>

<pre><code>jupyter notebook --no-browser --port=8080
</code></pre>

<p>Use a browser to open one of the urls that Jupyter presents:<br />
<a href="http://localhost:8080/?token=xyz">http://localhost:8080/?token=xyz</a></p>
]]></content>
        </item>
        
        <item>
            <title>What is Marginalization?</title>
            <link>https://lukesalamone.github.io/posts/what-is-marginalization/</link>
            <pubDate>Wed, 07 Jul 2021 14:23:12 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/what-is-marginalization/</guid>
            <description>In machine learning and statistics, marginalization simply means summing over a set of independent variables. For example, suppose an avid tennis player kept track of the number of days he played tennis over a period of time as well as the weather on that day:
 .blue { background-color:#09f1; } .gray { background-color:#80808012; }    weather     sunny cloudy rainy totals   play? yes 70 25 1 96  no 70 5 9 84  totals 140 30 10 180   (In this table we&amp;rsquo;re keeping track of the number of days.</description>
            <content type="html"><![CDATA[<p>In machine learning and statistics, marginalization simply means summing over a set of independent variables. For example, suppose an avid tennis player kept track of the number of days he played tennis over a period of time as well as the weather on that day:</p>

<style>
  .blue {
    background-color:#09f1;
  }
  .gray {
    background-color:#80808012;
  }
</style>

<table style="width:100%">
  <tr style="font-weight:bold">
    <th></th>
    <th></th>
    <th colspan="3" style="text-align:center">weather</th>
    <th></th>
  </tr>
  <tr style="font-weight:bold; text-align:center; background-color: inherit">
    <td></td>
    <th></th>
    <td>sunny</td>
    <td>cloudy</td>
    <td>rainy</td>
    <th>totals</th>
  </tr>
  <tr>
    <td rowspan="2" style="font-weight:bold; text-align:right">play?</td>
    <td style="text-align:right">yes</td>
    <td class="blue">70</td>
    <td class="blue">25</td>
    <td class="blue">1</td>
    <td class="gray" style="font-weight:bold">96</td>
  </tr>
  <tr style="background-color: inherit;">
    <td style="text-align:right">no</td>
    <td class="blue">70</td>
    <td class="blue">5</td>
    <td class="blue">9</td>
    <td class="gray" style="font-weight:bold">84</td>
  </tr>
  <tr style="font-weight:bold">
    <td colspan="2" style="text-align:right">totals</td>
    <td class="gray">140</td>
    <td class="gray">30</td>
    <td class="gray">10</td>
    <td class="gray">180</td>
  </tr>
</table>

<p>(<em>In this table we&rsquo;re keeping track of the number of days. If you want probabilities, divide each value in the table by 180. But I think whole numbers are easier to think about so I&rsquo;m keeping them.</em>)</p>

<p>To marginalize one of the variables, we just sum one of the variables. For example, to marginalize the weather, we would sum each of the rows to find that <sup>96</sup>&frasl;<sub>180</sub>=53% of the time tennis was played, and 47% of the time tennis was not played. Likewise, to marginalize the boolean variable of whether tennis was played, we just sum the columns: no matter whether tennis was played on that day, how many days was it sunny? 140.</p>

<p>Another way of saying this is the marginal distribution of sunny weather is the first column (containing 70 and 70). The marginal distribution of playing tennis is the first row (containing 70, 25, and 1).</p>
]]></content>
        </item>
        
        <item>
            <title>Colab: Connect to Google Drive</title>
            <link>https://lukesalamone.github.io/posts/connect-to-colab/</link>
            <pubDate>Wed, 30 Jun 2021 22:58:18 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/connect-to-colab/</guid>
            <description>Here&amp;rsquo;s how to connect your Google Colab notebook to your Drive directory:
from google.colab import drive drive.mount(&#39;/content/gdrive&#39;)  Follow the prompts from there. That is all.</description>
            <content type="html"><![CDATA[<p>Here&rsquo;s how to connect your Google Colab notebook to your Drive directory:</p>

<pre><code class="language-python">from google.colab import drive
drive.mount('/content/gdrive')
</code></pre>

<p>Follow the prompts from there. That is all.</p>
]]></content>
        </item>
        
        <item>
            <title>BERT vs GPT-2 Performance</title>
            <link>https://lukesalamone.github.io/posts/bert-vs-gpt2/</link>
            <pubDate>Mon, 21 Jun 2021 01:04:42 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/bert-vs-gpt2/</guid>
            <description>There are quite a few BERT vs GPT-2 breakdowns online, mostly focusing on the architectural differences between the two models. However, I am more interested in the performance differences between the two models, specifically their predictive capabilities. This blog post outlines the results of my experiments.
The code used in this experiment can be found on my Github
BERT The Devlin et al. model was released in November 2018. It is a transformer-based language model pretrained on masked input (also known as the cloze task).</description>
            <content type="html"><![CDATA[

<p>There are quite a few BERT vs GPT-2 breakdowns online, mostly focusing on the architectural differences between the two models. However, I am more interested in the performance differences between the two models, specifically their predictive capabilities. This blog post outlines the results of my experiments.</p>

<p><a href="https://github.com/lukesalamone/gpt2-vs-bert">The code used in this experiment can be found on my Github</a></p>

<h2 id="bert">BERT</h2>

<p>The <a href="https://arxiv.org/pdf/1810.04805.pdf">Devlin et al. model</a> was released in November 2018. It is a transformer-based language model pretrained on masked input (also known as the <em>cloze</em> task). During pretraining, 15% of tokens are hidden from the model, and it is trained to predict the masked tokens. As a result, I was able to evaluate its ability to correctly predict a masked token at a random position in a fixed-size input.</p>

<p>I looked at the following varieties of BERT:</p>

<table>
<thead>
<tr>
<th>Model</th>
<th># Parameters</th>
<th>Compare to</th>
</tr>
</thead>

<tbody>
<tr>
<td>bert-base-uncased</td>
<td>110 million</td>
<td>gpt2</td>
</tr>

<tr>
<td>bert-base-cased</td>
<td>109 million</td>
<td>gpt2</td>
</tr>

<tr>
<td>bert-large-uncased</td>
<td>336 million</td>
<td>gpt2-medium</td>
</tr>

<tr>
<td>bert-large-cased</td>
<td>335 million</td>
<td>gpt2-medium</td>
</tr>
</tbody>
</table>

<p>This table also includes corresponding GPT-2 models which have a similar number of parameters. <a href="https://huggingface.co/transformers/pretrained_models.html">Source</a></p>

<h2 id="gpt-2">GPT-2</h2>

<p>The <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Radford et al. model</a> hit the scene in February of 2019. Like BERT it is a transformer-based  model, and comes in various sizes ranging from 117M parameters up to 1.5B parameters (gpt2-xl). Because GPT-2 is an autoregressive model, experiments with this family of models perform one token of generation following input context, comparing with the target token for accuracy measurement.</p>

<p>Here we will be evaluating two flavors of this model:</p>

<table>
<thead>
<tr>
<th>Model</th>
<th># Parameters</th>
<th>Compare to</th>
</tr>
</thead>

<tbody>
<tr>
<td>gpt2</td>
<td>117 million</td>
<td>bert-base</td>
</tr>

<tr>
<td>gpt2-medium</td>
<td>345 million</td>
<td>bert-large</td>
</tr>
</tbody>
</table>

<p>This table also includes corresponding BERT models which have a similar number of parameters. <a href="https://huggingface.co/transformers/pretrained_models.html">Source</a></p>

<h2 id="wikitext-token-prediction">Wikitext Token prediction</h2>

<p>To evaluate the models, I sampled 10,000 random sequences from <a href="https://paperswithcode.com/dataset/wikitext-2">Wikitext-2</a>.</p>

<p><strong>For BERT</strong>, a random sequence of 100 tokens is selected. Then, for each sequence, a random position within that sequence is selected and masked. BERT will be required to predict this token, so accuracy is measured as the percentage of the time which its masked token is predicted correctly.</p>

<p><strong>For GPT-2</strong>, a random sequence of 100 tokens is selected. Then, for each sequence, a random position within that sequence is selected. Because GPT-2 is autoregressive, it cannot attend to tokens on the right, so the sequence is truncated at the selected position. The sequence is then padded appropriately to maintain a fixed sequence length of 100.</p>

<p>Below we can see the performance of all 6 models on these tasks. The data has been smoothed by bucketing into groups of 5 positions at once (i.e. positions 0-4, 5-9, etc). You can see that performance of GPT-2 continues to rise as it is given additional context, while BERT models are relatively stable after being given around 5 tokens of context. Interestingly, BERT performance drops off quite steeply over the last 5-10 token positions.</p>

<p><script src="https://cdn.jsdelivr.net/npm/chart.js@3.4.0/dist/chart.min.js" type="text/javascript"></script>
<script src="/js/bert-vs-gpt2.js"></script>
<script src="/js/util.js"></script>
<div id="graph1"><canvas></canvas></div></p>

<p>When we zoom in on the final 10 positions, things start to get interesting. Both varieties of GPT-2 actually beat out all varieties of BERT at the final position.</p>

<div id="graph2"><canvas></canvas></div>

<h2 id="conclusion">Conclusion</h2>

<p>BERT and GPT-2 perform quite differently on the token prediction task depending on the position of the token being predicted. For a fixed sequence length of 100 tokens, BERT performs best when the masked token is between positions 5 and 95, while GPT-2 tends to continually improve as context length increases. Interestingly, when the final token in the sequence is to be predicted, BERT&rsquo;s performance falls off dramatically, while GPT-2 performance remains stable.</p>
]]></content>
        </item>
        
        <item>
            <title>How does GPT-2 Tokenize Text?</title>
            <link>https://lukesalamone.github.io/posts/gpt2-tokenization/</link>
            <pubDate>Thu, 17 Jun 2021 19:30:48 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/gpt2-tokenization/</guid>
            <description>Let&amp;rsquo;s explore how GPT-2 tokenizes text.
What is tokenization? It&amp;rsquo;s important to understand that GPT-2 doesn&amp;rsquo;t work with strings directly. Instead, it needs to tokenize the input string, which is essentially a process for converting the string into a list of numbers, or &amp;ldquo;tokens&amp;rdquo;. It is these tokens which are passed into the model during training or for inference. As a concrete example, let&amp;rsquo;s look at a few sample sentences:</description>
            <content type="html"><![CDATA[

<p>Let&rsquo;s explore how GPT-2 tokenizes text.</p>

<h2 id="what-is-tokenization">What is tokenization?</h2>

<p>It&rsquo;s important to understand that GPT-2 doesn&rsquo;t work with strings directly. Instead, it needs to tokenize the input string, which is essentially a process for converting the string into a list of numbers, or &ldquo;tokens&rdquo;. It is these tokens which are passed into the model during training or for inference. As a concrete example, let&rsquo;s look at a few sample sentences:</p>

<pre><code class="language-python">tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokens1 = tokenizer('I love my dog')
</code></pre>

<p>When we look at <code>tokens1</code> we see there are 4 tokens:</p>

<pre><code class="language-python">{'input_ids': [40, 1842, 616, 3290], 'attention_mask': [1, 1, 1, 1]}
</code></pre>

<p>Here what we care about is the <code>'input_ids'</code> list. We can ignore the <code>'attention_mask'</code> for now. We can convert the tokens in <code>[40, 1842, 616, 3290]</code> back into strings using <code>tokenizer.decode</code>:</p>

<pre><code class="language-python">tokens1 = tokens1['input_ids']

[tokenizer.decode(x) for x in tokens1]
# prints ['I', ' love', ' my', ' dog']

[tokenizer.decode(x).strip().lower() for x in tokens1]
# prints ['i', 'love', 'my', 'dog']
</code></pre>

<p>This process allows us to recover the tokens as strings from the tokenizer. For dictionary lookups, we&rsquo;ll also lowercase the strings and remove the whitespace from them.</p>

<p>Now, let&rsquo;s see what happens when we do the same thing with more complex words:</p>

<pre><code class="language-python">tokens2 = tokenizer('My favorite color is chartreuse')['token_ids']
[tokenizer.decode(x).strip().lower() for x in tokens2]
# prints ['my', 'favorite', 'color', 'is', 'chart', 're', 'use']
</code></pre>

<p>Because &ldquo;chartreuse&rdquo; isn&rsquo;t in GPT-2&rsquo;s vocabulary, it is tokenized as &ldquo;chart&rdquo;, &ldquo;re&rdquo; and &ldquo;use&rdquo;.</p>

<h3 id="about-that-attention-mask">About that attention mask</h3>

<p>For brevity I glossed over what <code>attention_mask</code> does above. If you&rsquo;re interested in attention masks, <a href="/posts/what-are-attention-masks">I have a blog post on that very topic</a>!</p>

<h2 id="english-words">English words</h2>

<p>Now it would be interesting to see how many tokens in GPT-2&rsquo;s vocabulary are actually English words. This is an imprecise metric since it depends heavily on which dictionary we use. (There is no single authoritative source of all English words.) I&rsquo;ll use several dictionaries and compare the results.</p>

<h3 id="enchant">Enchant</h3>

<p><a href="https://pyenchant.github.io/pyenchant/tutorial.html">PyEnchant</a> contains a python module <code>enchant</code> which we can use to check if a word is spelled correctly. It can also make spelling suggestions for incorrectly spelled words:</p>

<pre><code class="language-python">import enchant
d = enchant.request_dict(&quot;en_US&quot;)
d.check('Hello')
# prints True

d.check('Helo')
# prints False
</code></pre>

<h3 id="nltk-words">NLTK words</h3>

<p>The popular NLP library <a href="https://www.nltk.org/">NLTK</a> also contains a word list, accessible through its <code>corpus</code> module.</p>

<pre><code class="language-python">from nltk.corpus import words

nltk_words = set(words.words())
len(nltk_words)
# prints 235892
</code></pre>

<h3 id="english-350k">English 350k</h3>

<p>This list of words was taken from this <a href="https://github.com/dwyl/english-words/blob/master/words_alpha.txt">github repository</a>. It is a convenient list of lowercased words containing only letters. It seems to be the biggest of the word lists.</p>

<h3 id="lemmatization">Lemmatization</h3>

<p>We can bump our numbers up slightly through <a href="https://en.wikipedia.org/wiki/Lemmatisation">lemmatization</a>:</p>

<blockquote>
<p>In many languages, words appear in several inflected forms. For example, in English, the verb &lsquo;to walk&rsquo; may appear as &lsquo;walk&rsquo;, &lsquo;walked&rsquo;, &lsquo;walks&rsquo; or &lsquo;walking&rsquo;. The base form, &lsquo;walk&rsquo;, that one might look up in a dictionary, is called the lemma for the word. The association of the base form with a part of speech is often called a lexeme of the word.</p>
</blockquote>

<p>For our lemmatizer we will use <code>WordNetLemmatizer</code> from <code>nltk.stem.wordnet</code>.</p>

<h2 id="testing-gpt-2-tokens">Testing GPT-2 tokens</h2>

<p>So of the tokens which GPT-2 uses, how many are English words? We can break this down metric by the dictionary used.</p>

<table>
<thead>
<tr>
<th>Dictionary</th>
<th>% Words</th>
</tr>
</thead>

<tbody>
<tr>
<td>English370k †</td>
<td>72.92%</td>
</tr>

<tr>
<td>English370k</td>
<td>72.59%</td>
</tr>

<tr>
<td>Enchant †</td>
<td>60.48%</td>
</tr>

<tr>
<td>Enchant</td>
<td>60.17%</td>
</tr>

<tr>
<td>NLTK words †</td>
<td>57.07%</td>
</tr>

<tr>
<td>NLTK words</td>
<td>48.27%</td>
</tr>
</tbody>
</table>

<p><em>† indicates words were lemmatized</em></p>

<p>So the English370k word list seems to capture the most tokens from the three dictionaries. Also note the mild impact of lemmatization: although it may bump some of the percentages up a bit, it&rsquo;s not enough for one dictionary to outperform another.</p>

<div id="pie"><canvas></canvas></div>

<p>Looking at the tokens which aren&rsquo;t in the dictionary, around 73% of them are non-word alphabetical strings. The final 27% is accounted for by symbols, numbers, and non-ascii character sequences (unicode characters from languages like Arabic, Korean, and Chinese). If we remove these, we end up with about 10k tokens containing only letters, which is around 21% of GPT-2&rsquo;s total vocabulary. I&rsquo;ve included this list in a <a href="https://gist.github.com/lukesalamone/22ce6f362db3bdd09eda3cc5cbf5576f">github gist</a> (duplicates removed).</p>

<h2 id="now-what">Now what?</h2>

<p>Looking at these non-word alphabetical strings, it&rsquo;s interesting to see how the Internet (as GPT-2 saw it) was encoded. Then again, it also contains a lot of proper nouns which wouldn&rsquo;t be in a normal dictionary like &ldquo;starbucks&rdquo;.</p>

<p>Other tokens are clearly vestiges of the scraping process which was used to gather text which GPT-2 trained on. Tokens like &ldquo;rawdownloadcloneembedreportprint&rdquo;, &ldquo;buyableinstoreandonline&rdquo;, &ldquo;randomredditorwithno&rdquo;, and &ldquo;itemthumbnailimage&rdquo; contain next to zero semantic value and the vocabulary space would probably have been better served with more meaningful tokens.</p>

<p>The following are the longest non-dictionary tokens found in GPT-2&rsquo;s vocabulary:</p>

<table>
<thead>
<tr>
<th>Token ID</th>
<th>String</th>
</tr>
</thead>

<tbody>
<tr>
<td>39177</td>
<td>ItemThumbnailImage</td>
</tr>

<tr>
<td>30210</td>
<td>guiActiveUnfocused</td>
</tr>

<tr>
<td>39755</td>
<td>isSpecialOrderable</td>
</tr>

<tr>
<td>31576</td>
<td>externalActionCode</td>
</tr>

<tr>
<td>39753</td>
<td>quickShipAvailable</td>
</tr>

<tr>
<td>39757</td>
<td>channelAvailability</td>
</tr>

<tr>
<td>36174</td>
<td>RandomRedditorWithNo</td>
</tr>

<tr>
<td>30899</td>
<td>cloneembedreportprint</td>
</tr>

<tr>
<td>40242</td>
<td>BuyableInstoreAndOnline</td>
</tr>

<tr>
<td>30906</td>
<td>rawdownloadcloneembedreportprint</td>
</tr>
</tbody>
</table>

<p>We may also be able to measure performance of GPT-2 on certain tasks based on how many of the tokens were dictionary words. It might be true, for example, that sentences with higher proportions of dictionary word tokens would perform better on sentence completion tasks.</p>

<script src="/js/chart.min.js" type="text/javascript"></script>
<script src="/js/gpt2-tokens-pie.js" type="text/javascript"></script>
]]></content>
        </item>
        
        <item>
            <title>What Are Attention Masks?</title>
            <link>https://lukesalamone.github.io/posts/what-are-attention-masks/</link>
            <pubDate>Tue, 15 Jun 2021 19:09:36 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/what-are-attention-masks/</guid>
            <description>TLDR: Attention masks allow us to send a batch into the transformer even when the examples in the batch have varying lengths. We do this by padding all sequences to the same length, then using the &amp;ldquo;attention_mask&amp;rdquo; tensor to identify which tokens are padding.
 Here we use a batch with three samples padded from the left since we want to predict the next token on the right. (Padding on the right would probably predict another pad.</description>
            <content type="html"><![CDATA[

<p>TLDR: Attention masks allow us to send a batch into the transformer even when the examples in the batch have varying lengths. We do this by padding all sequences to the same length, then using the &ldquo;attention_mask&rdquo; tensor to identify which tokens are padding.</p>

<figure><img src="/img/attention_mask.png"
         alt="Here we use a batch with three samples padded from the left since we want to predict the next token on the right. (Padding on the right would probably predict another pad.)"/><figcaption>
            <p>Here we use a batch with three samples padded from the left since we want to predict the next token on the right. (Padding on the right would probably predict another pad.)</p>
        </figcaption>
</figure>


<p>If you want to perform inference with transformers one sequence at a time, you can ignore attention masks. The &ldquo;slow way&rdquo; will be sufficient for your needs.</p>

<h2 id="the-slow-way">The slow way</h2>

<p>We can perform inference with GPT-2 using sequences one at a time, but it&rsquo;s slow:</p>

<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')

context = tokenizer('It will rain in the', return_tensors='pt')

prediction = gpt2.generate(**context, max_length=10)
tokenizer.decode(prediction[0])
# prints 'It will rain in the morning, and the rain'
</code></pre>

<p>It&rsquo;s way faster to batch the inputs, which means adding their token vectors to the context and performing inference only once.</p>

<h2 id="the-un-slow-way">The un-slow way</h2>

<p>The cool way to perform inference on many samples is with batching. It&rsquo;s much faster but it&rsquo;s also slightly more complicated.</p>

<pre><code class="language-python">tokenizer.padding_side = &quot;left&quot;
tokenizer.pad_token = tokenizer.eos_token

sentences = [&quot;It will rain in the&quot;,
            &quot;I want to eat a big bowl of&quot;,
            &quot;My dog is&quot;]
inputs = tokenizer(sentences, return_tensors=&quot;pt&quot;, padding=True)

output_sequences = gpt2.generate(**inputs)

for seq in output_sequences:
    print(tokenizer.decode(seq))
</code></pre>

<p>What&rsquo;s happening here? And what does this have to do with attention masks? First let&rsquo;s explain padding, then take a look at the code line by line.</p>

<p>We feed tokens into transformer-based language models like GPT-2 and BERT for inference as <a href="https://pytorch.org/docs/stable/tensors.html">tensors</a>. A tensor is like a python list but with a few extra features and restrictions. Specifically, for a tensor of dimension 2+, all vectors in that dimension need to be the same length. For example,</p>

<pre><code class="language-python">from torch import tensor

tensor([[1,2], [3,4]])  # ok
tensor([[1,2], [3]])   # error!
</code></pre>

<p>When we tokenize an input, it it will be turned into a tensor containing sequence of integers, each corresponding to an item in the transformer&rsquo;s vocabulary. Here is an example tokenization in GPT-2:</p>

<table>
<thead>
<tr>
<th>String</th>
<th>Token ID</th>
</tr>
</thead>

<tbody>
<tr>
<td>It</td>
<td>1026</td>
</tr>

<tr>
<td>will</td>
<td>481</td>
</tr>

<tr>
<td>rain</td>
<td>6290</td>
</tr>

<tr>
<td>in</td>
<td>287</td>
</tr>

<tr>
<td>the</td>
<td>262</td>
</tr>
</tbody>
</table>

<p>Suppose we wanted to include a second sequence in our input:</p>

<table>
<thead>
<tr>
<th>String</th>
<th>Token ID</th>
</tr>
</thead>

<tbody>
<tr>
<td>My</td>
<td>3666</td>
</tr>

<tr>
<td>dog</td>
<td>3290</td>
</tr>

<tr>
<td>is</td>
<td>318</td>
</tr>
</tbody>
</table>

<p>Because these two sequences have different lengths, we can&rsquo;t just combine them in one tensor. Instead, we have to <em>pad</em> the shorter sequences with dummy tokens so that each sequence is the same length. And because we want the model to continue to add to the right side of our sequence, we will pad the left side of shorter sequences.</p>

<table>
<thead>
<tr>
<th>String</th>
<th>Token ID</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>&lt;pad&gt;</code></td>
<td>50256</td>
</tr>

<tr>
<td>My</td>
<td>3666</td>
</tr>

<tr>
<td>dog</td>
<td>3290</td>
</tr>

<tr>
<td>is</td>
<td>318</td>
</tr>
</tbody>
</table>

<p>This is where the attention mask comes in. The attention mask simply shows the transformer which tokens are padding, placing 0s in the positions of padding tokens and 1s in the positions of actual tokens. Now that we understand that, let&rsquo;s look at the code line by line.</p>

<pre><code class="language-python">tokenizer.padding_side = &quot;left&quot;
</code></pre>

<p>This line tells the tokenizer to begin padding from the left (default is right) because the logits of the rightmost token will be used to predict future tokens.</p>

<pre><code class="language-python">tokenizer.pad_token = tokenizer.eos_token
</code></pre>

<p>This line specifies which token we will use for padding. It doesn&rsquo;t matter which one you choose, but here we&rsquo;re choosing the &ldquo;end of sequence&rdquo; token.</p>

<pre><code class="language-python">sentences = [&quot;It will rain in the&quot;,
            &quot;I want to eat a big bowl of&quot;,
            &quot;My dog is&quot;]
</code></pre>

<p>These three sequences all have different lengths when tokenized, so should be a good test of our padding method.</p>

<pre><code class="language-python">inputs = tokenizer(sentences, return_tensors=&quot;pt&quot;, padding=True)
</code></pre>

<p>Now we tokenize. We&rsquo;re passing in the sentences from above, telling the tokenizer to use PyTorch tensors (rather than Tensorflow), and telling the tokenizer to add padding for us. We can print <code>inputs</code> here to confirm that, yes, tokenization is working as we thought:</p>

<pre><code class="language-python">{'input_ids': tensor([
    [50256, 50256, 50256,  1026,   481,  6290,   287,   262],
    [   40,   765,   284,  4483,   257,  1263,  9396,   286],
    [50256, 50256, 50256, 50256, 50256,  3666,  3290,   318]
  ]),
'attention_mask': tensor([
    [0, 0, 0, 1, 1, 1, 1, 1],
    [1, 1, 1, 1, 1, 1, 1, 1],
    [0, 0, 0, 0, 0, 1, 1, 1]
  ])}
</code></pre>

<p>As you can see, the first and third sequence include padding at the beginning, and the <code>attention_mask</code> parameter marks the position of this padding.</p>

<p>Now let&rsquo;s actually pass this input into the model to generate new text:</p>

<pre><code class="language-python">output_sequences = gpt2.generate(**inputs)
</code></pre>

<p>If you&rsquo;re unfamiliar with <code>**kwargs</code> syntax for function calls, this passes in the <code>inputs</code> dict as named parameters, using the keys as the parameter names and the values as the corresponding argument values. <a href="https://docs.python.org/3/tutorial/controlflow.html#keyword-arguments">Check the docs for more info</a>.</p>

<p>Finally, we just need to loop through each of the generated sequences and print out the result in human readable form, using the <code>decode()</code> function to convert token IDs to strings.</p>

<pre><code class="language-python">for seq in output_sequences:
    print(tokenizer.decode(seq))
</code></pre>
]]></content>
        </item>
        
        <item>
            <title>How Does Convolution Work?</title>
            <link>https://lukesalamone.github.io/posts/how-does-convolution-work/</link>
            <pubDate>Mon, 14 Jun 2021 21:05:06 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/how-does-convolution-work/</guid>
            <description>Convolutional neural networks have had breakthrough success in image recognition, natural language processing, and even board games like Chess and Go. But what&amp;rsquo;s really going on during convolution? Well, I think the easiest way to explain is with an interactive demo. Feel free to play around with the parameters below to see for yourself!
      number:   four three eight    padding:    kernel size:    stride:    speed:        You can use the settings above to control the hyperparameters of the convolutional layer.</description>
            <content type="html"><![CDATA[

<p>Convolutional neural networks have had breakthrough success in image recognition, natural language processing, and even board games like Chess and Go. But what&rsquo;s really going on during convolution? Well, I think the easiest way to explain is with an interactive demo. Feel free to play around with the parameters below to see for yourself!</p>

<p><script src="/js/util.js"></script>
<script src="/js/convolution-demo.js"></script>
<link rel="stylesheet" href="/css/convolution-demo.css" /></p>

<div id="input-output">
  <div id="input-grid"></div>
  <div id="output-grid"></div>
</div>

<div id="controls">
  <table>
    <tr id="number">
      <td>number:</td>
      <td>
        <select>
          <option value="four">four</option>
          <option value="three">three</option>
          <option value="eight">eight</option>
        </select>
      </td>
    </tr>
    <tr id="padding">
      <td>padding: <span class="val"></span></td>
      <td><input type="range" min="0" max="2" value="0"></td>
    </tr>
    <tr id="kernelsize">
      <td>kernel size: <span class="val"></span></td>
      <td><input type="range" min="1" max="4" value="2"></td>
    </tr>
    <tr id="stride">
      <td>stride: <span class="val"></span></td>
      <td><input type="range" min="1" max="3" value="1"></td>
    </tr>
    <tr id="speed">
      <td>speed: <span class="val"></span></td>
      <td><input type="range" min="1" max="5" value="3"></td>
    </tr>
    <tr id="errors" style="display:none">
      <td colspan="2"></td>
    </tr>
  </table>
</div>

<p>You can use the settings above to control the hyperparameters of the convolutional layer.</p>

<p>As you&rsquo;ve probably figured out, the left side shows the input, a 20x20 handwritten digit, and the right side shows the output as it is being drawn. A convolutional layer will pass a filter over the input, selecting only a small group of pixels at a time.</p>

<p>Not shown: once the filter selects a window of pixels, it will compute a dot product with the pixel values and its own weights. In this demo the filter weights are assumed to be 1 for simplicity.</p>

<h2 id="more-on-the-hyperparameters">More on the hyperparameters</h2>

<p>Sometimes the settings will result in an invalid convolution. This happens if the kernel extends beyond the activation map. We can write a python function to check whether it will be valid:</p>

<pre><code class="language-python">def valid_convolution(input_size, kernel_size, padding, stride):
  if (input_size + 2*padding - kernel_size) % stride == 0:
    return True
  else:
    return False
</code></pre>

<h3 id="padding">Padding</h3>

<p>Increasing the padding will add extra pixels, typically zeros, around the edges of the input. This is done in order to ensure that the layer parameters will be valid. Additionally, it causes pixels which would have been on the edges to be more to the middle of the activation map.</p>

<h3 id="kernel-size">Kernel Size</h3>

<p>This refers to the &ldquo;sliding window&rdquo; which passes over the input. Choosing a larger kernel tends to increase the &ldquo;smudging&rdquo; which happens in the output, while a smaller kernel preserves more information for deeper layers. Sometimes kernel size is referred to as filter size.</p>

<h3 id="stride">Stride</h3>

<p>Stride refers to the distance the sliding window will move between steps. A stride of 1 moves the window one pixel to the right, while a stride of 2 moves the window 2 pixels. A larger stride can reduce output layer size at the expense of granularity.</p>
]]></content>
        </item>
        
        <item>
            <title>Python: Serve an HTML File</title>
            <link>https://lukesalamone.github.io/posts/python-serve-html/</link>
            <pubDate>Sun, 09 May 2021 15:06:11 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/python-serve-html/</guid>
            <description>If you want to serve some HTML with python run
python -m http.server 8000  Then navigate to http://localhost:8000.
This is not meant for production environments but will get you around CORS restrictions that would come from simply opening a local file in your browser.</description>
            <content type="html"><![CDATA[<p>If you want to serve some HTML with python run</p>

<pre><code class="language-console">python -m http.server 8000
</code></pre>

<p>Then navigate to <a href="http://localhost:8000/">http://localhost:8000</a>.</p>

<p>This is not meant for production environments but will get you around CORS restrictions that would come from simply opening a local file in your browser.</p>
]]></content>
        </item>
        
        <item>
            <title>How to Train and Run a Simple Language Model</title>
            <link>https://lukesalamone.github.io/posts/running-simple-language-model/</link>
            <pubDate>Fri, 16 Apr 2021 21:08:53 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/running-simple-language-model/</guid>
            <description>This article will show how to run a simple language model, KenLM. It&amp;rsquo;s not as powerful as transformer-based models like BERT or GPT-3, but depending on what you&amp;rsquo;re trying to accomplish it may be more than enough. This tutorial should take you about 15 minutes, including the time to run the scripts.
Let&amp;rsquo;s work backwards from where we&amp;rsquo;re trying to get to. When you&amp;rsquo;ve finished, you should be able to run the following script:</description>
            <content type="html"><![CDATA[

<p>This article will show how to run a simple language model, KenLM. It&rsquo;s not as powerful as transformer-based models like BERT or GPT-3, but depending on what you&rsquo;re trying to accomplish it may be more than enough. This tutorial should take you about 15 minutes, including the time to run the scripts.</p>

<p>Let&rsquo;s work backwards from where we&rsquo;re trying to get to. When you&rsquo;ve finished, you should be able to run the following script:</p>

<pre><code class="language-python">import kenlm

path = 'path/to/model.arpa'
lm = kenlm.LanguageModel(path)

sentence = &quot;I am not superstitious but I am a little stitious&quot;
print(model.score(sentence))
</code></pre>

<p>The first step will be to build KenLM. Then, we will build the ARPA file which KenLM uses to evaluate.</p>

<h2 id="building-kenlm">Building KenLM</h2>

<p>First, clone this repository:</p>

<p><code>git clone git@github.com:kpu/kenlm.git</code></p>

<p>Now we need to build the KenLM toolkit. Run the following to build:</p>

<pre><code>mkdir -p build
cd build
cmake ..
make -j 4
</code></pre>

<p>Now we just need to provide the model with a <code>.arpa</code> ngram language model file. So let&rsquo;s get one.</p>

<h2 id="building-an-ngram-language-model-from-wikitext-2">Building an ngram language model from Wikitext-2</h2>

<p>First, let&rsquo;s clone a repository which will build an ARPA file. This repository builds the ngram file from <a href="https://paperswithcode.com/dataset/wikitext-2">Wikitext-2</a>, a common dataset used in natural language processing.</p>

<pre><code>git clone git@github.com:daandouwe/ngram-lm.git
cd ngram-lm
mkdir data
./get-data.sh
mkdir arpa
./main.py --order 3 --interpolate --save-arpa --name wiki-interpolate
</code></pre>

<p>Once that has finished, you&rsquo;ll have new <code>.arpa</code> in the <code>arpa</code> directory you created. This script took the longest to run on my machine. Be patient, your computer is busy reading all of Wikipedia.</p>

<h2 id="all-together-now">All Together Now</h2>

<p>Now we&rsquo;re finally ready to evaluate a sentence with the language model.</p>

<pre><code class="language-python">import kenlm

path = 'path/to/model.arpa'
lm = kenlm.LanguageModel(path)

sentence = &quot;I am not superstitious but I am a little stitious&quot;
print(model.score(sentence))
</code></pre>

<p>Which prints something like</p>

<pre><code>----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
-24.47921371459961
</code></pre>

<p>Now if you&rsquo;re interested in a bit more information about what&rsquo;s going on, you can add this bit at the bottom:</p>

<pre><code class="language-python">words = ['&lt;s&gt;'] + sentence.split() + ['&lt;/s&gt;']
for i, (prob, length, oov) in enumerate(lm.full_scores(sentence)):
  print(f'{prob} {length}: {&quot; &quot;.join(words[i+2-length:i+2])}')
  if oov:
    print(f'\t&quot;{words[i+1]}&quot; is an OOV')

for w in words:
  if not w in lm:
    print(f'&quot;{w}&quot; is an OOV')
</code></pre>

<p>Which adds this to your output:</p>

<pre><code>-3.1138248443603516 2: &lt;s&gt; I
-1.1560251712799072 3: &lt;s&gt; I am
-1.1645264625549316 3: I am not
-4.912360191345215 1: superstitious
-4.504511833190918 1: but
-2.2214112281799316 2: but I
-1.1531075239181519 3: but I am
-1.2614283561706543 3: I am a
-0.9001830816268921 3: am a little
-1.2325057983398438 3: a little stitious
	&quot;stitious&quot; is an OOV
-2.8593297004699707 2: stitious &lt;/s&gt;
&quot;stitious&quot; is an OOV
</code></pre>

<p>To the left of each term is the log (base 10) probability of each term occurring. For the first term, <code>&lt;s&gt; I</code> means start of sentence followed by &ldquo;I&rdquo;, which the model has assigned a log probability of -3.11. That&rsquo;s around 0.00078. You might think it&rsquo;s strange that a sentence beginning with &ldquo;I&rdquo; is so unlikely but we are using Wikitext-2. Wikitext-2 is Wikipedia articles. Not a lot of sentences on Wikipedia begin with &ldquo;I&rdquo;.</p>

<p>Notice that &ldquo;stitious&rdquo; is an OOV (out of vocabulary) term here. Clearly the language model doesn&rsquo;t appreciate humor. We&rsquo;ll have to tackle that next time.</p>
]]></content>
        </item>
        
        <item>
            <title>What is Temperature in NLP?🐭</title>
            <link>https://lukesalamone.github.io/posts/what-is-temperature/</link>
            <pubDate>Fri, 02 Apr 2021 00:50:38 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/what-is-temperature/</guid>
            <description>MathJax.Hub.Config({ tex2jax: { inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]], displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\[&#39;,&#39;\]&#39;]], processEscapes: true, processEnvironments: true, skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;], TeX: { equationNumbers: { autoNumber: &#34;AMS&#34; }, extensions: [&#34;AMSmath.js&#34;, &#34;AMSsymbols.js&#34;] } } });  MathJax.Hub.Queue(function() { // Fix  tags after MathJax finishes running. This is a // hack to overcome a shortcoming of Markdown. Discussion at // https://github.com/mojombo/jekyll/issues/199 var all = MathJax.Hub.getAllJax(), i; for(i = 0; i Temperature is a parameter used in natural language processing models to increase or decrease the &amp;ldquo;confidence&amp;rdquo; a model has in its most likely response.</description>
            <content type="html"><![CDATA[

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      },
      extensions: ["AMSmath.js", "AMSsymbols.js"]
    }
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<p>Temperature is a parameter used in natural language processing models to increase or decrease the &ldquo;confidence&rdquo; a model has in its most likely response.</p>

<p>In my opinion, the most intuitive way of understanding how temperature affects model outputs is to play with it yourself. If you&rsquo;re interested in the mathematical details, I&rsquo;ve included them below, but I won&rsquo;t be offended if you just want to play around with the slider 😃 .</p>

<script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.7.2/Chart.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
<script src="/js/temperature-demo.js"></script>

<p><canvas id="myChart" style="background-color: #0000"></canvas>
<span>Temperature (θ):
<input type="range" min="1" max="1000" value="250" class="slider" id="myRange">
<span id="temperature">25.0</span></p>

<h2 id="what-s-going-on">What&rsquo;s going on?</h2>

<p>Suppose we have a language model which predicts the last word in the sentence &ldquo;The mouse ate the _____&ldquo;. Given the previous words in the sentence and its prior training, our language model will try to fill in the blank with a reasonable final token. Suppose those raw outputs are as follows:</p>

<table>
<thead>
<tr>
<th>token</th>
<th>logit</th>
</tr>
</thead>

<tbody>
<tr>
<td>cat</td>
<td>3</td>
</tr>

<tr>
<td>cheese</td>
<td>70</td>
</tr>

<tr>
<td>pizza</td>
<td>40</td>
</tr>

<tr>
<td>cookie</td>
<td>65</td>
</tr>

<tr>
<td>fondue</td>
<td>55</td>
</tr>

<tr>
<td>banana</td>
<td>10</td>
</tr>

<tr>
<td>baguette</td>
<td>15</td>
</tr>

<tr>
<td>cake</td>
<td>12</td>
</tr>
</tbody>
</table>

<p>These outputs make sense. A mouse probably eats cheese, but <a href="https://en.wikipedia.org/wiki/If_You_Give_a_Mouse_a_Cookie">mice are also known to eat cookies</a>. A mouse probably wouldn&rsquo;t eat a baguette unless it was <a href="https://imgur.com/a/484AEO3">a French mouse</a>.</p>

<p>Since these are the raw outputs of the model, they won&rsquo;t sum to 100. To normalize these values, we typically use <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>:</p>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<p style="text-align: center; font-size: 1.5em">
$ σ(z_i) = {e^{z_i} \over \sum_{j=0}^N e^{z_j}} $
</p>

<p>When modulating with temperature, we introduce an additional temperature variable θ which affects the softmax distribution. A higher temperature θ &ldquo;excites&rdquo; previously low probability outputs. A lower temperature θ lowers the smaller outputs relative to the largest outputs. To accomplish this, we replace each z<sub>i</sub> in the formula above with the quotient z<sub>i</sub>/θ:</p>

<p style="text-align: center; font-size: 1.5em">
$ σ(z_i) = {e^{z_i \over θ} \over \sum_{j=0}^N e^{z_j \over θ}} $
</p>

<p>Higher temperatures make the model more &ldquo;creative&rdquo; which can be useful when generating prose, for example. Lower temperatures make the model more &ldquo;confident&rdquo; which can be useful in applications like question answering.</p>
]]></content>
        </item>
        
        <item>
            <title>What is Perplexity?</title>
            <link>https://lukesalamone.github.io/posts/perplexity/</link>
            <pubDate>Thu, 01 Apr 2021 12:14:49 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/perplexity/</guid>
            <description>MathJax.Hub.Config({ tex2jax: { inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]], displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\[&#39;,&#39;\]&#39;]], processEscapes: true, processEnvironments: true, skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;], TeX: { equationNumbers: { autoNumber: &#34;AMS&#34; }, extensions: [&#34;AMSmath.js&#34;, &#34;AMSsymbols.js&#34;] } } });  MathJax.Hub.Queue(function() { var all = MathJax.Hub.getAllJax(), i; for(i = 0; i TLDR: NLP metric ranging from 1 to infinity. Lower is better.
In natural language processing, perplexity is the most common metric used to measure the performance of a language model.</description>
            <content type="html"><![CDATA[

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      },
      extensions: ["AMSmath.js", "AMSsymbols.js"]
    }
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<p><strong>TLDR: NLP metric ranging from 1 to infinity. Lower is better.</strong></p>

<p>In natural language processing, perplexity is the most common metric used to measure the performance of a language model. To calculate perplexity, we use the following formula:</p>

<p style="text-align: center; font-size: 1.5em">
$ perplexity = e^z $
</p>

<p style="text-align:center">where</p>

<p style="text-align: center; font-size: 1.5em">
$ z = -{1 \over N} \sum_{i=0}^N ln(P_{n}) $
</p>

<p>Typically we use base <code>e</code> when calculating perplexity, but this is not required. Any base will do, so sometimes the formula will use base 2 or base 10, along with logarithms to the corresponding base.</p>

<h2 id="example">Example</h2>

<p>Imagine that we have a language model which generates the following sequence of tokens:</p>

<p><code>&lt;start&gt;</code> <code>jack</code> <code>and</code> <code>jill</code> <code>went</code> <code>up</code> <code>the</code> <code>hill</code></p>

<p>And suppose that the conditional probabilities for each of the tokens are as follows:</p>

<table>
<thead>
<tr>
<th>token</th>
<th>probability</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>&lt;start&gt;</code></td>
<td>15%</td>
</tr>

<tr>
<td><code>jack</code></td>
<td>5%</td>
</tr>

<tr>
<td><code>and</code></td>
<td>12%</td>
</tr>

<tr>
<td><code>jill</code></td>
<td>18%</td>
</tr>

<tr>
<td><code>went</code></td>
<td>25%</td>
</tr>

<tr>
<td><code>up</code></td>
<td>40%</td>
</tr>

<tr>
<td><code>the</code></td>
<td>33%</td>
</tr>

<tr>
<td><code>hill</code></td>
<td>50%</td>
</tr>
</tbody>
</table>

<p>For the purposes of calculating perplexity it doesn&rsquo;t matter how the sequence was generated. It may be using an n-gram model or an LSTM or a transformer. All that matters is the probabilities the model assigns to each of the tokens. To calculate perplexity, we calculate the logarithm of each of the values above:</p>

<table>
<thead>
<tr>
<th>token</th>
<th>P</th>
<th>ln(P)</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>&lt;start&gt;</code></td>
<td>15%</td>
<td>-1.897</td>
</tr>

<tr>
<td><code>jack</code></td>
<td>5%</td>
<td>-2.996</td>
</tr>

<tr>
<td><code>and</code></td>
<td>12%</td>
<td>-2.120</td>
</tr>

<tr>
<td><code>jill</code></td>
<td>18%</td>
<td>-1.715</td>
</tr>

<tr>
<td><code>went</code></td>
<td>25%</td>
<td>-1.386</td>
</tr>

<tr>
<td><code>up</code></td>
<td>40%</td>
<td>-0.916</td>
</tr>

<tr>
<td><code>the</code></td>
<td>33%</td>
<td>-1.109</td>
</tr>

<tr>
<td><code>hill</code></td>
<td>50%</td>
<td>-0.693</td>
</tr>
</tbody>
</table>

<p>Summing the logs, we get -12.832. Since there are 8 tokens, we divide -12.832 by 8 to get -1.604. Negating that allows us to calculate the final perplexity:</p>

<p style="text-align: center; font-size: 1.5em">
$ perplexity = e^{1.604} = 4.973 $
</p>

<p>Therefore the perplexity of this sequence is about 4.973.</p>
]]></content>
        </item>
        
        <item>
            <title>S3 Bucket Url</title>
            <link>https://lukesalamone.github.io/posts/s3-bucket-url/</link>
            <pubDate>Wed, 10 Mar 2021 03:03:53 -0600</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/s3-bucket-url/</guid>
            <description>Assuming your bucket is publicly accessible, the url of your S3 bucket will be
http://[bucket-name].s3-website-[region].amazonaws.com  For example for &amp;ldquo;mybucket&amp;rdquo; in &amp;ldquo;us-east-1&amp;rdquo; your url will be
http://mybucket.s3-website-us-east-1.amazonaws.com  </description>
            <content type="html"><![CDATA[<p>Assuming your bucket is publicly accessible, the url of your S3 bucket will be</p>

<pre><code>http://[bucket-name].s3-website-[region].amazonaws.com
</code></pre>

<p>For example for &ldquo;mybucket&rdquo; in &ldquo;us-east-1&rdquo; your url will be</p>

<pre><code>http://mybucket.s3-website-us-east-1.amazonaws.com
</code></pre>
]]></content>
        </item>
        
        <item>
            <title>About My Quick Reference Articles</title>
            <link>https://lukesalamone.github.io/posts/why-how-to/</link>
            <pubDate>Sun, 07 Mar 2021 14:44:37 -0600</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/why-how-to/</guid>
            <description>I&amp;rsquo;ve created a few quick-reference articles and it might not be clear why. There are a few reasons:
 These articles are mainly a reference for me. I find myself searching the same things over and over, looking for the purple link, scrolling through the article, then copy &amp;amp; pasting code. I&amp;rsquo;d rather not go through the hassle. These articles aim to solve that problem. I aim to keep the answers above the fold.</description>
            <content type="html"><![CDATA[<p>I&rsquo;ve created a few quick-reference articles and it might not be clear why. There are a few reasons:</p>

<ol>
<li>These articles are mainly a reference for me. I find myself searching the same things over and over, looking for the purple link, scrolling through the article, then copy &amp; pasting code. I&rsquo;d rather not go through the hassle. These articles aim to solve that problem.</li>
<li>I aim to keep the answers <a href="https://en.wikipedia.org/wiki/Above_the_fold#In_web_design">above the fold</a>. I don&rsquo;t want to have to scroll down to find the answer. I almost never read the surrounding prose when I am in &ldquo;coding mode&rdquo;.</li>
<li>I don&rsquo;t have ads or popups on my blog. I will never ask people to sign up for a newsletter or login to read more. I also don&rsquo;t use pictures unless there&rsquo;s a good reason. <a href="https://www.google.com/search?q=ai+thinking+robot+stock+photo&amp;tbm=isch">The &ldquo;thinking AI robot stock photo&rdquo; industry is definitely a bubble.</a></li>
<li>Writing these things out explicitly helps me to remember them. Paradoxically, this may make these how-to pages less useful to me, but maybe someone else will find them useful.</li>
</ol>

<p>These quick reference articles don&rsquo;t explain much because I don&rsquo;t need an explanation of what is going on. I just need a chunk of working code. There are other websites which have far more comprehensive guides covering how to do things. They cover all of the fundamentals of how things are done. But I don&rsquo;t need that, I just want a 30 second reference with a working chunk of code.</p>
]]></content>
        </item>
        
        <item>
            <title>Python: Read &amp; Write Json</title>
            <link>https://lukesalamone.github.io/posts/read-write-json/</link>
            <pubDate>Sun, 07 Mar 2021 14:05:27 -0600</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/read-write-json/</guid>
            <description>Often it is useful to save python data to json files. The following code will demonstrate how that can be done.
 &amp;ldquo;God bless JSON!&amp;rdquo; ~ a soon to be famous programmer
 import json data = {&#39;a&#39;: 1, &#39;b&#39;:&#39;hello&#39;, &#39;c&#39;:False} filename = &#39;awesome_data.json&#39; # write data to file with open(filename, &#39;w&#39;) as f: json.dump(data, f) # read json from file with open(filename, &#39;r&#39;) as f: data = json.load(f) print(data) # prints {&#39;a&#39;: 1, &#39;b&#39;:&#39;hello&#39;, &#39;c&#39;:False}  </description>
            <content type="html"><![CDATA[<p>Often it is useful to save python data to json files. The following code will demonstrate how that can be done.</p>

<blockquote>
<p>&ldquo;God bless JSON!&rdquo; ~ a soon to be famous programmer</p>
</blockquote>

<pre><code class="language-python">import json

data = {'a': 1, 'b':'hello', 'c':False}
filename = 'awesome_data.json'

# write data to file
with open(filename, 'w') as f:
  json.dump(data, f)


# read json from file
with open(filename, 'r') as f:
  data = json.load(f)


print(data)
# prints {'a': 1, 'b':'hello', 'c':False}
</code></pre>
]]></content>
        </item>
        
        <item>
            <title>Autoencoding Stock Prices</title>
            <link>https://lukesalamone.github.io/posts/build-an-autoencoder/</link>
            <pubDate>Sun, 07 Mar 2021 01:31:51 -0600</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/build-an-autoencoder/</guid>
            <description>Autoencoding stock prices as found in Heaton et al., 2016
  So you want to build an autoencoder? Great! This article will demonstrate how to build an autoencoder and use it to measure stock prices against an index. This technique is described in more technical terms here.
Once we&amp;rsquo;ve trained the autoencoder, we can use it to measure how well each component follows the other members of the index.</description>
            <content type="html"><![CDATA[

<figure><img src="/img/autoencoder.png"
         alt="Autoencoding stock prices as found in Heaton et al., 2016"/><figcaption>
            <p>Autoencoding stock prices as found in Heaton et al., 2016</p>
        </figcaption>
</figure>


<p>So you want to build an autoencoder? Great! This article will demonstrate how to build an autoencoder and use it to measure stock prices against an index. This technique is described in more technical terms <a href="https://arxiv.org/pdf/1602.06561.pdf">here</a>.</p>

<p>Once we&rsquo;ve trained the autoencoder, we can use it to measure how well each component follows the other members of the index. This can be useful for finding deeper insights into an index, and doesn&rsquo;t require a priori knowledge of the index price or the weighting of its components. Note, this is only one metric which one could use to determine how well one member of the group follows the group overall. Another might be <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson Correlation</a>.</p>

<h2 id="github-repository">Github repository</h2>

<p>To follow along with the code in this tutorial, please download the <a href="https://github.com/lukesalamone/stock-data-autoencoder">corresponding repository on Github</a>:</p>

<pre><code>git clone git@github.com:lukesalamone/stock-data-autoencoder.git
cd stock-data-autoencoder
pip install -r requirements.txt
</code></pre>

<h1 id="what-is-an-autoencoder">What is an autoencoder?</h1>

<p>An autoencoder is a neural network which encodes information back to itself. The structure of the network is such that the input layers (the &ldquo;encoder&rdquo;) will be large compared to the hidden layers (the &ldquo;code&rdquo;), forcing the network to compress information inside its hidden layers.</p>

<p>The idea of our autoencoder is that we would like to encode stock price information back to itself while discarding trends that aren&rsquo;t important. To do this, we will feed our network stock price data and ask the network to return those prices to us as outputs. Component stocks which are important to the index will be preserved well and thus be highly accurate, while components which are less important will not be well-preserved. We will measure the performance of the network on each component using mean squared error.</p>

<h1 id="the-model">The model</h1>

<p>We will use an autoencoder with a number of inputs and outputs equal to the number of component stocks in our index. For this exercise, we will use the <a href="https://en.wikipedia.org/wiki/S%26P_500">S&amp;P 500 index</a> which contains 505 components. This means our input and output size will be 505. We will also use a hidden layer with 5 units.</p>

<pre><code class="language-python">class StonksNet(nn.Module):
  def __init__(self, size):
    super().__init__()
      self.fc1 = nn.Linear(in_features=size, out_features=5)
      self.out = nn.Linear(in_features=5, out_features=size)

  def forward(self, x: Tensor) -&gt; Tensor:
    x = F.relu(self.fc1(x))
    x = F.relu(self.out(x))
    return x
</code></pre>

<h1 id="the-data">The data</h1>

<p>We will use daily stock prices downloaded using <a href="https://pypi.org/project/yfinance/">yfinance</a>. This data is readily available online and I recommend downloading it for yourself. We will use data between January 1, 1991 to January 1, 2021 (30 years of data).</p>

<p>To download the S&amp;P 500 stock data please run <code>gather_stocks.py</code> from the project directory:</p>

<pre><code>python gather_stocks.py
</code></pre>

<p>This will download all 505 components into the <code>stock_data</code> directory. Data will also be cleaned such that each component has the same number of days, which will be important when feeding it into the model.</p>

<h1 id="training-the-model">Training the model</h1>

<p>The model itself is a simple feed-forward neural network. As such, we use a standard training loop to train the model. We don&rsquo;t expect the loss to ever fall to zero during training since it is impossible for the network to perfectly encode and decode so many inputs into so few hidden code units. Some information will inevitably be lost. In my training, validation losses bottomed out at around 4000, but yours may be different depending on the initialization of your autoencoder.</p>

<p><img src="/img/autoencoder_validation_losses.png" alt="validation loss" /></p>

<h1 id="ranking-components">Ranking components</h1>

<p>Finally we&rsquo;re ready to rank the components of the S&amp;P 500 for &ldquo;closeness&rdquo;. After running <code>python train_model.py</code> you will see the best and worst components as scored by the autoencoder. Here were my results, yours may be different.</p>

<pre><code>best 5 results:
DRE: 16.66
LNT: 37.27
MU: 38.88
HOLX: 43.18
CERN: 47.46

worst 5 results:
HUM: 105244.19
SHW: 108542.73
LMT: 113654.48
C: 357073.88
NVR: 10955169.00
</code></pre>

<h1 id="future-research">Future research</h1>

<p>Upon inspection, it appears that better results might be achieved if we normalize the stock data before training. It appears that stocks with higher prices and higher volatility tended to perform worse than those with tight price ranges. In a way this is expected, since the autoencoder will naturally have a harder time modeling large values with a limited set of hidden units. However, normalizing the prices into similar ranges might be an interesting exercise to see if we can squeeze even more out of the model.</p>
]]></content>
        </item>
        
        <item>
            <title>Python: Formatting a string</title>
            <link>https://lukesalamone.github.io/posts/python-format-string/</link>
            <pubDate>Wed, 24 Feb 2021 21:22:42 -0600</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/python-format-string/</guid>
            <description>There are three main ways to format strings in python:
name = &#39;Luke&#39; food = &#39;pizza&#39; # old style &amp;quot;My name is %s and I like %s.&amp;quot; % (name, food) # str.format() &amp;quot;My name is {0} and I like {1}.&amp;quot;.format(name, food) # f-strings f&amp;quot;My name is {name} and I like {food}.&amp;quot;  </description>
            <content type="html"><![CDATA[<p>There are three main ways to format strings in python:</p>

<pre><code class="language-python">name = 'Luke'
food = 'pizza'

# old style
&quot;My name is %s and I like %s.&quot; % (name, food)

# str.format()
&quot;My name is {0} and I like {1}.&quot;.format(name, food)

# f-strings
f&quot;My name is {name} and I like {food}.&quot;
</code></pre>
]]></content>
        </item>
        
        <item>
            <title>Siamese Neural Networks (Video)</title>
            <link>https://lukesalamone.github.io/posts/siamese-nn-video/</link>
            <pubDate>Thu, 17 Dec 2020 11:22:43 -0600</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/siamese-nn-video/</guid>
            <description>The following is a transcript of the above video
In this paper, the authors present a novel neural network architecture to enable audio search via sounds humans are able to make, for example humming and whistling. This is an important capability when searching through audio for a specific sound.
Motivation Imagine you have hundreds of unlabeled sound effects on your computer, and you are looking for a specific one.</description>
            <content type="html"><![CDATA[

<div style="text-align:center">
  <iframe src="https://player.vimeo.com/video/491725663" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>
</div>

<p><em>The following is a transcript of the above video</em></p>

<p>In this paper, the authors present a novel neural network architecture to enable audio search via sounds humans are able to make, for example humming and whistling. This is an important capability when searching through audio for a specific sound.</p>

<h2 id="motivation">Motivation</h2>

<p>Imagine you have hundreds of unlabeled sound effects on your computer, and you are looking for a specific one. It could be very tedious to listen to every single one until you can find the right sound. Even if the sounds do have some kind of word labels, it could be hard to pinpoint exactly which words to search for. A lot of sounds don’t exactly lend themselves to text descriptors, so finding the right sound can be difficult with a text search.</p>

<p>This paper contains three main contributions:</p>

<p>First, it introduces a new neural network architecture for matching imitated sounds with a sound corpus, the semi-siamese convolutional neural network.</p>

<p>Second, the researchers built a second architecture which utilizes transfer learning from other audio tasks in an attempt for better performance.</p>

<p>Third, the researchers visualized and sonified input patterns which excited the neurons in different layers.</p>

<p>Both neural networks outperform the state of the art systems as we will see later on.</p>

<h2 id="data">Data</h2>

<p>To train the siamese model, the authors used a dataset called VocalSketch, which contains sounds from 4 broad categories: acoustic instruments, commercial synthesizers, everyday, and single synthesizer notes. The dataset also contains a number of human vocal imitations of each of the sounds.
For each of the 4 categories in VocalSketch, the researchers selected half of the sounds in each category along with corresponding vocal imitations as the training and validation set, and the other half for their test set.</p>

<p>Since each of the categories other than Everyday contained 40 sounds, 20 sounds would be for training and 20 for test and validation. The everyday category contained 120 sounds, so that category had 60 in training and 60 in test.
Each sound in the dataset has 10 corresponding human vocal imitations. In the training set, 7 of them for each sound were selected for training, and the remaining 3 were used for validation. Overall, the researchers used this dataset to create 840 positive and 840 negative pairs for training, and 360 positive and negative pairs for validation.</p>

<p>I thought it was interesting that the researchers opted not to use balanced categories of 20 sounds for each category. There isn’t a comment in the paper about the reasoning behind this but it may be due to the difficulty of categorizing this class of sounds.</p>

<p>The transfer learning model required pretraining the two towers on two additional datasets before training the full network on VocalSketch.</p>

<p>For the vocal imitation tower, they used a dataset called VoxForge. From this dataset the researchers selected 8 thousand samples for each of 7 different languages. They used a 70 - 30 split for training and testing, and achieved a 69.8% accuracy, which seems pretty good for a 7 class classification task.</p>

<p>For the second transfer learning tower, the environmental sound classification tower, they used a dataset called UrbanSound8k. This dataset contains 8732 sound samples in 10 different classes, things like car horns, jackhammers, and street music. The researchers used 10 fold cross validation when pretraining on this dataset and achieved a 70.2% accuracy over the dataset.</p>

<p>Note that for pre-training the two towers, the researchers used a slightly modified neural network architecture, appending two fully connected layers to categorize the results into the necessary number of classes.</p>

<p>So how are sounds fed into the neural networks? You’re probably familiar with the way that convolutional neural networks work with images. Typically the input for each image is of a shape width by height by color depth. Creating an input when working with sound is similar. The width of the audio file in this case is time, and the height is the frequencies during that time step. This is similar to the output of a spectrogram.</p>

<p>In order to be fed into the network, audio must first undergo a preprocessing step. The specific preprocessing involved varied between the networks, but generally involves downsampling the audio and splitting it by frequency band. This resulted in an input which resembles a spectrogram image.</p>

<h2 id="methodology">Methodology</h2>

<p>The heart of this problem is an architecture the researchers dubbed siamese style or semi siamese neural networks. A true siamese neural network consists of two identical neural network towers with the same weights, and is used similar to the way that hashing is used to match similar inputs. A siamese style network is similar to a siamese neural network, but may have different weights in one of its towers.</p>

<p>The first network the researchers called IMINET, which included a true siamese neural network as one of its configurations. This network consisted of two convolutional network towers, a concatenation step, and a fully-connected network with three more layers to compute a similarity score between 0 and 1. The convolutional neural networks in IMINET had the same structure, even though in some configurations their weights were not the same. They consisted of four convolutional layers with pool layers following convolutional layers 1 and 2.</p>

<p>The researchers detailed the parameters they used for each of the convolutional layers, specifically the number of filters and the receptive field size. Each of the filters in a layer learns to detect various characteristics of the input, and the receptive field is the part of the input that the filter is able to see. The max pooling layers output the maximum value among all of their inputs, reducing the number of inputs for the next layer in the network.</p>

<p>The researchers experimented with three different configurations for the convolutional network towers: a tied configuration, where both towers would share the exact same weights and biases; an untied configuration, where the two towers were required to share no weights and biases at all; and a partially tied configuration, where weights and biases were shared for convolutional layers 3 and 4 but not 1 and 2. Because the untied and partially tied configurations are not truly siamese neural networks, the researchers called these configurations semi siamese.</p>

<p>After both inputs pass through the convolutional towers, they are concatenated together into one input vector and fed into a fully connected network with three layers. This network’s job is to compute a similarity score between 0 and 1 for the two inputs. Both of the first two layers used ReLU activation. The final layer was a single neuron which used sigmoid activation to squash its input into an output between 0 and 1.</p>

<p>This architecture achieved state of the art results in sound retrieval which I will discuss in a moment along with the rest of the findings of this paper.</p>

<p>The second neural network developed by researchers was called TL-IMINET. This network was very similar to the first network, but this time the researchers tried using transfer learning to achieve better performance. The researchers hypothesized that the vocal imitation task shares many characteristics with language identification, and that sound recognition shares many similarities with environmental sound classification. This would allow networks which were pre-trained on these tasks to require only fine tuning to be adapted to this task.</p>

<p>The network architectures for the language identification and environmental classification tasks were slightly different from those used in IMINET and are shown here. Note that the towers are also different in architecture from each other.</p>

<p>The researchers also experimented with fusion strategies between different models. For IMINET, the similarity likelihoods for all three configurations were multiplied together to achieve a combined score. They also experimented with combining IMINET with the previous state of the art model. Since that model computes a cosine distance between the input sound and a candidate sound, this output was converted into a likelihood using softmax, and that softmax was multiplied by the output of IMINET. The transfer learning model TL-IMINET was also combined with the state of the art model in a similar way by computing the softmax and multiplying by the output of TL-IMINET. These fusion strategies ended up improving the performance of each of the models quite a bit.</p>

<h2 id="experiments-and-findings">Experiments and Findings</h2>

<p>To measure the performance of these networks, recall that the output of the networks was a number between 0 and 1 indicating how similar the network believed the two inputs were to each other. For example, two very similar inputs might have a similarity rating of 0.9, while two dissimilar inputs might have a similarity rating of 0.1. After gathering the similarity of the human vocalization sound to each of its potential matches, the matches were ranked according to their similarity score.</p>

<p>The authors then used a metric called mean reciprocal rank, which is a number between 0 and 1 indicating how well the algorithm ranked the sounds. For example, a mean reciprocal rank of 0.5 suggests that the target sound is ranked second among all possible matches on average.</p>

<p>Here are the performances of the various network configurations when measured by mean reciprocal rank. The researchers highlighted several insights which could be drawn from their results.</p>

<p>First, it seems that tied configurations performed the best among all configurations of IMINET. This runs contrary to the researchers’ expectations that untied configurations would outperform tied configurations.</p>

<p>Second, the tied configuration outperformed the previous state of the art benchmark in two categories of sounds: commercial synthesizers and everyday. It performed worse than the state of the art for acoustic instruments, and was about the same performance for the single synthesizer category.</p>

<p>Third, IMINET achieved even better performance in most categories by using an ensemble of different configurations.</p>

<p>Fourth, even without pretraining, the TL-IMINET model performed better than the untied configuration of IMINET for all categories except commercial synthesizers. This is interesting because the only difference between these two models is the network structure of the convolutional towers.</p>

<p>And finally, the pre-trained TL-IMINET model outperformed the previous state of the art model by quite a bit in all categories, but the best performing configuration overall was TL-IMINET fused with the previous state of the art model.</p>

<p>One of the most interesting experiments was the visualization of the input patterns which activate neurons in each of the layers. This was done by performing gradient ascent of the neuron activation with respect to the input from a random initialization state. Visualizing the convolutional layers showed that the first layer tends to learn local features like edges, intermediate layer neurons learn more complex features like texture and direction, and the deepest layer recognizes concentrations of frequency ranges. The visualizations also helped to confirm that pretraining indeed helped the networks to learn more detail. The patterns from the pretrained vocal imitation tower were sharper than those in the naive IMINET towers.</p>

<h2 id="takeaways">Takeaways</h2>

<p>There are a few key takeaways from this research. The first is that the transfer learners had much better performance than the naive non-transfer learner, as evidenced by the fact that TL-IMINET performed better than IMINET for most categories even though neither model was pretrained. The research also showed that ensemble methods can outperform any single model on its own. IMINET performed better when used in combination with its different configurations, and combining it with the state of the art model performed better than either model on its own. Finally, visualizing the inputs can help to confirm that the network is learning the correct things, and helps to provide insights as to what types of sound properties are important.</p>
]]></content>
        </item>
        
        <item>
            <title>Managing Python Environments</title>
            <link>https://lukesalamone.github.io/posts/managing-python-environments/</link>
            <pubDate>Sat, 24 Oct 2020 17:45:41 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/managing-python-environments/</guid>
            <description>Need to switch between python versions often? Use pyenv.
Installing pyenv # install pyenv curl https://pyenv.run | bash # check pyenv install location which pyenv  Install another python version # see a list of available python versions pyenv install --list # check installed python versions pyenv versions # installs python 3.7.5 pyenv install 3.7.5  Switch python versions # use python 3.7.5 everywhere on your machine pyenv global 3.</description>
            <content type="html"><![CDATA[

<p>Need to switch between python versions often? Use <a href="https://github.com/pyenv/pyenv"><code>pyenv</code></a>.</p>

<h3 id="installing-pyenv">Installing pyenv</h3>

<pre><code class="language-bash"># install pyenv
curl https://pyenv.run | bash

# check pyenv install location
which pyenv
</code></pre>

<h3 id="install-another-python-version">Install another python version</h3>

<pre><code class="language-bash"># see a list of available python versions
pyenv install --list

# check installed python versions
pyenv versions

# installs python 3.7.5
pyenv install 3.7.5
</code></pre>

<h3 id="switch-python-versions">Switch python versions</h3>

<pre><code class="language-bash"># use python 3.7.5 everywhere on your machine
pyenv global 3.7.5

# use python 3.7.5 in current directory
pyenv local 3.7.5

# use python 3.7.5 in current shell session
pyenv shell 3.7.5
</code></pre>
]]></content>
        </item>
        
        <item>
            <title>What is the Hardest Hangman Word?</title>
            <link>https://lukesalamone.github.io/posts/hardest-hangman-word/</link>
            <pubDate>Tue, 21 Jul 2020 17:34:05 +0800</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/hardest-hangman-word/</guid>
            <description>It seems like a simple enough question. Which word should you choose so that it takes your opponent the most guesses to discover it? Should you choose a long word to use up your opponent&amp;rsquo;s guesses? Or perhaps a short word with obscure letters? In this document I look into this question. But first, a bit of background.
If you&amp;rsquo;re not familiar with the rules of hangman, it is a guessing game played between two people.</description>
            <content type="html"><![CDATA[

<p><img src="https://i.imgur.com/p33HisS.png" alt="Example hangman game" /></p>

<p>It seems like a simple enough question. Which word should you choose so that it takes your opponent the most guesses to discover it? Should you choose a long word to use up your opponent&rsquo;s guesses? Or perhaps a short word with obscure letters? In this document I look into this question. But first, a bit of background.</p>

<p>If you&rsquo;re not familiar with the rules of hangman, it is a guessing game played between two people. Player A chooses a secret word, and tells player B the length of the secret word. Player B guesses letters which she thinks might be in the word. If she chooses a correct letter, player A reveals the locations of <em>each instance</em> of the guessed letter. However, if player B guesses an incorrect letter, this counts as a &ldquo;strike&rdquo; against her. After an agreed-upon number of strikes, player B loses.</p>

<h1 id="an-algorithmic-approach">An algorithmic approach</h1>

<p>A few years ago, I created a &ldquo;hangman solver&rdquo; for the popular paper and pencil game. This game assessed each game analytically, to determine a list of possible words given the clues available. The algorithm works as follows: at the beginning of the game, we know the length of the secret word, which narrows our dictionary considerably. Then, for each letter in the alphabet, count the number of words available which contain that letter.</p>

<p>Suppose our dictionary consisted of a random list of 50 four-letter words as follows:</p>

<pre><code class="language-javascript">[&quot;pull&quot;, &quot;dipt&quot;, &quot;dorp&quot;, &quot;poky&quot;, &quot;jism&quot;, &quot;cues&quot;, &quot;hood&quot;, &quot;drag&quot;,
&quot;inky&quot;, &quot;mhos&quot;, &quot;kerf&quot;, &quot;jess&quot;, &quot;mete&quot;, &quot;lues&quot;, &quot;wipe&quot;, &quot;kane&quot;,
&quot;tiro&quot;, &quot;keys&quot;, &quot;jape&quot;, &quot;lime&quot;, &quot;sees&quot;, &quot;sass&quot;, &quot;demo&quot;, &quot;ilia&quot;,
&quot;mink&quot;, &quot;dips&quot;, &quot;hove&quot;, &quot;jees&quot;, &quot;that&quot;, &quot;pops&quot;, &quot;isle&quot;, &quot;teas&quot;,
&quot;dens&quot;, &quot;dogy&quot;, &quot;pink&quot;, &quot;sizy&quot;, &quot;cole&quot;, &quot;pact&quot;, &quot;thaw&quot;, &quot;lead&quot;,
&quot;mile&quot;, &quot;dodo&quot;, &quot;litu&quot;, &quot;scup&quot;, &quot;colt&quot;, &quot;soma&quot;, &quot;seat&quot;, &quot;dewy&quot;,
&quot;pits&quot;, &quot;mojo&quot;]
</code></pre>

<p>This would result in letter counts as follows:</p>

<table>
<thead>
<tr>
<th>letter</th>
<th>count</th>
</tr>
</thead>

<tbody>
<tr>
<td>a</td>
<td>16</td>
</tr>

<tr>
<td>b</td>
<td>7</td>
</tr>

<tr>
<td>c</td>
<td>15</td>
</tr>

<tr>
<td>d</td>
<td>5</td>
</tr>

<tr>
<td>e</td>
<td>26</td>
</tr>

<tr>
<td>f</td>
<td>1</td>
</tr>

<tr>
<td>g</td>
<td>5</td>
</tr>

<tr>
<td>h</td>
<td>3</td>
</tr>

<tr>
<td>i</td>
<td>10</td>
</tr>

<tr>
<td>j</td>
<td>1</td>
</tr>

<tr>
<td>k</td>
<td>6</td>
</tr>

<tr>
<td>l</td>
<td>14</td>
</tr>

<tr>
<td>m</td>
<td>6</td>
</tr>

<tr>
<td>n</td>
<td>6</td>
</tr>

<tr>
<td>o</td>
<td>14</td>
</tr>

<tr>
<td>p</td>
<td>7</td>
</tr>

<tr>
<td>r</td>
<td>8</td>
</tr>

<tr>
<td>s</td>
<td>15</td>
</tr>

<tr>
<td>t</td>
<td>13</td>
</tr>

<tr>
<td>u</td>
<td>4</td>
</tr>

<tr>
<td>w</td>
<td>1</td>
</tr>

<tr>
<td>x</td>
<td>1</td>
</tr>

<tr>
<td>y</td>
<td>4</td>
</tr>
</tbody>
</table>

<p>In this case, the letter E is found in 26 words, the most of any letter. Therefore our algorithm should pick E since it is the safest guess. Once we guess a letter correctly, this gives important positional information which can filter the word list even further.</p>

<p>This process is repeated, each time picking the most likely letter, given the constraints we know. If we know some of the letters in the secret word, we can eliminate any words that don&rsquo;t have those letters in those positions. If we have guessed a letter incorrectly, we know that letter isn&rsquo;t in the secret word and can eliminate all words which have that letter. You can see a python implementation of this algorithm <a href="https://gist.github.com/lukesalamone/a815cda5e427b28db78e0caafdbbc0f3#file-hangman_solver-py">here</a>.</p>

<p>For this experiment I used the <a href="http://www.blogmybrain.com/words-with-friends-cheat/words.txt">Zynga dictionary</a>, the same dictionary used in the game Words With Friends. The dictionary contains 173,000 words. It does not include any proper nouns or profanities.</p>

<h1 id="live-experiment">Live experiment</h1>

<p>Below you can experiment with this algorithm among 4 letter words. Enter a secret word, and the steps used to uncover the secret word will be displayed below. (Need inspiration? Try comparing <em>jazz</em> to <em>rock</em> or <em>blue</em> to <em>grey</em>.)</p>

<div id="demo1" style="margin:20px;background:#ddd;padding:20px;font-family:monospace">
    <script src="/js/hangman-demo1.js"></script>

    <div style="width:100%; text-align:center">
        <input placeholder="enter 4 letter word"></input>
        <button class="play">play word</button>
        <button class="reset">reset</button>
    </div>
    <div class="output" style="margin: 20px 0"></div>
</div>

<p>Notice how some words have much higher difficulty than others? This is due to the fact that some words have many &ldquo;siblings&rdquo; which differ by only one letter. For example the pattern &ldquo;.ays&rdquo; could match the letters c, d, f, h, j, k, l, n, or r. Knowing the last 3 letters gives us no indication of which of these nine letters it will be.</p>

<p>One apparent shortcoming of the above calculation is that it assumes that letters with equal probability will be picked in alphabetical order, and therefore letters last in the alphabet will be picked last. Although this has the benefit of creating a deterministic algorithm which will always return the same result for the same word, in real life we don&rsquo;t know in which order people will pick letters. In actuality the words <em>days</em>, <em>jays</em> and <em>rays</em> have equal difficulty each is equally likely to be the secret word.</p>

<h1 id="preliminary-results">Preliminary results</h1>

<p>With this preliminary caveat in mind, we can still calculate the difficulty of every word in the dictionary. If we assume that in a situation where multiple letters are equally probable, our opponent will break the tie using alphabetical ordering, which hangman words are the hardest to guess? Here are the top 17:</p>

<table>
<thead>
<tr>
<th>word</th>
<th>difficulty</th>
</tr>
</thead>

<tbody>
<tr>
<td>zill</td>
<td>19</td>
</tr>

<tr>
<td>zills</td>
<td>18</td>
</tr>

<tr>
<td>yill</td>
<td>18</td>
</tr>

<tr>
<td>zin</td>
<td>17</td>
</tr>

<tr>
<td>zax</td>
<td>17</td>
</tr>

<tr>
<td>yills</td>
<td>17</td>
</tr>

<tr>
<td>will</td>
<td>17</td>
</tr>

<tr>
<td>vox</td>
<td>17</td>
</tr>

<tr>
<td>mem</td>
<td>17</td>
</tr>

<tr>
<td>zins</td>
<td>16</td>
</tr>

<tr>
<td>yuck</td>
<td>16</td>
</tr>

<tr>
<td>yin</td>
<td>16</td>
</tr>

<tr>
<td>wills</td>
<td>16</td>
</tr>

<tr>
<td>vill</td>
<td>16</td>
</tr>

<tr>
<td>oak</td>
<td>16</td>
</tr>

<tr>
<td>jazz</td>
<td>16</td>
</tr>

<tr>
<td>foy</td>
<td>16</td>
</tr>

<tr>
<td>(27 more)</td>
<td>15</td>
</tr>
</tbody>
</table>

<h1 id="previous-research">Previous research</h1>

<p>I should note that <a href="https://web.archive.org/web/20100815092214/http://blog.wolfram.com/2010/08/13/25-best-hangman-words/">previous research by Jon McLoone in 2010</a> has explored the same topic, although he used slightly different methodology and a smaller 90,000 word dictionary. His algorithm was not deterministic, and so does not always pick the most frequent letter available. Instead, his algorithm picks a letter with a probability proportional to the frequency with which it occurs in candidate words. For example, if we refer to the letter frequencies of the 50 word 4-letter dictionary above, <em>j</em> appears in just 1 word, while <em>e</em> appears in 26 of them. In this case, since the sum of the numbers in the frequency table is 188, McLoone&rsquo;s algorithm would pick <em>z</em> in 1 out of every 188 first guesses, and <em>e</em> in 26 of them.  Although it might seem that this strategy is not optimal, it does avoid the deterministic results shown above.</p>

<p>Additionally, McLoone chose to remain faithful to the logic of the hangman game, opting to end the games after a given number of mistakes, and recording the probability that a given word was not discovered after the game ended. So an 8-game means that the game was ended after 8 mistakes, and a 13-game after 13 mistakes. Using this methodology, he found the hardest hangman words were as follows:</p>

<p><img src="https://i.imgur.com/N5ginNw.gif" alt="Jon McLoone results" /></p>

<h1 id="future-research">Future research</h1>

<p>Now, I think that we can improve upon our results a bit. Rather than calculating difficulty deterministically, we can instead randomize the ordering which letters will be picked in. This should dramatically reduce some of the outliers from above, bringing &ldquo;rays&rdquo; down from a difficulty of 14 to something more reasonable.</p>

<p>Such a stochastic calculation will require simulating millions of games. For my 173,000 word dictionary, simply simulating 10 games would involve 1.7 million games. Fortunately, this operation is highly parallelizable. It should be possible to split the dictionary into 100 or even 1000 pieces and derive the results for each piece simultaneously.</p>

<p>A simulation of this algorithm is shown below, with a graph of the average number of mistakes the new randomized algorithm incurs before discovering your secret word. The simulation is set up to play 500 rounds of games, and the final average is displayed at the top.</p>

<div id="demo2" style="margin:20px;background:#ddd;padding:20px;font-family:monospace">
    <script src="/js/chart.min.js"></script>
    <script src="/js/hangman-demo2.js"></script>

    <div style="width:100%; text-align:center">
        <input placeholder="enter 4 letter word"></input>
        <button class="play">play word</button>
        <button class="reset">reset</button>
    </div>
    <div class="messages"><span></span></div>
    <div class="canvas-holder" style="display:none;width:500px;height:500px">
        <canvas></canvas>
    </div>
</div>
]]></content>
        </item>
        
        <item>
            <title>Estimating Pi with a Monte Carlo Simulation</title>
            <link>https://lukesalamone.github.io/posts/monte-carlo/</link>
            <pubDate>Thu, 09 Jul 2020 15:40:14 +0800</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/monte-carlo/</guid>
            <description>MathJax.Hub.Config({ tex2jax: { inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]], displayMath: [[&#39;$$&#39;,&#39;$$&#39;], [&#39;\[&#39;,&#39;\]&#39;]], processEscapes: true, processEnvironments: true, skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;], TeX: { equationNumbers: { autoNumber: &#34;AMS&#34; }, extensions: [&#34;AMSmath.js&#34;, &#34;AMSsymbols.js&#34;] } } });  MathJax.Hub.Queue(function() { // Fix  tags after MathJax finishes running. This is a // hack to overcome a shortcoming of Markdown. Discussion at // https://github.com/mojombo/jekyll/issues/199 var all = MathJax.Hub.getAllJax(), i; for(i = 0; i A Monte Carlo simulation is a method of estimating events or quantities which are difficult or computationally infeasible to derive a closed-form solution to.</description>
            <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<p>A Monte Carlo simulation is a method of estimating events or quantities which are difficult or computationally infeasible to derive a closed-form solution to. The value of the mathematical constant Pi is a good example of this: although it is possible to calculate the exact value of Pi, a good estimate is easily demonstrated with just a few lines of code.</p>

<p>Monte Carlo simulations work when the input can be drawn from a random probability distribution, and the outcome can be derived deterministically from the input. In the case of calculating Pi, this can be modeled geometrically. The <em>random distribution</em> is all points within the square, and the outcome is whether a selected point lies within the circle inside of the square. We know that for a square circumscribed about a circle,</p>

<p><code>$ A_{circle} = \pi r ^ 2, A_{square} = (2r) ^ 2 = 4r ^ 2 $</code></p>

<p><code>$ { A_{circle}  \over  A_{square} } = { \pi r^2  \over 4r ^ 2 } = { \pi \over 4 } $</code></p>

<p><code>$ { 4 ({ A_{circle}  \over  A_{square} }) } = \pi $</code></p>

<p>If we notice that the probability that a randomly placed dot will fall within the circle is the same as the ratio of their areas (i.e. the circle takes up about 78% of the area of the square, so a random dot has about a 78% chance of landing inside the circle), then multiplying that probability by 4 gives Pi. By placing dots randomly, we play out that probability in real-time. As to whether a given dot lies within the circle, we simply use the Pythagorean theorum to calculate its distance from the origin:</p>

<p><code>$ \sqrt{ x^2 + y^2 } &lt; 1 $</code></p>

<p>So all dots greater than 1 unit from the origin are outside the circle.</p>

<p>Below is a simulation of the derivation of the value of Pi. Click &ldquo;start simulation&rdquo; to see for yourself. <strong>Warning:</strong> this simulation may become slow once many dots are drawn on the screen.</p>

<div id="sim1" style="font-family: monospace; background-color: #444; margin: 20px 0">
    <script src="/js/chart.min.js" type="text/javascript"></script>
    <script src="/js/monte-carlo.js" type="text/javascript"></script>

    <div style="padding: 20px; text-align: center">
        <button class="start">start simulation</button>
        <button class="stop" disabled>stop simulation</button>
    </div>
    <div style="width:500px; height:500px">
        <canvas></canvas>
    </div>
    <div style="color: #fff; margin-left: 20px;">
        <p style="margin:0" id="num-points">Number of points: 0</p>
        <p style="margin:0" id="pct-inside">% inside circle: 0</p>
        <p style="margin:0" id="pi">Approximate value of pi: 0</p>
    </div>
</div>

<p>Next, I was interested in the way that this simulation would play out once the number of points became large. Intuitively, this should approach Pi, but doing so requires that the random numbers generated by browsers be evenly distributed.</p>

<div id="sim2" style="font-family: monospace; background-color: #444; margin: 20px 0">
    <div style="padding: 20px; text-align: center">
        <button class="start">start simulation</button>
        <button class="stop" disabled>stop simulation</button>
    </div>
    <div style="width:600px; height:700px">
        <canvas></canvas>
    </div>
    <div style="color: #fff; margin-left: 20px;">
        <p style="margin:0" class="num-points">Number of points: 0</p>
        <p style="margin:0" class="pi">Approximate value of pi: 0</p>
        <p style="margin:0" class="pct-error">% error: 0</p>
    </div>
</div>
]]></content>
        </item>
        
        <item>
            <title>Creating an AI for Gomoku</title>
            <link>https://lukesalamone.github.io/posts/gomoku2049/</link>
            <pubDate>Tue, 19 May 2020 14:28:57 +0800</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/gomoku2049/</guid>
            <description>Gomoku is a strategy game similar to tic tac toe, but played on a larger board and with the goal of getting 5 in a row rather than 3. Since the game has perfect information and has simple rules, I thought it would be a fun exercise in creating a game AI. In February 2020 I decided to code up Gomoku2049. The game is a demonstration of MiniMax, which is an algorithm for finding the move which minimizes the opponent’s best moves.</description>
            <content type="html"><![CDATA[

<p>Gomoku is a strategy game similar to tic tac toe, but played on a larger board and with the goal of getting 5 in a row rather than 3. Since the game has perfect information and has simple rules, I thought it would be a fun exercise in creating a game AI.
In February 2020 I decided to code up Gomoku2049. The game is a demonstration of MiniMax, which is an algorithm for finding the move which minimizes the opponent’s best moves. This article is an overview of the game’s technical highlights.</p>

<p><a href="http://gomoku2049.s3-website-us-east-1.amazonaws.com/#">Click here to try out the game!</a></p>

<h2 id="minimax-with-alpha-beta-pruning">Minimax with alpha-beta pruning</h2>

<figure><img src="/img/game-tree.png"
         alt="In the tree above, the current game is shown on the left, green to move. If green fails to block orange’s 3 in a row now, orange cannot be stopped."/><figcaption>
            <p>In the tree above, the current game is shown on the left, green to move. If green fails to block orange’s 3 in a row now, orange cannot be stopped.</p>
        </figcaption>
</figure>


<p>The Minimax algorithm represents every game as a tree of moves, with the current game position at the root of the tree. The algorithm is recursive with exponential time complexity and can have a very high branching factor: after the first move there are 225–1=224 possible moves. Because it is not feasible to evaluate all possible games to completion, Minimax calculation is usually limited to a fixed depth, after which the algorithm evaluates terminal leaf nodes using the gameover function and the static evaluator.</p>

<p>After each human move (known as “plies”), Minimax assigns a score to each of the possible reply moves. By convention, the AI will score favorable moves with a positive score, and unfavorable moves with a negative score. The move corresponding to the highest score is then selected. In other words, the AI is called the “maximizer”. Likewise, the human is known as the minimizer. To determine the score of each possible move, the minimax algorithm will recursively either maximize or minimize the possible moves available. After a given depth, the evaluation will stop, and return either an infinite value (+∞ for an AI win, -∞ for human win) or a finite evaluation of the state of the board. This static evaluation can be rather expensive, but luckily even a rough approximation is effective.</p>

<p>In practice, in addition to a depth limitation, this minimax algorithm also reduces the branching factor by limiting the squares which will be evaluated to those which are adjacent to squares which have been played. Given the fact that a disconnected “island” square cannot immediately lead to a win, this seems to be a reasonable simplification.
At the leaf nodes of the tree, either the game is over (the human has won or the computer has won) or the board needs to be evaluated with regards to who is winning.</p>

<h2 id="alpha-beta-pruning">Alpha-beta pruning</h2>

<p>Alpha-beta pruning is an improvement on the minimax algorithm, reducing the number of branches and leaf nodes which need to be evaluated. This is achieved by “pruning” unnecessary branches, ignoring them because the parent minimizer/maximizer would never choose it. For a maximizer (whose parent is a minimizer), this will occur if the parent minimizer has already seen a lower evaluation than a number the maximizing child sees.</p>

<h2 id="static-evaluator">Static evaluator</h2>

<p>This function is used to evaluate a board position with regards to which player is winning, and by how much. The MiniMax algorithm will then choose the highest value for itself, while minimizing the options for its opponent. For gomoku, it was important to derive an evaluation function which could be calculated quickly, and which builds towards the final desired result of 5 squares in a row. Note that such a function would necessarily be isomorphic in four directions: vertical, horizontal, and on both diagonals.</p>

<p>My initial thought was that this would be extremely computationally expensive. There are many permutations of selected squares which can lead to a win, and many which do not. For example, XX — — OOO — — XX with O to move will lead to a win for O, but with X to move will not. However, I convinced myself that any static evaluation which built towards a win would find winning nodes at sufficient depth, so finding extremely detailed evaluation was less important than a general approximation.</p>

<p>Building from this thought, I decided to count the number of 4-in-a-rows (4s) and give them a high score, along with the 3s and 2s. Each in-a-row would be given an exponentially increasing “reward”, so that 4s scores much higher than 2×2s. For example, the payout function might be f(n) = 2^N for 2, 3, and 4 so that f(4) = 16 and 2×f(2) = 8. This ensures the desired result, that the optimal configuration of N squares is Ns.</p>

<p>Eventually, I determined that it was sufficient to simply count 2s with overlaps, since allowing double counts would still favor longer sequences of squares, but would not require separate checks for each length. Therefore, if 2s was rewarded 1, then XXX would be rewarded 2, and XXXX would be rewarded 3. This means that 4s is still more the most efficient configuration of four squares, since XX — XX only evaluates to 2.</p>

<h2 id="gameover-function">Gameover function</h2>

<p>This function simply needs to return true if the game is over and a player has won. After the simplifications to the static evaluator, the gameover function behaves almost identically. Instead of counting 2s, we check for the presence of 5s.</p>

<h2 id="bitmasks">Bitmasks</h2>

<p>Here is where the fun begins. I realized that a very efficient way of representing a game board was with a sequence of bits, where 1 represented an occupied square, and 0 represented an unoccupied square. A game state would therefore only require a bit sequence for each player (the game engine would prevent overlapping bits). For a 15×15=225 square board, each player’s occupied squares could be represented with a number 225 bits in length. Although Javascript Numbers are only 53 bits long, Javascript has a newer primitive, BigInt, which can store numbers of arbitrary length.</p>

<p>The biggest benefit of representing the game board this way is that it facilitates bitwise operations, which drastically reduces the time complexity for the static evaluator and gameover functions.</p>

<figure><img src="/img/bitmask.gif"
         alt="Here the mask is shown in white, and the actual squares occupied are shown in orange. With each step in the bitmask check, the board and the mask are bitwise ANDed together, a very fast operation which reduces the computational complexity required in the static evaluator and gameover function."/><figcaption>
            <p>Here the mask is shown in white, and the actual squares occupied are shown in orange. With each step in the bitmask check, the board and the mask are bitwise ANDed together, a very fast operation which reduces the computational complexity required in the static evaluator and gameover function.</p>
        </figcaption>
</figure>


<h2 id="about-bigint">About BigInt</h2>

<p>The BigInt primitive is a <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/BigInt">newer built-in type</a> in Javascript, and as such is unsupported in some browsers. In particular, Internet Explorer and Safari do not have BigInt as a primitive. Although there are polyfills available for BigInt, they do not have the same performance as the native type. I decided that as a demonstration of the Minimax algorithm, supporting all browsers was not a priority.</p>

<h2 id="web-workers">Web Workers</h2>

<p>Most people know that Javascript is single threaded. It is, except when it isn’t. Web Workers are a way of multithreading in the browser, which in this context is pretty important because it helps to avoid freezing the user interface. In this game, the board state is handed off to a Web Worker thread, which computes the best move and returns it to the main thread. Progress is reported back periodically to the main thread as well, which is shown in a progress bar underneath the Gomoku2049 logo.</p>

<p>Theoretically, I could have taken further advantage of multithreading when creating this game. Each branch in the decision tree can be parallelized, allowing for simultaneous computation of each node’s value. For example, a new thread could be used to evaluate each of the AI’s possible moves. Unfortunately, the number of possible moves for the AI can be quite high later in the game, and browsers limit the number of Web Workers allowed (Chrome allows 60, Firefox allows 20, etc.) so instead of spawning a new worker for each top level branch, threads would need to be spawned from a shared thread pool.</p>

<p><a href="https://github.com/lukesalamone/gomoku-2049">The full source code for this game can be found here.</a></p>
]]></content>
        </item>
        
    </channel>
</rss>
