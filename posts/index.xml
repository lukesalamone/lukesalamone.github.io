<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Luke Salamone&#39;s Blog</title>
        <link>https://lukesalamone.github.io/posts/</link>
        <description>Recent content in Posts on Luke Salamone&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Wed, 29 May 2024 23:23:17 -0700</lastBuildDate>
        <atom:link href="https://lukesalamone.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>How does HNSW work?</title>
            <link>https://lukesalamone.github.io/posts/how-does-hnsw-work/</link>
            <pubDate>Mon, 20 May 2024 13:38:01 -0700</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/how-does-hnsw-work/</guid>
            <description>Suppose we have a vector database with a billion items in it (the haystack). And suppose we are looking for K vectors, the needles which maximize some similarity function. (In the case of cosine similarity or euclidean distance, we may be maximizing 1-distance(x,y).) And also suppose that we&amp;rsquo;d like to do this quickly.
Naive and semi-naive approaches One approach might be to compare every vector and take the argmax. In that case, for vectors of length D our runtime will be 1 billion x D.</description>
            <content type="html"><![CDATA[<p>Suppose we have a vector database with a billion items in it (the <em>haystack</em>). And suppose we are looking for K vectors, the <em>needles</em> which maximize some similarity function. (In the case of cosine similarity or euclidean distance, we may be maximizing <code>1-distance(x,y)</code>.) And also suppose that we&rsquo;d like to do this quickly.</p>
<h2 id="naive-and-semi-naive-approaches">Naive and semi-naive approaches</h2>
<p>One approach might be to compare every vector and take the argmax. In that case, for vectors of length D our runtime will be <code>1 billion x D</code>.</p>
<pre><code class="language-python">def search(needle, haystack, arg_k=1):
    # normalize needle and haystack vectors
    needle /= np.linalg.norm(needle)
    haystack /= np.linalg.norm(haystack, axis=1, keepdims=True)
    
    # compute cosine similarities
    similarities = np.dot(haystack, needle)
    
    # get the indexes of the top k elements
    idx_topk = np.argpartition(a, -k)[-k:]

    # return haystack vector with highest cosine similarity
    return haystack[idx_topk]
</code></pre>
<p>In practice, there are some optimizations we can try which can increase the practical speed of this operation:</p>
<ul>
<li>We can parallelize computations on-device with <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a> or <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">BLAS</a>.</li>
<li>We can parallelize computations on-device with GPU.</li>
<li>We can parallelize across cores for machines with multiple cores.</li>
<li>We can parallelize across devices. For example, partitioning the database across 10 clusters, finding a &ldquo;winner&rdquo; from each cluster, and then repeating the search across the 10 winners.</li>
</ul>
<p>There are also some relatively simple but &ldquo;destructive&rdquo; techniques we can also try:</p>
<ul>
<li>We can try pruning the dataset in some way beforehand. For example, maybe we don&rsquo;t need to consider old or unpopular vectors.</li>
<li>We can try reducing the dimensionality of the vectors with something like PCA.</li>
<li>We can try quantizing the vectors e.g. <code>f64</code> -&gt; <code>f16</code> reduces memory requirements 4x. (Usually this requires quantization-aware training.)</li>
</ul>
<h2 id="hierarchical-navigable-small-worlds-hnsw">Hierarchical Navigable Small Worlds (HNSW)</h2>
<p>Sometimes linear time is still too slow. In that case, we can consider an <em>approximate</em> solution, where some acceptable percent of vectors returned are actually not in the top K nearest neighbors. This is called approximate nearest neighbors (ANN). If we have a large system, it&rsquo;s possible to <a href="https://sbert.net/examples/applications/retrieve_rerank/">use another more precise model to rerank</a> the vectors results anyways. In that case, we just need to set K large enough to reliably include the vectors we really need.</p>
<p>ANN algorithms typically come in three flavors: graph based approaches, space partitioning based approaches, and hash based approaches. If your dataset is large enough, you might add compression (e.g. product quantization) on top of it.</p>
<h3 id="intuition">Intuition</h3>
<p><a href="https://arxiv.org/pdf/1603.09320">HNSW</a> is a bit of a Six Degrees of Kevin Bacon approach. I&rsquo;ll give what I think this is a fairly good intuition for how it works.</p>
<p>Suppose you have 1,000 friends and you&rsquo;d like to know which five of them live closest to a landmark like the Golden Gate Bridge. You don‚Äôt have everyone&rsquo;s exact address, but each friend keeps a list of their close friends&rsquo; addresses. You might start with a friend who lives in the US. From there, you check their friends to find who lives closest to the target, and then search among their friends for someone even closer. By using a hierarchical approach, you don&rsquo;t need to search through everyone, just those likely to be closest.</p>
<p>In the analogy, the Golden Gate Bridge is the &ldquo;needle&rdquo; and the friends are the &ldquo;haystack&rdquo;. This is a pretty efficient method, but of course it is possible to miss a node if their friend wasn&rsquo;t linked to an earlier friend.</p>
<figure><img src="/img/hnsw.png"
    alt="HNSW uses a multi-layered graph for efficient nearest neighbor search."><figcaption>
      <p>HNSW uses a multi-layered graph for efficient nearest neighbor search.</p>
    </figcaption>
</figure>

<h3 id="searching">Searching</h3>
<p>Searching is fairly straightforward. Start from an entry point (maybe a random point) in the top layer. For each of the neighbors of the entry point, compute the distance between the query and that point. If any of the neighbors are closer to the query, hop to that neighbor and repeat the process. Select the closest point and go to the next layer down. We keep searching until we reach layer 0.</p>
<p>To compute the K nearest neighbors during search (with K &gt; 1) we simply maintain a min heap, and add all nodes considered during the search. The number of nodes returned in this search (K) is also known as <code>ef_search</code> in the HNSW paper.</p>
<h3 id="addition">Addition</h3>
<p>To add a vector X, we start by computing the highest layer <code>L</code> the node will appear in. <code>L</code> is randomly selected using an exponentially decaying probability. (All nodes appear in level 0, but can have skip links to higher levels.) Next, starting from the top layer, we traverse the graph structure as if we were searching for X.</p>
<p>Once we reach layer <code>L</code> from earlier, we begin to make connections. Connections can be made with previously found nearest neighbors or nearest neighbors on the current layer. The number of nearest neighbors to consider connections with is controlled by the <code>ef_construction</code> parameter. The number of links to actually create is controlled by the parameter <code>M</code>.</p>
<h4 id="performance-considerations">Performance considerations</h4>
<ul>
<li>HNSW can require a large amount of memory. This can be reduced with product quantization, a lossy vector compression method.</li>
<li>Adding new elements is slow. There is a tradeoff between index quality and time required to build the index controlled by <code>ef_construction</code>.</li>
</ul>
<p>More information about HNSW parameters can be found <a href="https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md">here</a>.</p>
]]></content>
        </item>
        
        <item>
            <title>Learning the Haystack</title>
            <link>https://lukesalamone.github.io/posts/learning-the-haystack/</link>
            <pubDate>Wed, 27 Mar 2024 18:19:54 -0700</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/learning-the-haystack/</guid>
            <description>Embeddings, or vector representations of a document (which could be a piece of text, image, sound, etc.), can be extremely useful for making sense of large datasets. They transform information into a vector space such that their distance corresponds to their similarity.
Enterprising readers might be asking themselves how to get these vectors (also known as embeddings) in the first place. One way is to simply pay for them. This isn&amp;rsquo;t ideal for a couple of reasons:</description>
            <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      },
      extensions: ["AMSmath.js", "AMSsymbols.js"]
    }
  }
});
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<p>Embeddings, or vector representations of a document (which could be a piece of text, image, sound, etc.), can be extremely useful for making sense of large datasets. They transform information into a vector space such that their distance corresponds to their similarity.</p>
<p>Enterprising readers might be asking themselves how to get these vectors (also known as embeddings) in the first place. One way is to simply pay for them. This isn&rsquo;t ideal for a couple of reasons:</p>
<ol>
<li>It can be expensive. You&rsquo;ll need to pay once to embed each document, and separately for each query. If you have a large corpus, it can be cost-prohibitive. At the time of writing, OpenAI charges about $130 per million tokens (around 1000 paragraphs) for their largest model.</li>
<li>&ldquo;Similarity&rdquo; may mean different things depending on your use case. For example, suppose we are retrieving documents for customer support. For the embedding model to learn that two documents share user behavior characteristics (i.e. two documents were opened by support agents in the same session), that information needs to be available in the training process.</li>
</ol>
<p>Below is an overview of three of the main training regimes I have used for creating embeddings. For more information and in-depth examples, I highly recommend the loss overview page of the <a href="https://sbert.net/docs/sentence_transformer/loss_overview.html">Sentence Transformers</a> library. Generally speaking, there are three categories of methods: unsupervised methods, contrastive learning methods (positive/negative labels), and regression methods (floating-point labels).</p>
<h2 id="autoencoders">Autoencoders</h2>
<figure><img src="/img/mona_lisa_autoencoder.png"
    alt="An autoencoder uses a bottleneck to reduce the dimensionality of the input."><figcaption>
      <p>An autoencoder uses a bottleneck to reduce the dimensionality of the input.</p>
    </figcaption>
</figure>

<p>One approach for vectorizing a document, image, or other blob of information is to simply <a href="/posts/build-an-autoencoder/">use an autoencoder</a>. An autoencoder is a function which learns a lossy compression function. It can be considered an unsupervised method since each item is its own label.</p>
<p>However, although they are conceptually pretty simple, autoencoders aren&rsquo;t always best. They may learn an efficient representation of your document, but it may be wasting a lot of space on things you don&rsquo;t care about.</p>
<p>For example, if it was reproducing a picture which contained a TV showing static, you may not care exactly what the static <em>looks like</em>, just that it <em>has</em> static. So it&rsquo;s probably a waste of space to try to reproduce every pixel of static. Likewise, if it were encoding people, it might be really important to you that the people have 5 fingers. Unfortunately, the autoencoder wasted too much space encoding the TV and not on boring details like numbers of fingers.</p>
<p>See also: <a href="https://www.youtube.com/watch?v=UABxQ19Sqt0">the &ldquo;noisy TV problem&rdquo;</a></p>
<h2 id="fine-tuning-with-sentence-transformers">Fine-tuning with Sentence Transformers</h2>
<p>The applicability of the next few methods will depend on the format of your data and labels.</p>
<p>Here is a simple example for creating text embeddings with <code>sentence-transformers</code> using ContrastiveLoss:</p>
<pre><code class="language-python">from sentence_transformers import (
  SentenceTransformer, 
  SentenceTransformerTrainer
)
from sentence_transformers.losses import ContrastiveLoss
from datasets import Dataset

model = SentenceTransformer('all-MiniLM-L6-v2')
train_dataset = Dataset.from_dict({
    &quot;sentence1&quot;: [
      &quot;I feel the need...&quot;, 
      &quot;Life is like a box of chocolates.&quot;
    ],
    &quot;sentence2&quot;: [
      &quot;...the need for speed!&quot;, # similar
      &quot;Here's Johnny!&quot; # dissimilar
    ],
    &quot;label&quot;: [1, 0],
})

trainer = SentenceTransformerTrainer(
    model=model,
    train_dataset=train_dataset,
    loss=ContrastiveLoss(model),
)
trainer.train()
</code></pre>
<p>And to generate the embedding:</p>
<pre><code class="language-python">embedding = model.encode([&quot;There's no crying in baseball!&quot;])
print(embedding.shape) # (1, 384)
</code></pre>
<h2 id="contrastive-learning-triplet-loss">Contrastive Learning: Triplet loss</h2>
<p><a href="https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginWithDistanceLoss.html">Triplet loss</a> explicitly learns embeddings to be used in cosine similarity or euclidean distance (L2 norm) comparison. It uses triplets of the form (A, P, N), where A is an anchor, P is a positive example which is similar to A, and N is a negative example which is dissimilar to A. For example <a href="https://repositori.upf.edu/bitstream/handle/10230/54158/Alonso_ismir_musi.pdf">if we wanted to learn embeddings for songs</a>, we might say a positive example is a spectrogram clip from the same artist as the anchor, and a negative sample is from a different artist.</p>
<p style="text-align: center; font-size: 1.5em">
$$
\text{L} = \sum_{i=1}^N \text{max} \left(0, cos( f(a_i) - f(p_i) ) - cos( f(a_i) - f( n_i )) + m \right)
$$
</p>
<p>for cosine similarity or for euclidean distiance,</p>
<p style="text-align: center; font-size: 1.5em">
$$
\text{L} = \sum_{i=1}^N \text{max} \left(0, \lVert f(a_i) - f(p_i) \rVert_2 - \lVert f(a_i) - f( n_i ) \rVert_2 + m \right)
$$
</p>
<p>In other words, we enforce that the distance between the anchor and the negative sample is greater than the distance between the anchor and the positive sample <em>plus</em> some margin. For cosine similarity, what this would look like is that your vectors appear as points on the perimeter of a high-dimensional clock face (i.e. the surface of an N-dimensional hypersphere). Positive samples grow closer to the anchor while the negative samples are pushed away. The higher the margin, the closer positive samples will be pushed together.</p>
<figure><img src="/img/contrastive_learning.png"
    alt="Learning to differentiate Bangarang and Clair de lune. Triplet loss pushes positive examples closer to the anchor and negatives farther away. Some say Claude DeBussy was the Skrillex of 1890."><figcaption>
      <p>Learning to differentiate Bangarang and Clair de lune. Triplet loss pushes positive examples closer to the anchor and negatives farther away. Some say Claude DeBussy was the Skrillex of 1890.</p>
    </figcaption>
</figure>

<h2 id="contrastive-learning-contrastive-loss">Contrastive learning: Contrastive loss</h2>
<p>If you have binary labels, you can use <a href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf">contrastive loss</a>. It works with positive pairs <code>(anchor, positive)</code> and negative pairs <code>(anchor, negative)</code>, denoted with a binary label. In the original paper, $ y \in [1,0] $ is the label indicating the desired distance between the points (<strong>not</strong> whether they are similar).</p>
<p>Like triplet loss, contrastive loss also includes a margin parameter. For dissimilar pairs, a loss is incurred only if their predicted distance falls within the margin&rsquo;s radius.</p>
<p style="text-align: center; font-size: 1.5em">
$$
\text{loss} = \frac{(1-y_{i})(D_{W_i})^2 + y_i \left( \text{max}(0, m - D_{W_i}) \right) ^ 2 }{2}
$$
</p>
<p>Where $ D_{W_i} $ is euclidean distance between the two embeddings of sentences $ a_i $ and $ b_i $:</p>
<p style="text-align: center; font-size: 1.5em">
$$
D_{W_i} = \lVert f(a_i) - f(b_i) \rVert _{2}
$$
</p>
<p>Note that triplet loss is often confused with contrastive loss, but they are not the same.</p>
<h2 id="regression-cosine-similarity-loss">Regression: Cosine similarity loss</h2>
<p>If you already have some numerical labels for the similarity of two pieces of text, you can compute mean squared error between their predicted cosine similarity and the actual similarity.</p>
<p>For pairs of sentences a<sub>i</sub> and b<sub>i</sub> and label y<sub>i</sub>,</p>
<p style="text-align: center; font-size: 1.5em">
$$
\text{loss} = \frac{1}{N} \sum_{i=1}^{N} \left[ cos\left(f(a_{i}), f(b_{i})\right) - y_{i} \right] ^ 2
$$
</p>
<h2 id="regression-cosent-loss">Regression: CoSENT loss</h2>
<p>CoSENT loss is an improved version of cosine similarity learning which makes better use of in-batch values. For a batch with size <code>N</code> it makes up to <code>Nx(N-1)/2</code> predictions. (I.e. for a batch size of 10 we make up to 45 predictions rather than the 10 in Cosine Similarity Loss.) It is essentially a ranking loss, taking advantage of the knowledge of the relative similarities within the batch.</p>
<p style="text-align: center; font-size: 1.5em">
$$
\text{loss} = \log \left( 1 + \sum_{\text{sim}(i,j) > \text{sim}(k,l)} e^{\lambda (\cos(k, l) - \cos(i, j))} \right)
$$
</p>
<p>Experiments have shown <a href="https://github.com/UKPLab/sentence-transformers/pull/2454#issuecomment-1914664814">improved performance</a> over Cosine Similarity loss.</p>
]]></content>
        </item>
        
        <item>
            <title>A 3D Game of Life</title>
            <link>https://lukesalamone.github.io/posts/game-of-life-3d/</link>
            <pubDate>Wed, 23 Aug 2023 23:34:38 -0700</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/game-of-life-3d/</guid>
            <description>Conway&amp;rsquo;s Game of Life is a simulation developed in 1970 describing a grid of binary cells and transition rules for each cell which depend on the state of the cell&amp;rsquo;s neighbors. It&amp;rsquo;s capable of creating some pretty cool patterns.
This variant of the Game of Life uses three overlapping channels, so instead of just one simulation, there are three simultaneous simulations. I visualize these in the three color channels, red, green and blue.</description>
            <content type="html"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Conway&rsquo;s Game of Life</a> is a simulation developed in 1970 describing a grid of binary cells and transition rules for each cell which depend on the state of the cell&rsquo;s neighbors. It&rsquo;s capable of creating some pretty cool patterns.</p>
<p>This variant of the Game of Life uses three overlapping channels, so instead of just one simulation, there are three simultaneous simulations. I visualize these in the three color channels, red, green and blue. Two or more channels active on the same cell are represented with <a href="https://en.wikipedia.org/wiki/Additive_color">additive color mixing</a>.</p>
<p>These additional channels enable additional interaction between channels, which is pretty neat. I&rsquo;ve added a rule which revives a dead cell if the sum of its active neighbors from other channels is 9. One nice result of this inter-channel interaction is that stagnant colors can be reactivated by other colors that pass by.</p>
<p>Finding parameters that don&rsquo;t cause the simulation to blow up or die out is a bit of a trial and error. I made an empirical choice which results in a lively &ldquo;pond&rdquo; after the chaotic initial state. I&rsquo;m sure there are other nice combinations as well.</p>
<p>Other variants I experimented with briefly were to include more than three channels and to consider more complex relationships between the channels. However these ran slower and/or were harder to balance between chaotic and desolate tendencies.</p>
<div id="game-of-life-controls">
  <button class="start">start game</button>
  <button class="pause" style="display:none">pause game</button>
</div>
<div id="game-of-life-container"></div>
<script src="/js/game_of_life.js"></script>
<link rel="stylesheet" href="/css/game-of-life.css" />
]]></content>
        </item>
        
        <item>
            <title>Can ChatGPT Recognize Handwritten Digits?</title>
            <link>https://lukesalamone.github.io/posts/chatgpt-mnist/</link>
            <pubDate>Sun, 30 Jul 2023 22:45:57 -0700</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/chatgpt-mnist/</guid>
            <description>TLDR: No. No it cannot.
This was admittedly a fairly stupid experiment on the face of it. ChatGPT is a decoder-only model. It shouldn&amp;rsquo;t be able to perform an image recognition task. But then again, a decoder-only model wouldn&amp;rsquo;t have been my first choice for translation or summarization either. In my experience, ChatGPT has created translations which are at least as coherent and idiomatic as Google Translate, if not more so.</description>
            <content type="html"><![CDATA[<p><strong>TLDR: No. No it cannot.</strong></p>
<p>This was admittedly a fairly stupid experiment on the face of it. ChatGPT is a decoder-only model. It shouldn&rsquo;t be able to perform an image recognition task. But then again, a decoder-only model wouldn&rsquo;t have been my first choice for translation or summarization either. In my experience, ChatGPT has created translations which are at least as coherent and idiomatic as Google Translate, if not more so.</p>
<p>So I thought, why not give it a shot?</p>
<h1 id="process">Process</h1>
<p><a href="https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST">MNIST</a> is a computer vision dataset containing thousands of handwritten digits along with their actual labels. Images are represented as 28x28 tensors where each element of the tensor represents a intensity pixel intensity between 0 and 1. Traditionally this is formulated into a classification problem, with the goal of choosing the correct class out of the 10.</p>
<p>To simplify the input, I flattened the tensor and translated the pixel intensity values to the range 0-10. This allowed me to use fewer tokens at the expense of some of the gradations between light and dark.</p>
<p>I gathered 10 random examples from each of the 10 classes (100 total images) and fed them into <code>gpt-3.5-turbo</code> with the following prompt:</p>
<blockquote>
<p>The following is a flattened representation of an image of a handwritten digit. The image was 28x28 but has been flattened to 1x784. Each number represents a pixel intensity from 0-10. Please tell me which digit the following pixel list represents:</p>
</blockquote>
<p>Followed by the list of 784 pixel intensities.</p>
<h1 id="results">Results</h1>
<p>ChatGPT scored 11/100. Barely better than guessing. A summary of the sessions can be found at <a href="https://gist.github.com/lukesalamone/744272ef00e56afb6cab56cdc70a593a">this gist</a>. A visual summary is below, with the blue signifying correct guesses.</p>
<p><em>Note: although the image shows the digits sorted, they were prompted in random order.</em></p>
<p><img alt="chatgpt performance on 100 handwritten digits" src="/img/chatgpt_mnist.png"></p>
<p>It is interesting that only the 0s and the 7s were correctly identified. Let&rsquo;s take a look at the distribution of ChatGPT&rsquo;s guesses.</p>
<table>
<thead>
<tr>
<th>Guess</th>
<th>Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>85</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>3</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>5</td>
<td>4</td>
</tr>
<tr>
<td>6</td>
<td>1</td>
</tr>
<tr>
<td>7</td>
<td>5</td>
</tr>
<tr>
<td>8</td>
<td>0</td>
</tr>
<tr>
<td>9</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Oh. It got 8/10 zeros correct because it guessed zero 85% of the time.</p>
]]></content>
        </item>
        
        <item>
            <title>Paper Summary: Antenna Design with Evolutionary Algorithms</title>
            <link>https://lukesalamone.github.io/posts/evolutionary-antenna-design/</link>
            <pubDate>Mon, 17 Apr 2023 19:46:25 -0700</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/evolutionary-antenna-design/</guid>
            <description>This is a summary of Automated Antenna Design with Evolutionary Algorithms, a 2006 paper by Hornby et al. As large language models become more and more synonymous with &amp;ldquo;AI&amp;rdquo;, it is interesting to see how researchers solved problems in the past.
Typically, antennas are designed and built by hand by domain experts. This is a very time-consuming process, however, so researchers have been investigating evolutionary algorithms since the 1990s. Inspired by natural evolution, an evolutionary algorithm is based on small, random changes and an evaluation metric.</description>
            <content type="html"><![CDATA[<p><strong>This is a summary of <a href="https://www.researchgate.net/profile/Al-Globus/publication/228909002_Automated_Antenna_Design_with_Evolutionary_Algorithms/links/547375300cf216f8cfaff65a/Automated-Antenna-Design-with-Evolutionary-Algorithms.pdf">Automated Antenna Design with Evolutionary Algorithms</a>, a 2006 paper by Hornby et al. As large language models become more and more synonymous with &ldquo;AI&rdquo;, it is interesting to see how researchers solved problems in the past.</strong></p>
<p>Typically, antennas are designed and built by hand by domain experts. This is a very time-consuming process, however, so researchers have been investigating evolutionary algorithms since the 1990s. Inspired by natural evolution, an evolutionary algorithm is based on small, random changes and an evaluation metric. In this paper, the authors describe the use of an evolutionary algorithm to design an antenna for a small satellite weighing only 25 kilograms called ST5.</p>
<p>The researchers then describe the technical specifications which the antenna would need to satisfy. The antenna needed to weigh under 165 grams and have a height and diameter of around 15 centimeters.</p>
<p>In representing the antenna, the researchers used a tree structure, with each node capable of representing one of several operations: forward, rotate-x, rotate-y, and rotate-z. The ‚Äúforward‚Äù operation adds a length of wire with a given radius. The ‚Äúrotate-x‚Äù operation rotates the current state about the x-axis. The ‚Äúrotate-y‚Äù and ‚Äúrotate-z‚Äù operations do the same for the y and z-axes.</p>
<p>The researchers used a fitness function which was the product of VSWR, a gain_error term and a  gain_smoothness term. The gain_error term is similar to the least squares error function. The gain_smoothness term describes how uniform the gain pattern was, since the satellite would be spinning. All three of these factors were multiplied together to calculate an overall fitness score against which all candidate antennas were measured.</p>
<p>Due to a change in the orbit of the satellite, the specifications for the antenna were updated and the researchers needed to modify their evolutionary algorithm. This was completed within one month. The evolved antenna consumed less power, took less time to build, was less complex, and performed better than traditionally designed antennas. Since the satellite had 2 antennas, the researchers measured the combined performance. The evolved antennas were 93% efficient while the designed antennas were only 38% efficient. Additionally, the evolved antennas were much faster to design and fabricate.</p>
<p>The researchers then designed another antenna for NASA‚Äôs TDRS-C satellite. This design used an evolutionary algorithm in combination with a stochastic hill-climbing algorithm. They used a loss function which attended to the standing wave ratio and gain at several frequencies relevant to the satellite. In performing the evolutionary algorithm, 150 algorithm processes were run for 50,000 iterations after randomizing many parameters. After this first stage, the best antenna from the previous 150 was optimized using stochastic hill climbing with random mutations. From this second stage, the 23 best antennas were selected and run through another stochastic hill climbing step for 100,000 iterations. Of the 23 finalists, one of the evolved antennas exceeded the project specifications and was optimized further using more accurate software.</p>
]]></content>
        </item>
        
        <item>
            <title>Paper Summary: Dual-Encoders in Ranking</title>
            <link>https://lukesalamone.github.io/posts/dual-encoders-ranking/</link>
            <pubDate>Sat, 17 Dec 2022 16:53:47 -0800</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/dual-encoders-ranking/</guid>
            <description>In Defense of Dual-Encoders for Neural Ranking by Menon et. al. discusses the question of why dual-encoder (DE) models, also called Bi-Encoders elsewhere, don&amp;rsquo;t match the performance of cross-attention (CA) models. The authors investigate what is actually going on, and demonstrate some improved performance over baseline DE models with a new model distillation method.
Background Search requires an automatic way to find the most relevant documents to a query. There are bag-of-word approaches to this task (for example BM25) and neural approaches.</description>
            <content type="html"><![CDATA[<p><a href="https://proceedings.mlr.press/v162/menon22a/menon22a.pdf">In Defense of Dual-Encoders for Neural Ranking by Menon et. al.</a> discusses the question of why dual-encoder (DE) models, also called Bi-Encoders elsewhere, don&rsquo;t match the performance of cross-attention (CA) models. The authors investigate what is actually going on, and demonstrate some improved performance over baseline DE models with a new model distillation method.</p>
<h2 id="background">Background</h2>
<p>Search requires an automatic way to find the most relevant documents to a query. There are bag-of-word approaches to this task (for example BM25) and neural approaches. An example of a bag-of-words approach might simply be to count the number of similar words between the query and each document, and return the document with the highest number of similar words. There are word-stuffing issues with this idea, but the larger issue is that a bag-of-words strategy can&rsquo;t account for synonyms. If I search for <em>bad guy</em> I will never find <em>villain</em> without some additional logic to account for this. A neural network implicitly understands the relationship between words, and avoids the fragile logic of simple word counts.</p>
<p>The idea of a neural encoding approach is pretty simple. For each document in your corpus, pass the query a document into a function which will return a similarity score between 0 and 1. Then, just sort the documents by that score. There are two main architectures for doing this: dual-encoders and cross-attention models.</p>
<figure><img src="/img/dual_vs_cross_encoder.png"
    alt="Dual encoder architectures can precompute document embeddings but tend to be less accurate than cross attention models."><figcaption>
      <p>Dual encoder architectures can precompute document embeddings but tend to be less accurate than cross attention models.</p>
    </figcaption>
</figure>

<p>The great thing about DE models is that document embeddings can be computed ahead of time. When users enter a query, only that query embedding needs to be calculated, and then compared with the embeddings already calculated for each of the documents. It&rsquo;s a lot faster. So much faster, in fact, that CA is generally not used for initial retrieval, only for reranking afterwards.</p>
<p>However, DE models tend to be less accurate than CA models. It would be great if it was possible to transfer some of the benefits of CA models to DE models.</p>
<h2 id="the-problem">The Problem</h2>
<p>It&rsquo;s unclear whether the shortcomings of DEs are due to the DE model&rsquo;s capacity or because of its training procedure. DEs may be overfitting.</p>
<figure><img src="/img/dual_encoder_overfitting.png"
    alt="DE models can match CA model training performance but are often lower during evaluation."><figcaption>
      <p>DE models can match CA model training performance but are often lower during evaluation.</p>
    </figcaption>
</figure>

<p>Dual encoders can also be improved by distillation, of which there are two kinds:</p>
<ol>
<li>Logit matching. Try to match embeddings between teacher and student.</li>
<li>Probability matching. Try to match the softmax probabilities between teacher and student.</li>
</ol>
<figure><img src="/img/de_ca_margins.png"
    alt="CA models have better separation between positive and negative examples, strongly predicting negative examples. DE models have more overlap between positive and negative predictions. Normalizing the margins between positive and negative predictions (higher is better), CA models clearly have better performance. The distilled model is slightly better than the DE model."><figcaption>
      <p>CA models have better separation between positive and negative examples, strongly predicting negative examples. DE models have more overlap between positive and negative predictions. Normalizing the margins between positive and negative predictions (higher is better), CA models clearly have better performance. The distilled model is slightly better than the DE model.</p>
    </figcaption>
</figure>

<p>Part of the cause of this discrepancy may be the fact that DE models have noisier updates. DE models may have difficulty modeling negative scores since updating their weights on positive (q, d+) pairs can inadvertently increase scores for negative (q, d-) pairs. Dropout also doesn&rsquo;t seem to mitigate overfitting.</p>
<figure><img src="/img/de_ca_score_evolution.png"
    alt="The evolution of scores for five positive and five negative documents for a fixed query. Scores from the CA model separate much more smoothly than in the DE model."><figcaption>
      <p>The evolution of scores for five positive and five negative documents for a fixed query. Scores from the CA model separate much more smoothly than in the DE model.</p>
    </figcaption>
</figure>

<h2 id="solutions">Solutions</h2>
<p>Previous work has tried to improve training procedures in several ways:</p>
<ol>
<li>Adjusting the scoring layer. Usually embeddings are scored with a simple dot product, but a more sophisticated scoring function may be able to capture more information at the cost of inference speed.</li>
<li>Distilling predictions from CA models. Model distillation uses a teacher-student framework where the smaller &ldquo;student&rdquo; model attempts to mirror the &ldquo;teacher&rdquo;. This paper explores a new approach to distillation.</li>
</ol>
<p>The authors introduce <em>multi-margin MSE loss</em> (M3SE):</p>
<figure><img src="/img/m3se.png">
</figure>

<p>M3SE loss attempts to match the margins of score differences between teacher and student. For performance reasons, however, rather than matching each margin, it only encourages the student to be less than or equal to the teacher&rsquo;s highest negative score.</p>
<p>M3SE can be seen as an extension of Margin MSE loss where instead of matching logits it matches raw scores. It can also be seen as a smooth approximation of softmax cross-entropy loss. The authors also highlight parallels between M3SE and RankDistil.</p>
<h2 id="results">Results</h2>
<figure><img src="/img/m3se_results.png"
    alt="Apart from TREC, M3SE distillation appears to nearly close the gap with cross-attention models. Distilled models are 6-layer BERT models with embedding size 768."><figcaption>
      <p>Apart from TREC, M3SE distillation appears to nearly close the gap with cross-attention models. Distilled models are 6-layer BERT models with embedding size 768.</p>
    </figcaption>
</figure>

]]></content>
        </item>
        
        <item>
            <title>My Favorite Antimaia Games</title>
            <link>https://lukesalamone.github.io/posts/best-antimaia-games/</link>
            <pubDate>Sat, 26 Nov 2022 20:25:13 -0800</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/best-antimaia-games/</guid>
            <description>This is a follow up to When Suboptimal Minimax is Better. After running 400 simulations, I can conclusively say that opponent modeling is pretty cool.
The TLDR on opponent modeling is that if we have a pretty good idea of what the opponent might do, we can beat them faster by playing moves which aren&amp;rsquo;t objectively &amp;ldquo;optimal&amp;rdquo; as far as minimax is concerned. Here, Maia 1900 is a model of a relatively high-level chess player.</description>
            <content type="html"><![CDATA[<p>This is a follow up to <a href="/posts/suboptimal-minimax/">When Suboptimal Minimax is Better</a>. After running 400 simulations, I can conclusively say that opponent modeling is pretty cool.</p>
<p>The TLDR on opponent modeling is that if we have a pretty good idea of what the opponent might do, we can beat them faster by playing moves which aren&rsquo;t objectively &ldquo;optimal&rdquo; as far as minimax is concerned. Here, Maia 1900 is a model of a relatively high-level chess player. Antimaia 1900 is specifically designed to counter Maia 1900.</p>
<table>
<thead>
<tr>
<th>White</th>
<th>Black</th>
<th>Games</th>
<th>White win %</th>
<th>Average half moves per game</th>
</tr>
</thead>
<tbody>
<tr>
<td>Antimaia 1900</td>
<td>Maia 1900</td>
<td>100</td>
<td>100%</td>
<td>37</td>
</tr>
<tr>
<td>Maia 1900</td>
<td>Antimaia 1900</td>
<td>100</td>
<td>0%</td>
<td>43</td>
</tr>
<tr>
<td>Maia 1900</td>
<td>Stockfish @10</td>
<td>100</td>
<td>0%</td>
<td>67</td>
</tr>
<tr>
<td>Stockfish @10</td>
<td>Maia 1900</td>
<td>100</td>
<td>100%</td>
<td>70</td>
</tr>
</tbody>
</table>
<ul>
<li>Maia 1900 uses Maia Chess weights trained on games from players rated near 1900.</li>
<li>Antimaia 1900 uses Maia 1900 weights and Stockfish @10.</li>
<li>Stockfish @10 is Stockfish 14.1 evaluated at depth 10.</li>
</ul>
<p>As you can see, Antimaia finishes its games much more quickly (12-16 moves faster!). This demonstrates that Antimaia is exploiting weaknesses in Maia&rsquo;s policy as measured by Stockfish, and playing high risk/high reward moves which usually pay off.</p>
<h2 id="selected-games">Selected Games</h2>
<p>I had to keep reminding myself that even though these games look ridiculous, Maia plays poorly because Antimaia is finding moves where Maia is most likely to mess up.</p>
<h3 id="antimaia-vs-maia">Antimaia vs Maia</h3>
<ul>
<li>
<p><a href="https://lichess.org/cRCrl2yO">Game 15</a>: Smothered mate comes out of nowhere.
<img alt="gif" src="https://lichess1.org/game/export/gif/white/cRCrl2yO.gif"></p>
</li>
<li>
<p><a href="https://lichess.org/CIJn9rD0">Game 24</a>: Antimaia lays a poison pawn trap then waits patiently for 2 moves before Maia falls for it, blundering a queen.
<img alt="gif" src="https://lichess1.org/game/export/gif/white/CIJn9rD0.gif"></p>
</li>
<li>
<p><a href="https://lichess.org/EySDK7IP">Game 32</a>: The shortest game. Another nice checkmate delivered by the knight.
<img alt="gif" src="https://lichess1.org/game/export/gif/white/EySDK7IP.gif"></p>
</li>
<li>
<p><a href="https://lichess.org/kkRE06DB">Game 59</a>: Maia blunders a piece and then falls for Antimaia&rsquo;s poison rook to get checkmated.
<img alt="gif" src="https://lichess1.org/game/export/gif/white/kkRE06DB.gif"></p>
</li>
<li>
<p><a href="https://lichess.org/qTJg2D26">Game 75</a>: Promoting to a knight not because it&rsquo;s the best move, but because it doesn&rsquo;t matter.
<img alt="gif" src="https://lichess1.org/game/export/gif/white/qTJg2D26.gif"></p>
</li>
<li>
<p><a href="https://lichess.org/ERFLY3Wa">Game 88</a>: Antimaia makes the crazy move Bh7 but Maia blunders into checkmate by taking with the knight.
<img alt="gif" src="https://lichess1.org/game/export/gif/white/ERFLY3Wa.gif"></p>
</li>
</ul>
<h3 id="maia-vs-antimaia">Maia vs. Antimaia</h3>
<ul>
<li>
<p><a href="https://lichess.org/aCUt0XbV">Game 2</a>: Queen takes e3 is objectively a horrible move but Antimaia gambles correctly that Maia won&rsquo;t know how to respond.
<img alt="gif" src="https://lichess1.org/game/export/gif/black/aCUt0XbV.gif"></p>
</li>
<li>
<p><a href="https://lichess.org/pgPCKq4E">Game 7</a>: Antimaia baits Maia into blundering a queen and mate in 3.
<img alt="gif" src="https://lichess1.org/game/export/gif/black/pgPCKq4E.gif"></p>
</li>
<li>
<p><a href="https://lichess.org/DyBQrLwo">Game 51</a>: Antimaia tricks Maia with a poison pawn.
<img alt="gif" src="https://lichess1.org/game/export/gif/black/DyBQrLwo.gif"></p>
</li>
<li>
<p><a href="https://lichess.org/25aF17bv">Game 59</a>: Maia manages to lose in 5 moves.
<img alt="gif" src="https://lichess1.org/game/export/gif/black/25aF17bv.gif"></p>
</li>
<li>
<p><a href="https://lichess.org/my88HT0H">Game 69</a>: Antimaia swindles a queen.
<img alt="gif" src="https://lichess1.org/game/export/gif/black/my88HT0H.gif"></p>
</li>
</ul>
<h2 id="the-rest-of-the-games">The rest of the games</h2>
<p>I added the rest of the games to github: <a href="https://github.com/lukesalamone/antimaia_games">https://github.com/lukesalamone/antimaia_games</a></p>
]]></content>
        </item>
        
        <item>
            <title>The Other End of the Earth</title>
            <link>https://lukesalamone.github.io/posts/earth-antipodes/</link>
            <pubDate>Wed, 23 Nov 2022 10:07:36 -0800</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/earth-antipodes/</guid>
            <description>White areas show points of earth on land whose antipode is also on land. This is only about 8.6% of all of earth&amp;rsquo;s surface.
If you want to fly across the Pacific Ocean, you&amp;rsquo;ll have to board an airplane and fly around 12 hours. It&amp;rsquo;s pretty slow. A much faster route would be to go directly through the center of the earth. &amp;ldquo;Digging to China&amp;rdquo; was a common expression I heard growing up, with the implication that the opposite side of the globe is somewhere in Asia.</description>
            <content type="html"><![CDATA[<figure><img src="/img/antipode_land.png"
    alt="White areas show points of earth on land whose antipode is also on land. This is only about 8.6% of all of earth&amp;rsquo;s surface."><figcaption>
      <p>White areas show points of earth on land whose antipode is also on land. This is only about 8.6% of all of earth&rsquo;s surface.</p>
    </figcaption>
</figure>

<p>If you want to fly across the Pacific Ocean, you&rsquo;ll have to board an airplane and fly around 12 hours. It&rsquo;s pretty slow. A much faster route would be to go directly through the center of the earth. &ldquo;Digging to China&rdquo; was a common expression I heard growing up, with the implication that the opposite side of the globe is somewhere in Asia.</p>
<p>For the US, that&rsquo;s actually not true. The opposite end of the earth for most of the US is actually somewhere west of Australia. For North America, there are actually very few places whose antipode (the point on the exact opposite side of the globe) is on dry land. If you could dig a hole through the center of the earth from the US, you&rsquo;d pop out in the Pacific Ocean.</p>
<p>It is actually possible to &ldquo;dig to China&rdquo; though. Most of South America (excluding most of Brazil) has an antipode in east Asia. In fact, several of the world&rsquo;s biggest cities have antipodes on dry land. I&rsquo;ll get to this later.</p>
<h2 id="calculating-antipodes">Calculating Antipodes</h2>
<p>Given the latitude and longitude of a point on earth, it&rsquo;s pretty easy to calculate its antipode:</p>
<pre><code class="language-python">def antipode(lat, lon):
  lat = 0 - lat
  lon = 180 + lon if lon &lt; 0 else lon - 180
  return lat, lon
</code></pre>
<h2 id="dry-land">Dry land?</h2>
<p>Once I have the antipode of a particular latitude and longitude, I want to know if it&rsquo;s on dry land or not. The process of figuring out what&rsquo;s at a particular latitude/longitude point is called reverse geocoding, and there are a few nice tools for doing this for dry land. These tools will give fairly good information about the country a point is in.</p>
<p>But for now all I care about is whether a particular point is on land or water, not what country it&rsquo;s in. I wasn&rsquo;t able to find a great tool online that does this, so I opted for a more brute force method.</p>
<p>I used <a href="https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Blue_Marble_2002.png/2560px-Blue_Marble_2002.png">this picture from Wikipedia</a> which represents almost all water as the same color of blue. Based on that, I can create a boolean mask to indicate oceans and lakes, and reverse project pixels from that image to geographic coordinates. Simple enough.</p>
<h2 id="pixels-to-geocoordinates">Pixels to Geocoordinates</h2>
<p>Once you have the (x,y) coordinates they need to be converted to longitude and latitude. Latitude is how far north (or south) you are on the globe and ranges from -90 degrees at the North Pole to 90 degrees at the South Pole. On a flat map this is represented by a y coordinate.</p>
<p>Longitude represents how far east or west you are from the Prime Meridian (which runs through Greenwich, UK) and ranges from -180 to 180, both of which represent the same meridian.</p>
<p>How a latitude/longitude point is projected onto the globe depends on the map projection. In this case we&rsquo;re using <a href="https://en.wikipedia.org/wiki/Equirectangular_projection">Equirectangular projection</a>, so we just need to reverse that operation to get the x/y coordinates. The formula for doing so is:</p>
<pre><code class="language-python">#height is the height of the image
def lat2y(lat):
    return int((90 - lat) * (height / 180))

# width is the width of the image
def lon2x(lon):
    return int((lon + 180) * (width / 360))

# the reverse operations of the above
def y2lat(y):
    return 90 - 180 * y / height

def x2lon(x):
    return 360 * x / width - 180
</code></pre>
<h2 id="pixel-antipodes">Pixel Antipodes</h2>
<p>Now we can calculate the antipodes of a pixel.</p>
<pre><code class="language-python">def pixel_antipode(y, x):
    lat, lon = antipode(y2lat(y), x2lon(x))
    return lat2y(lat), lon2x(lon)
</code></pre>
<h2 id="antipodal-cities">Antipodal Cities</h2>
<p>If we look at <a href="https://en.wikipedia.org/wiki/List_of_largest_cities">a list of the world&rsquo;s largest cities</a>, which of them have antipodes on land? In the top 20, only four cities do.</p>
<table>
<thead>
<tr>
<th>city</th>
<th>population (millions)</th>
<th>lat, lon</th>
<th>antipode country</th>
</tr>
</thead>
<tbody>
<tr>
<td>Shanghaiüá®üá≥</td>
<td>25.5</td>
<td>31.2, 121.5</td>
<td>Argentinaüá¶üá∑</td>
</tr>
<tr>
<td>Beijingüá®üá≥</td>
<td>19.6</td>
<td>39.9, 116.4</td>
<td>Argentinaüá¶üá∑</td>
</tr>
<tr>
<td>Manilaüáµüá≠</td>
<td>13.5</td>
<td>14.6, 121.0</td>
<td>Brazilüáßüá∑</td>
</tr>
<tr>
<td>Tianjinüá®üá≥</td>
<td>13.2</td>
<td>39.1, 117.2</td>
<td>Argentinaüá¶üá∑</td>
</tr>
</tbody>
</table>
]]></content>
        </item>
        
        <item>
            <title>A Few Notes on the Transformer</title>
            <link>https://lukesalamone.github.io/posts/self-attention/</link>
            <pubDate>Wed, 16 Nov 2022 15:24:15 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/self-attention/</guid>
            <description>A self-attention block depicted as a neural network.
In this post I will describe the attention mechanism, commonly used in transformers, a popular neural language architecture. Most of the most well-known large language models of late are based on the transformer architecture. Attention was first described in Attention is All You Need by Vaswani et al.
What is attention? At a high level, attention is a mechanism for neural networks to boost portions of an input which are relevant and ignore those which aren&amp;rsquo;t.</description>
            <content type="html"><![CDATA[<figure><img src="/img/self-attention.png"
    alt="A self-attention block depicted as a neural network."><figcaption>
      <p>A self-attention block depicted as a neural network.</p>
    </figcaption>
</figure>

<p>In this post I will describe the attention mechanism, commonly used in transformers, a popular neural language architecture. Most of the most well-known large language models of late are based on the transformer architecture. Attention was first described in <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention is All You Need</a> by Vaswani et al.</p>
<h2 id="what-is-attention">What is attention?</h2>
<p>At a high level, attention is a mechanism for neural networks to boost portions of an input which are relevant and ignore those which aren&rsquo;t. In language models, attention is used as a way for the model to learn which portions of a sentence are relevant to each word.</p>
<h2 id="what-is-attention-for">What is attention for?</h2>
<p>Let&rsquo;s use an example:</p>
<blockquote>
<p>I am sitting at the library with my friend.</p>
</blockquote>
<p>It should be pretty clear that not all words in this sentence are equally important. What words are relevant to &ldquo;I&rdquo;? Probably &ldquo;sitting&rdquo;, &ldquo;library&rdquo;, and &ldquo;friend&rdquo;. Likewise, &ldquo;the&rdquo; might only be relevant to &ldquo;library&rdquo;. Attention provides a way for a model to increase and decrease the importance of each word.</p>
<p>Since the value of each token in the sequence is dependent on other tokens, this method of generating word embeddings is very different from more classical methods like <a href="https://arxiv.org/pdf/1301.3781.pdf">Word2Vec</a> and <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>. There is no one fixed vector for a given word.</p>
<p>And this makes sense. Many words in English are homonyms, and have identical spellings for distinct meanings. For example, &ldquo;rock&rdquo; is a genre of music but also can mean a stone. <a href="https://www.npr.org/2011/05/30/136796448/has-run-run-amok-it-has-645-meanings-so-far">The word &ldquo;run&rdquo; has 645 meanings and has recently replaced &ldquo;set&rdquo; as the word with the most definitions.</a> It would not make sense for all of these homonyms to have the same vector.</p>
<h2 id="an-interactive-example">An interactive example</h2>
<p>You can hover over each word to see the relative importances of each word in the sentence to the hovered word.</p>
<div id="attention_demo"></div>
<link rel="stylesheet" href="/css/attention-demo.css" />
<script src="/js/attention_demo.js"></script>
<h2 id="how-does-self-attention-work">How does self-attention work?</h2>
<p><figure><img src="/img/scaled-dot-prod-attention.png"
    alt="The Vaswani paper describes scaled dot product attention, which involves normalizing by the square root of the input dimension."><figcaption>
      <p>The Vaswani paper describes scaled dot product attention, which involves normalizing by the square root of the input dimension.</p>
    </figcaption>
</figure>

This is the part where Vaswani delves into a database analogy with <strong>keys</strong>, <strong>queries</strong>, and <strong>values</strong>. Most online resources try to salvage this analogy. Personally, I always found this a bit confusing. What you need to know is that keys, values, and queries correspond to 3 matrices M<sub>k</sub>, M<sub>q</sub>, and M<sub>v</sub>, which are used in a dot product with the original input vectors.</p>
<p>In linear algebra terms this means multiplying the 1xd input vector by a matrix of size dxd. In neural network terms, this means passing the input vector through a full-connected layer. After M<sub>k</sub> and M<sub>q</sub> are multiplied, they are normalized by the square root of d<sub>k</sub>, a constant representing the dimension of the input vector.</p>
<h2 id="can-we-attend-to-multiple-parts-of-a-sentence">Can we attend to multiple parts of a sentence?</h2>
<figure><img src="/img/multi_head_attention.png"
    alt="Multi-headed attention means performing attention n times in parallel inside of an encoder block."><figcaption>
      <p>Multi-headed attention means performing attention n times in parallel inside of an encoder block.</p>
    </figcaption>
</figure>

<p>Yes, that is called multi-headed attention. Its architecture is very similar, using  additional M<sub>k</sub>, M<sub>q</sub>, and M<sub>v</sub> matrices for each additional &ldquo;attention head&rdquo;. In the Vaswani paper they used 8 heads.</p>
<h2 id="how-do-transformers-compare-with-other-architectures-eg-rnncnn">How do transformers compare with other architectures (e.g. RNN/CNN)?</h2>
<figure><img src="/img/attention_performance.png"
    alt="When the input sequence length n is lower than the input dimensionality d, self-attention is faster than recurrent neural networks. Self-attention is also easily parallelizable."><figcaption>
      <p>When the input sequence length n is lower than the input dimensionality d, self-attention is faster than recurrent neural networks. Self-attention is also easily parallelizable.</p>
    </figcaption>
</figure>

<p>Generally speaking, RNNs are able to memorize but not parallelize, and CNNs are able to parallelize but not memorize. Transformers are able to do both.</p>
<p>The Vaswani paper outlines three main benefits:</p>
<ol>
<li>
<p>Computational complexity per layer. Self-attention layers are faster than recurrent layers when the input sequence length is smaller than the input vector dimensionality.</p>
</li>
<li>
<p>The opportunity to parallelize calculations. Each head in multi-headed attention can be computed separately in an encoder layer.</p>
</li>
<li>
<p>Easier to learn long-range dependencies. For many English sentences, especially fairly complex ones found in more scientific writings, the full context of a word cannot be learned from its immediate neighbors. Sometimes it can&rsquo;t even be found in the same sentence. However, even though most language models prior to the transformer had theoretically infinite input sequence lengths, in practice it was quite difficult for them to learn long-range dependencies. Because a transformer sees its whole input simultaneously, Vaswani argues, it is able to more easily learn those dependencies.</p>
</li>
</ol>
<h2 id="is-that-all">Is that all?</h2>
<p>No. In part two I will describe the encoder and decoder blocks, as well as the self-supervised training process.</p>
]]></content>
        </item>
        
        <item>
            <title>Rolling My Own Blog Search</title>
            <link>https://lukesalamone.github.io/posts/rolling-my-own-blog-search/</link>
            <pubDate>Wed, 09 Nov 2022 02:42:51 -0700</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/rolling-my-own-blog-search/</guid>
            <description>I&amp;rsquo;ve found myself hitting ctrl+f on this blog enough that I figured it&amp;rsquo;s about time to add some search functionality to it. While there are certainly prefab solutions out there, this task is simple enough and fairly instructive. I had a few requirements, though:
The search needs to be fast, useful, and aesthetically pleasing. Search in the browser. Standing up a server is a lot of extra work. It&amp;rsquo;s also overkill since I only have about 30 articles so far.</description>
            <content type="html"><![CDATA[<p>I&rsquo;ve found myself hitting ctrl+f on this blog enough that I figured it&rsquo;s about time to add some search functionality to it. While there are certainly prefab solutions out there, this task is simple enough and fairly instructive. I had a few requirements, though:</p>
<ol>
<li>The search needs to be fast, useful, and aesthetically pleasing.</li>
<li>Search in the browser. Standing up a server is a lot of extra work. It&rsquo;s also overkill since I only have about 30 articles so far.</li>
</ol>
<h2 id="semantic-search">Semantic search</h2>
<p>I did some experiments with small neural networks deployed using ONNX but ultimately they didn&rsquo;t seem to be a good fit for this blog. The search experience was not quite as snappy as I&rsquo;d have liked it to be, and while I was able to get the model under 10MB, it still added a good amount of bloat to the page size. Further, it wasn&rsquo;t clear to me that the search results were significantly better, and in some cases they were worse. In any case, the advantages were not enough to justify the added page size.</p>
<p>I did learn a lot with this experiment which probably deserves a blog post of its own at some point. I might revisit semantic matching at a later point as well.</p>
<h2 id="bm25">BM25</h2>
<p>Rather than loading a neural network, I decided instead to index the text of my blog and use <a href="https://en.wikipedia.org/wiki/Okapi_BM25">BM25</a> to rank documents.</p>
<p>One difficulty in this was finding the actual algorithm which didn&rsquo;t include a ton of extra code. I was only interested in the function itself, not all of the extra boilerplate code floating around. In the end I broke down and rewrote it myself.</p>
<h2 id="tokenizing-text">Tokenizing Text</h2>
<p>BM25 works by scoring document tokens against query tokens. Documents with more relevant tokens should be scored higher than those with fewer relevant tokens.</p>
<p>This raises the question of how to convert a query string into a group of tokens. Initially, I simply split by word. That is,</p>
<pre><code>&quot;hello my friend&quot; =&gt; ['hello', 'my', 'friend']
</code></pre>
<p>Although this works pretty well, it has the downside that different forms of a word will be missed. For example &ldquo;friends&rdquo; will not match with &ldquo;friend&rdquo;. To solve this problem, I decided to use fixed token sizes of three characters instead:</p>
<pre><code>&quot;hello my friend&quot; =&gt; ['hel', 'ell', 'llo', 'lo#', '#my', 'my#', '#fr', 'fri', 'rie', 'ien', 'end']
</code></pre>
<p>In this way &ldquo;friend&rdquo; will be tokenized very similarly to &ldquo;friends&rdquo;, so the penalty for mismatches between forms of words will be minimized. Subjectively, I felt that this yielded a much more fluid and versatile ranking as well.</p>
<h2 id="multi-factor-scoring">Multi-factor scoring</h2>
<p>My blog posts have title, body, and tags. I could simply concatenate them all together and score the combined string, and it would work pretty well. But this seems to throw away the fact that the title and tags may have more concentrated information than the article body does. They deserve to be weighted differently. I decided to weight the title/body/tags components as 30/50/20 in the final ranking.</p>
]]></content>
        </item>
        
        <item>
            <title>A new type of chess tournament</title>
            <link>https://lukesalamone.github.io/posts/qualitative-analysis-chess/</link>
            <pubDate>Sat, 08 Oct 2022 15:17:36 -0700</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/qualitative-analysis-chess/</guid>
            <description>This is part 2 of a paper I wrote for Ken Forbus&amp;rsquo; Qualitative Reasoning course, adapted for this blog. You can find a printable version of the paper here and part 1 here.
In the previous post I discussed the history of chess engines and why they don&amp;rsquo;t &amp;ldquo;think&amp;rdquo; like we think. Trading interpretability for computation cycles ultimately led to the engines we have today, fairly alien in nature and perhaps less pedagogically useful because of it.</description>
            <content type="html"><![CDATA[<p><em><strong>This is part 2 of a paper I wrote for <a href="https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/forbus-ken.html">Ken Forbus</a>&rsquo; Qualitative Reasoning course, adapted for this blog. You can find a printable version of the paper <a href="/files/anthropomorphic-chess-evaluation-via-qualitative-analysis.pdf">here</a> and part 1 <a href="/posts/chess-engine-history/">here</a>.</strong></em></p>
<p>In the previous post I discussed the history of chess engines and why they don&rsquo;t &ldquo;think&rdquo; like we think. Trading interpretability for computation cycles ultimately led to the engines we have today, fairly alien in nature and perhaps less pedagogically useful because of it. At the time, though, the goal was to beat human grandmasters by any means necessary, a great engineering feat that the field had been working on for decades.</p>
<p>This post contains two related proposals. The first is a chess engine tournament, unique in the type of engine which will be permitted to enter and likely to succeed. Importantly, the vast majority of engines currently holding the highest performance ratings will likely not be effective.</p>
<p>The second proposal is the outlines of a chess engine that is likely to be successful in this tournament, taking advantage of the highly qualitative nature of chess position evaluation. Although it is unlikely to perform as strongly against top-performing engines, there are several distinct advantages of such an engine. In short, there is likely to be a great deal of educational value as well as financial incentive driving the construction of highly successful qualitative chess engines.</p>
<h1 id="qualitative-chess-analysis">Qualitative chess analysis</h1>
<p>Qualitatively, there are many aspects to a chess game that may be captured. Let&rsquo;s take a look at the way a grandmaster analyzes a position. It will become quite apparent that at the highest levels, the qualitative aspects of position analysis dominate over quantitative aspects (i.e. the number and value of each piece).</p>
<p>In the selected lecture, Grandmaster Varuzhan Akobian details a game he played previously. At a key moment of the game, Akobian sacrificed his rook for a key pawn in the center of the board. The resulting position is reproduced for convenience in Figure 1.</p>
<figure><img src="/img/human_analysis_fig4.png"
    alt="Figure 1. Quantitative analysis would posit that black is winning due to his extra rook for white‚Äôs knight and pawn. However, qualitative analysis provides a more complete picture of black‚Äôs predicament."><figcaption>
      <p>Figure 1. Quantitative analysis would posit that black is winning due to his extra rook for white‚Äôs knight and pawn. However, qualitative analysis provides a more complete picture of black‚Äôs predicament.</p>
    </figcaption>
</figure>

<p><a href="https://youtu.be/h80Mu4N6oYI?t=1515">His analysis starts at 25:15 in the video</a>:</p>
<blockquote>
<p>I would like you to spend a minute or two just to give me the evaluation of this position. It may not seem that clear because I‚Äôm down the exchange. [Novice chess players are taught that chess pieces have quantitative values, which may come into consideration when exchanging one piece for another. These values are measured in terms of pawns. Knights and bishops are generally understood to be worth 3 pawns, rooks are worth 5, and queens are worth 9.] I have a knight and a pawn for a rook. Rook is valued 5, knight and a pawn is 4. It may seem like I‚Äôm down a pawn here. But what do you think is the proper evaluation of this position?</p>
</blockquote>
<blockquote>
<p>&hellip;Basically white is very active. There are a few other things we can mention about white‚Äôs position, that it‚Äôs very strong. What else is very strong? White‚Äôs king is very safe, he cannot attack me. But how about the black king? Do you think the black king is very safe? [No.] For example, I could put my queen here [the e4 square] then I have a battery! Remember when we have a queen and a bishop on the same diagonal we call that a battery. And suddenly if I can deflect this queen [black queen on the g7 square] I will just go queen takes pawn, checkmate!</p>
</blockquote>
<blockquote>
<p>His dark square bishop is basically trapped behind his own pawns so it‚Äôs ineffective. . . . My bishop is very active. . . . And one more thing that you can mention. Passed pawn, exactly! And it‚Äôs a very strong passed pawn because with a knight on d6 very quickly [the pawn] will turn into [a queen].</p>
</blockquote>
<blockquote>
<p>. . . How much advantage does white have here? Big advantage, slight advantage, maybe winning? . . . We‚Äôre not going to use Houdini [a chess engine], Houdini will probably say black is slightly worse. But in practical play, I would be very comfortable to play this against anybody, and pretty comfortable I can win this position for white.</p>
</blockquote>
<p>Note that quantitative analysis is almost entirely absent from GM Akobian‚Äôs evaluation. Towards the beginning he mentions that he has sacrificed his rook for a knight and a pawn, and consequently is at a material deficit. However, he quickly discards this shallow evaluation, going so far as to label his subsequent qualitative evaluation as the ‚Äúproper‚Äù evaluation.</p>
<p>GM Akobian goes on to mention several other qualitative features of the position which are difficult to assign quantitative value to. Firstly, the activity of his pieces means that it is much easier to play the position as white because his pieces are on better squares, including some deep in black‚Äôs half of the board. The lack of activity is mentioned later on, noting that black‚Äôs bishop is essentially trapped behind his own pieces.</p>
<p>King safety is another difficult thing to quantify. In the given position, it is difficult to find a way that black can even check the white king. Moving the d8 rook to b1 will take 2 moves, and even then the b1 square is guarded by the bishop on d3. So the white king is indeed quite safe. In contrast black king is quite vulnerable, guarded mainly by the black queen, who is herself vulnerable to deflection or direct attack. (Deflection is a chess tactic which involves ‚Äúdistracting‚Äù an opponent‚Äôs piece which plays an important defensive role. For example, a piece which is defending two pieces simultaneously may be deflected by capturing one of the defended pieces.)</p>
<p>GM Akobian emphasizes the weakness of black‚Äôs king by sketching out a simple game-winning checkmate plan: arrange the bishop and queen in a battery which attacks the h7 pawn, deflect the black queen, and deliver checkmate with the queen by taking the h7 pawn. Although it is not immediately clear how to implement the plan, this type of simple plan creates a well-defined long-term ‚Äúthreat‚Äù that black must contend with.</p>
<p>Another threat he mentions is encompassed by white‚Äôs passed pawn on c5. This pawn may become a queen, which would become an insurmountable advantage for white. Therefore, this threat is another long-term vulnerability for black. (A passed pawn is a pawn which cannot be stopped or attacked by an opponent‚Äôs pawns. This occurs when there are no opponent pawns in the ‚Äúfile‚Äù (vertical column) of the pawn, as well as the file to the left and the right, if applicable. For example, a pawn in the C file is a passed pawn if there are no opponent pawns in the B, C or D files. A pawn in the H file is passed if there are no opponent pawns in the G or H files.)</p>
<p>Finally, note that GM Akobian does not assign a quantitative value to the board position, but rather a ‚Äúvery comfortable to win‚Äù assessment. Very little of this analysis involves quantities, but rather qualitative situations which must be dealt with. Consequently, it seems that qualitative reasoning is an ideal tool which a chess engine might use.</p>
<h2 id="qualitative-analysis-is-more-human">Qualitative analysis is more human</h2>
<p>The nature of expert-level perception described by GM Akobian was studied directly in a <a href="https://drive.google.com/file/d/1gvAMm39EVVN9bPnWVOdrAXU_MwDqkf0Z/view?usp=sharing">1973 paper</a> by Chase and Simon. Participants at three different levels of chess ability (a master-level player, an experienced club-level player, and a novice) were asked to complete two chess-related cognitive tasks. The first was a perception task, requiring him to reproduce a chess position on an adjacent board as quickly as possible, with the model board in plain view. The second task was a memory task, requiring participants to reproduce a position from memory after viewing it for only 5 seconds.</p>
<p>Importantly, the perceptual study attended to chess players‚Äô tendencies to ‚Äúchunk‚Äù the board position as they reproduced positions, tending to remember groups of interrelated pieces. These pieces tended to have relationships which the authors characterized in five ways: a piece attacks another, a piece defends another, two pieces are adjacent, two pieces are the same color, two pieces are the same type.</p>
<p>The results of the study found that ‚Äúthe C, S, and null relations are low because subjects are placing pieces which usually have multiple relations. Thus, from the within-glance relations, it appears that subjects are noticing the pawn structure, clusters of pieces of the same color, and attack and defense relations over small spatial distances.‚Äù In other words, it seems likely that human players use the qualitative relationships between pieces to remember the board.</p>
<h1 id="a-qualitative-chess-engine">A qualitative chess engine</h1>
<p>It is unlikely that a qualitative chess engine will be able to entirely do away with the basic structural algorithm involved in chess calculations, i.e. minimax. We would like our qualitative engine to calculate in a way most similar to humans, and thus will require some level of ply depth to the calculations. However, a qualitative engine will have a much stronger sense of the ‚Äúflow‚Äù of the game, and will thus explore fewer branches. Rather than considering each position as discrete, a qualitative engine should note how each move guides the evolution of the chess board position.</p>
<p>It is important to note that a qualitative chess engine may not be the most computationally efficient, a factor which was the primary motivation during the period of time when top chess engines needed to be run on supercomputers and every ounce of performance needed to be squeezed out of the machine. A qualitative engine should instead favor explainability over performance whenever possible. Instead, an engine should produce a good explanation of which moves were considered and why a particular move was chosen.</p>
<p>More modern qualitative research can improve upon Wilkins‚Äô knowledge-based PARADISE approach. It is important to recognize that his knowledge base is quite similar in nature to the FAC component of the retrieval model <a href="https://groups.psych.northwestern.edu/gentner/papers/GentnerForbus91.pdf">presented</a> by Forbus et al. in 1995, but does not take advantage of the performance speedups presented there. Because of the high number of positional examples <a href="https://database.lichess.org/#puzzles">available online</a>, there is a huge opportunity for a performant analogical retrieval system at present. The MAC/FAC retrieval system could pay huge performance dividends in retrieval if applied to this problem.</p>
<p>Specifically, the Lichess database referenced above contains 1,737,489 chess ‚Äúpuzzles‚Äù as of the time of writing. A chess puzzle is simply a chess board position in which players are encouraged to find the best move. Each puzzle relates to one or more chess ‚Äúthemes‚Äù (e.g. ‚Äúmate in 1‚Äù, ‚Äúpin‚Äù, ‚Äúdiscovered attack‚Äù, etc.), analogous to Wilkins‚Äô concepts outlined above. Each puzzle also includes the best move to make in the position. Some research will need to be done to derive meaning from this best move, relating it by analogy to the current position being evaluated by the engine.</p>
<p>Qualitative spatial calculi may also be used to construct more psychologically plausible models of chess positions than simply noting which pieces occupy which squares, seeking to emulate the models suggested by Chase and Simon. Chess pieces have intricate relationships which can be captured, and which change whenever a piece moves to another square. Importantly, however, not all relationships are affected by the movement of a single chess piece, suggesting that performance gains may be realized by recomputing only those relationships which have changed.</p>
<p>It is likely that low-level piece relationships may give rise to higher level relationships and tactics. For example, the concept of ‚Äúcapturing the defender‚Äù arises from the concept of attacking a piece A which defends B, which works when pieces A and B are attacked by pieces C and D of opposing colors. And in the case of Figure 2 below, a defender may be ‚Äúdeflected‚Äù to win the piece it is defending. Defensive relationships may be thought of in a chain or directed graph, with each piece defending another and the safety of a piece being considered in relation to its connection to a defensive group.</p>
<figure><img src="/img/human_analysis_fig5.png"
    alt="Figure 2. White to move. From this image, many basic piece relationships are apparent with only 8 pieces left on the board. Importantly, the black king is defending the black queen. The black queen is also being attacked by the white queen, and is attacking the white queen. White‚Äôs queen is undefended, a state sometimes referred to as hanging. These relationships reveal the opportunity for a tactic. White can simultaneously move his rook to attack black‚Äôs king (danger levels) and take advantage of the defensive connection between the black king and queen with the move rook to b8. This forces black to move his king, removing the defense of his queen. In this position, black‚Äôs queen can be freely captured by white‚Äôs queen. Due to the mobility advantages of a queen over a rook, this is a favorable move sequence for white."><figcaption>
      <p>Figure 2. White to move. From this image, many basic piece relationships are apparent with only 8 pieces left on the board. Importantly, the black king is defending the black queen. The black queen is also being attacked by the white queen, and is attacking the white queen. White‚Äôs queen is undefended, a state sometimes referred to as hanging. These relationships reveal the opportunity for a tactic. White can simultaneously move his rook to attack black‚Äôs king (danger levels) and take advantage of the defensive connection between the black king and queen with the move rook to b8. This forces black to move his king, removing the defense of his queen. In this position, black‚Äôs queen can be freely captured by white‚Äôs queen. Due to the mobility advantages of a queen over a rook, this is a favorable move sequence for white.</p>
    </figcaption>
</figure>

<h2 id="applications-of-a-qualitative-chess-engine">Applications of a qualitative chess engine</h2>
<p>There are many benefits to reopening the pursuit of qualitative reasoning in chess. The first and most clear value proposition is that qualitative reasoning is likely to serve as a more plausible model for how humans think about the game. This is evidenced by the fact that as Chase and Simon found, chess players do not ‚Äúsee‚Äù the whole board at once, but rather in chunks of interrelated pieces. Even if the details of human mental models differ slightly from the implementation of a qualitative reasoning engine, it will be able to provide a traceable account of its decision-making process, an important step towards explainability.</p>
<p>As we saw in <a href="/posts/chess-engine-history/">part 1</a>, current top chess engines reason about chess in ways that are quite contrary to human intuition. Stockfish uses full-width search, considering each move in each position without prejudice and assigning numerical values to each position. As we saw from the analysis from GM Akobian, qualitative evaluations are far more meaningful to humans.</p>
<p>ther chess engines approach chess in an even more alien way. Specifically, it is unlikely that any engine which makes heavy use of neural evaluation functions will model human-derived organic strategies in ways which chess players will recognize. At the far end of this spectrum is the fully neural Maia chess engine, but even Lc0‚Äôs Monte-Carlo tree search precludes consideration for cognitive plausibility.</p>
<p>Qualitative chess engines which are able to better reproduce the types of chess reasoning used by top human chess players are also likely to serve as better pedagogical tools for those interested in studying chess. This applies at every level, from beginner to grandmaster. The skill level of such a chess engine would be quite easily tunable simply by disabling more advanced knowledge from the knowledge base.</p>
<p>This is a far more natural method of ‚Äúhandicapping‚Äù than the search depth limitations used in current chess engines. Each piece of knowledge becomes a tunable parameter to the engine. As students learn concepts, the corresponding representations in the knowledge base could be enabled, allowing for gradual learning in a far more accessible way. In fact, it is likely true that a qualitative chess engine could outperform human grandmasters (who often teach chess to others) in this respect.</p>
<p>Finally, it is likely that a qualitative engine would become a key component of a first line of defense against cheating in chess. Most cheating is performed by using assistance from a chess engine during online games with unsuspecting opponents. Consulting a functionally omniscient computer program can thus provide a cheater with a theoretically insurmountable advantage.</p>
<p>In an <a href="https://youtu.be/bmIFdrUVHXw?t=2280">interview</a> with the Perpetual Chess Podcast, Chris Callahan of the popular chess website LiChess.org stated that the majority of employees of the website work primarily to detect cheaters and yet the problem still persists. By exploiting the difference between conventional full-width engines like Stockfish and a qualitative evaluation, those working to detect cheaters will be better equipped to detect ‚Äúsuspicious‚Äù moves. However, qualitative chess engines are unlikely to be able to completely replace human moderation.</p>
<h1 id="nerf-the-engine">Nerf the Engine!</h1>
<p>A computer chess tournament be held between chess engines can encourage the type of reasoning and gameplay which resembles human games. However, because we are not interested in the best overall chess engine, but one which can reason like a human might, the rules of the tournament will be adjusted in several key ways to discourage brute-force computational methods. We already know that calculating millions of positions can find the optimal move. But what happens when an engine is limited to e.g. 1000 positions?</p>
<p>Because we expect few entrants in early iterations of this special tournament, engineering an automatic enforcement mechanism for the limitations stipulated in this document are likely to be unnecessary. Engine compliance may simply be verified through manual inspection. Future iterations may include further safeguards, potentially separating the position evaluation function and directly counting the number of invocations while arbitrating the tournament to directly verify compliance.</p>
<h2 id="position-limitation">Position limitation</h2>
<p>Firstly, competing chess engines will be limited in the number of board positions they can evaluate during any one move. Because human grandmasters evaluate around 100 positions before making a move, the tournament arbitration system will artificially impose this limitation on all competing engines.</p>
<p>This cap immediately creates an issue for full-width chess engines because of chess‚Äô high branching factor. Were an engine to evaluate each possible move, it would perform quite poorly in board positions with many possible plies and replies available, rarely reaching a depth of more than 2 or 3. As a result, any engine which naively assesses a chess board would perform quite poorly in this setup.</p>
<p>The practical upshot of the position limitation is that the engine will be incentivized to gather as much relevant information about a position as possible rather than optimizing for the maximum number of positions.</p>
<h2 id="position-saliency">Position saliency</h2>
<p>Additionally, engines will be required to implement scheduling logic which takes the time remaining into consideration. While this creates the immediate problem of how an engine should allocate its time, it creates the ancillary challenge of evaluating a position‚Äôs quiescence. Positions which are ‚Äúquiet‚Äù and have few forcing moves require less evaluation than positions in which there are many non-forcing moves.</p>
<p>This requirement immediately motivates a qualitative chess engine to recognize the futility of falling prey to the Horizon Effect. The Horizon Effect causes engines to waste many position calculations pursuing delaying moves which amount to hopeless rabbit trails. Instead, an engine should recognize that quiescence has to do with the <em>relationships between pieces</em>. Humans understand this and can quite quickly see the futility of a move and terminate their search. A qualitative analysis which takes this factor into consideration will be able to save a great deal of position calculations, behaving more like a human player.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Given that computers have achieved and sustained superhuman capabilities in the domain of chess, the next frontier is not in building increasingly strong engines, but harnessing the present computational power to reason about the game in ways that humans do. Qualitative reasoning can provide novel and intuitive ways to reason about previously seen moves and think about the game.</p>
]]></content>
        </item>
        
        <item>
            <title>The Chess Engine&#39;s Final Horizon</title>
            <link>https://lukesalamone.github.io/posts/chess-engine-history/</link>
            <pubDate>Fri, 07 Oct 2022 20:17:21 -0700</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/chess-engine-history/</guid>
            <description>This is part 1 of a paper I wrote for Ken Forbus&amp;rsquo; Qualitative Reasoning course, adapted for this blog. You can find a printable version of the paper here and part 2 here.
Computers that play chess, otherwise known as chess engines, have existed since at least the late 1940s. Because the game was said to require the perfect combination of planning, strategy, psychology, and calculation, chess was once thought to be an activity directly correlated with intelligence, and that only a truly intelligent computer should be able to defeat humans.</description>
            <content type="html"><![CDATA[<p><em><strong>This is part 1 of a paper I wrote for <a href="https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/forbus-ken.html">Ken Forbus</a>&rsquo; Qualitative Reasoning course, adapted for this blog. You can find a printable version of the paper <a href="/files/anthropomorphic-chess-evaluation-via-qualitative-analysis.pdf">here</a> and part 2 <a href="/posts/qualitative-analysis-chess/">here</a>.</strong></em></p>
<p>Computers that play chess, otherwise known as chess engines, have existed <a href="https://www.youtube.com/watch?v=wrxdWkjmhKg">since at least the late 1940s</a>. Because the game was said to require the perfect combination of planning, strategy, psychology, and calculation, chess was once thought to be an activity directly correlated with intelligence, and that only a truly intelligent computer should be able to defeat humans. However, as a recent chess.com <a href="https://drive.google.com/file/d/11IokKgTVSXdpYEzAuyViIleSZ_2wl0ag/view">report</a> explains, computers are now far stronger than humans:</p>
<blockquote>
<p>Human chess and computer chess are different, even at the highest levels. The best humans play at an Elo rating of 2800. ‚ÄúStockfish,‚Äù the most powerful chess engine, has an estimated rating of more than 3500. In a theoretical match between World Champion Magnus Carlsen vs. Stockfish, we estimate that it is most likely that Magnus Carlsen would lose every single game‚Äîno wins and no draws.</p>
</blockquote>
<p>The supremacy of Machine over Man should be marked as a great triumph in artificial intelligence, and in a broad sense it is. However, I would argue that playing skill should not be the only goal, and a truly useful engine should be interpretable as well. We&rsquo;ve already &ldquo;solved&rdquo; the problem of playing chess at an extremely high level. Yet for historical reasons, interpretability was sacrificed for the sake of speed and playing strength. However, these tradeoffs may not make as much sense today. <a href="/posts/qualitative-analysis-chess/">Part 2</a> discusses opportunities and clear benefits for reevaluating those decisions moving forward.</p>
<h1 id="background">Background</h1>
<p>The general algorithm for performing search in zero-sum perfect information games like chess is known as minimax. The algorithm attempts to find the move which minimizes the opponent&rsquo;s maximum (i.e. best) move. Minimax visualizes the possible future state of the game as a tree, and the value of each node in the tree as a function of the nodes following from it. If you could compute the full game tree (in simple games like tik-tac-toe this is possible), you would be able to enumerate all possible terminal nodes, and propagate the result (1 for a win, -1 for a loss, 0 for a draw) back up the tree.</p>
<p>That&rsquo;s not possible in chess. The game tree is too big. Instead, the value of many of the nodes needs to be <em>estimated using heuristics</em> so that they can be propagated upwards. Therefore a fast and accurate static evaluation function is critical. (However, since the problem is recursive, the problem of creating a perfectly accurate static evaluator is as hard as evaluating that node&rsquo;s entire associated game tree.)</p>
<p>Minimax is often augmented with ‚Äúalpha-beta pruning‚Äù to reduce the number of positions which will be evaluated. This effectively cuts the <a href="https://imgur.com/wRZINqS">computational complexity exponent in half</a> by removing from consideration those branches which cannot affect the final result.</p>
<p>Other challenges in minimax evaluation exist as well. In particular, a phenomenon dubbed the ‚ÄúHorizon Effect‚Äù is a peculiar failure mode of minimax searches. The Horizon Effect was <a href="https://drive.google.com/file/d/1XHhMQUgZHhD8Be7Klrl7_RMHQm8B9Zlc/view">first described</a> by grandmaster and computer scientist Hans Berliner in 1975. His illustration of the problem (his Figure 1.3) is reproduced in Figure 1. Algorithms that do not account for the Horizon Effect will try to ‚Äúpush‚Äù bad outcomes of their search beyond their search horizon, instead opting to make hopeless moves which only serve to delay the inevitable. As humans we intuitively understand this illusion.</p>
<figure><img src="/img/human_analysis_fig1.png"
    alt="Figure 1. White to move. Here, white‚Äôs bishop on a4 is doomed, attacked by black‚Äôs pawn on b5. White could delay the inevitable by moving his bishop to b3, but then black simply seals the bishop‚Äôs fate with pawn to c4. In that position, white does not have time to save his bishop, and it will be captured no matter what on the next move by the pawn on c4. Due to the Horizon Effect, at a limited depth white will not recognize this and will play hopeless moves like pawn from e4 to e5, temporarily attacking the knight but easily parried by capturing with the pawn on d6. This phenomenon is deemed the ‚ÄúHorizon Effect‚Äù because by pushing negative outcomes beyond the ‚Äúhorizon‚Äù of calculation depth, the engine is able to trick itself into believing that the problem doesn‚Äôt exist. A true case of ‚Äúsee no evil‚Äù."><figcaption>
      <p>Figure 1. White to move. Here, white‚Äôs bishop on a4 is doomed, attacked by black‚Äôs pawn on b5. White could delay the inevitable by moving his bishop to b3, but then black simply seals the bishop‚Äôs fate with pawn to c4. In that position, white does not have time to save his bishop, and it will be captured no matter what on the next move by the pawn on c4. Due to the Horizon Effect, at a limited depth white will not recognize this and will play hopeless moves like pawn from e4 to e5, temporarily attacking the knight but easily parried by capturing with the pawn on d6. This phenomenon is deemed the ‚ÄúHorizon Effect‚Äù because by pushing negative outcomes beyond the ‚Äúhorizon‚Äù of calculation depth, the engine is able to trick itself into believing that the problem doesn‚Äôt exist. A true case of ‚Äúsee no evil‚Äù.</p>
    </figcaption>
</figure>

<h2 id="branching-factor">Branching Factor</h2>
<p>In any given position, there may be many possible moves. By some estimates, chess has an average branching factor of around 30, with other estimates putting the number around 40. It isn‚Äôt easy to find a concrete value for the branching factor of chess. One source claims (without citation) that the branching factor may be <a href="https://courses.csail.mit.edu/6.034f/ai3/rest.pdf">up to 40</a> (see pg. 117). However, a more recent statistical <a href="https://chess.stackexchange.com/a/24325">analysis</a> of 2.5 million chess games put the real number closer to 31.</p>
<figure><img src="/img/avg_moves_available.png"
    alt="The branching factor depends on the stage of the game; middle games have far more available moves than end games. Because not all games are the same length, shorter games will tend to have higher average branching factors than longer ones. Barnes&amp;rsquo; graphic is reproduced above."><figcaption>
      <p>The branching factor depends on the stage of the game; middle games have far more available moves than end games. Because not all games are the same length, shorter games will tend to have higher average branching factors than longer ones. Barnes&rsquo; graphic is reproduced above.</p>
    </figcaption>
</figure>

<p>In any case, it is possible to dramatically reduce the branching factor by employing ‚Äúselective search‚Äù, which means excluding nodes from recursive tree search altogether. Reducing the number of possible moves that will be explored in any given position promises dramatic computational speedups, allowing the tree to be searched deeper at the expense of width. Given the slow speeds (by contemporary standards) of early computers, the allure of such a technique should be clear. It is also a more psychologically plausible way of playing, since most players quickly rule out ridiculous seeming moves and focus on the most promising ones.</p>
<h1 id="early-chess-engines">Early Chess Engines</h1>
<p>An early attempt to narrow the search tree was proposed by Berliner, who in 1975 devised CAPS-II which utilized a tree search algorithm with five total ‚Äúreference levels‚Äù including ALPHA and BETA. His paper also was cognizant of the work of Chase and Simon from two years prior, recognizing the need for a bottom-up board representation. Unfortunately, his board representation was largely geometrical and included little in the way of qualitative relationships between pieces. Nevertheless, the resulting program was able to solve tactical puzzles in a quite impressive manner for the time.</p>
<p>Another example of the use of such a narrowed search tree is <a href="https://drive.google.com/file/d/1I54sVUe4Ybkln5hy5_MlZBnmrZBqbj2l/view?usp=sharing">PARADISE (PAttern Recognition Applied to DIrecting SEarch)</a>. This system used a knowledge base containing 200 ‚Äúproduction rules‚Äù to match board patterns and determine the best move at any time. An example production rule is shown in Figure 2.</p>
<figure><img src="/img/human_analysis_fig2.png"
    alt="Figure 2. A sample production rule from the PARADISE knowledge base. This production rule detects and acts upon opponent‚Äôs trapped pieces. A trapped piece is identified as a non-pawn defensive piece which cannot move to another square without being captured and is not already attacked. Finally, the production rule describes the threat of this action (winning the piece) and how likely it is to succeed."><figcaption>
      <p>Figure 2. A sample production rule from the PARADISE knowledge base. This production rule detects and acts upon opponent‚Äôs trapped pieces. A trapped piece is identified as a non-pawn defensive piece which cannot move to another square without being captured and is not already attacked. Finally, the production rule describes the threat of this action (winning the piece) and how likely it is to succeed.</p>
    </figcaption>
</figure>

<p>Because the tree search was narrower, the system was able to search to higher depths. Still, because of hardware limitations of the time, PARADISE was extremely slow, generating only 109 nodes in 20 minutes of search time. (Current chess engines evaluate millions of nodes per second.)</p>
<p>PARADISE executes static analysis on a position using its many production rules with the ultimate goal of creating and executing a plan. Matched production rules post concepts to the database in addition to information about the intentions of the concept and its likelihood of success.</p>
<p>The program is then able to use this information to form a plan, which is a set of actions for the side to move, along with corresponding plans for each defensive alternative. Because there may be many potential alternatives at each move, this plan includes many branches.</p>
<figure><img src="/img/human_analysis_fig3.png"
    alt="Figure 3. White to move, PARADISE has produced a plan which involves checking the black king by moving the knight to g5, then checking the black king by moving the rook to d7. Depending on black‚Äôs next move white will then try to either capture the queen on d4 or the rook on d7."><figcaption>
      <p>Figure 3. White to move, PARADISE has produced a plan which involves checking the black king by moving the knight to g5, then checking the black king by moving the rook to d7. Depending on black‚Äôs next move white will then try to either capture the queen on d4 or the rook on d7.</p>
    </figcaption>
</figure>

<p>An example of such a plan is reproduced in Figure 3 above. (An interesting sidebar about this position is that based upon Stockfish analysis, this position results in inescapable checkmate for black within 15 moves. That is, even with optimal play, black will be checkmated in 15 or fewer moves. The winning sequence begins with white moving his queen to e5, exploiting the pin on the f6 pawn from the f1 rook.)</p>
<p>Searching only selective lines is more difficult to implement than full-width search. Further, selective searches may miss important continuations of a position, causing the computer to select an incorrect move. It was for this reason that Berliner himself, an original proponent for the application of strict logical rules in chess, decided to seek <a href="https://www.nytimes.com/2017/01/16/business/hans-berliner-master-chess-player-and-programmer-dies-at-87.html">brute-force search methods</a> instead.</p>
<h1 id="modern-chess-engines">Modern Chess Engines</h1>
<p>Stockfish is currently the #1 ranked chess engine in the world. Stockfish is open-source and performs a ‚Äúfull width‚Äù search on the game tree. Leaf nodes are evaluated using either a classical, hand-crafted evaluation heuristic, or more recently, a neural network evaluator called NNUE. The classical evaluation function uses a set of around 30 factors weighted empirically using a dedicated testing framework called Fishtest. Altogether, the project has <a href="https://github.com/official-stockfish/Stockfish">around 200 contributors</a>. Optimized for speed, Stockfish can evaluate around <a href="https://chessify.me/blog/nps-what-are-the-nodes-per-second-in-chess-engine-analysis">millions of nodes per second</a> on a typical 4-core computer.</p>
<figure><img src="/img/stockfish_starting_pos.png"
    alt="Stockfish analyzing the starting position on my laptop at around 1 million nodes per second."><figcaption>
      <p>Stockfish analyzing the starting position on my laptop at around 1 million nodes per second.</p>
    </figcaption>
</figure>

<p>Neural networks may also be used more directly. <a href="https://lczero.org/dev/wiki/technical-explanation-of-leela-chess-zero/">Leela Chess Zero</a>, also known as Lc0 is another open-source chess engine, modeled after DeepMind‚Äôs AlphaZero chess engine. Lc0 uses Predictor + Upper Confidence Bound tree search (PUCT) to search its game tree. Leela evaluates new nodes by iteratively choosing moves from a probability distribution until it reaches an unexplored node. At that point, Leela&rsquo;s neural network estimates the node&rsquo;s value and propagates that value back up the tree. PUCT is very similar to Monte Carlo Tree Search, but with game ‚Äúrollouts‚Äù replaced by neural network evaluations.</p>
<p>One other notable engine is <a href="https://arxiv.org/abs/2006.01855">Maia Chess</a>. Maia is interesting for two reasons. Unlike most other engines, Maia&rsquo;s objective is to model human behavior rather than to perform optimally. This is interesting because Maia is in many cases trained on suboptimal data. The other notable feature about Maia is the unusual manner in which moves are selected: instead of performing any type of tree search at all, the engine simply returns the neural network&rsquo;s static evaluation of the current board position.</p>
<p>Suffice it to say, however, that despite computation no longer being a significant limitation, prior qualitative approaches still have garnered relatively little attention.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Engineers have historically sought to build the most powerful chess engine possible. With the singular goal of defeating human (and now computer) opponents, explainability was an expedient but understandable tradeoff to make. In the present day, however, there is no longer a question of whether humans or computers are superior chess players. It seems quite clear that the time has come to revisit some of the tradeoffs made in the past.</p>
<p>In <a href="/posts/qualitative-analysis-chess/">part 2</a> I will discuss some clear benefits of a more interpretable chess engine, and some possible routes of getting there.</p>
]]></content>
        </item>
        
        <item>
            <title>When Suboptimal Minimax is Better</title>
            <link>https://lukesalamone.github.io/posts/suboptimal-minimax/</link>
            <pubDate>Sat, 02 Jul 2022 16:24:10 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/suboptimal-minimax/</guid>
            <description>Minimax solves for optimal opponent play, minimizing the best move an opponent could make. But what if we knew the opponent wouldn&amp;rsquo;t make the best move? What if we knew what the opponent would do ahead of time? In that case, we could beat them faster by playing moves which take advantage of this fact, even if our move isn&amp;rsquo;t objectively the best move. Don&amp;rsquo;t play the game, play the man.</description>
            <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      },
      extensions: ["AMSmath.js", "AMSsymbols.js"]
    }
  }
});
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<p>Minimax solves for optimal opponent play, minimizing the best move an opponent could make. But what if we knew the opponent wouldn&rsquo;t make the best move? What if we knew what the opponent would do ahead of time? In that case, we could beat them faster by playing moves which take advantage of this fact, even if our move isn&rsquo;t objectively the best move. <a href="https://www.youtube.com/watch?v=XsdefZRavj4">Don&rsquo;t play the game, play the man.</a></p>
<p>For the purposes of this problem let&rsquo;s assume it is a chess algorithm but this concept can be generalized to any algorithm which can use minimax. Anyone who has played a game against another person understands the idea of &ldquo;downloading&rdquo; your opponent: playing against a person enough that you feel like you know what they will do. <a href="https://www.youtube.com/watch?v=42LatV92k1s">Hikaru Nakamura does</a>.</p>
<p>It seems like this type of player would have many possible applications, including training for specific skill levels and even trainers personalized to an individual&rsquo;s playing style.</p>
<h2 id="prior-work">Prior work</h2>
<p>There has been <a href="https://www.chessprogramming.org/Opponent_Model_Search#Further_Research">previous work</a> in terms of opponent modeling in chess. <a href="https://maiachess.com/">Maia chess</a> has been trained to model several Elo levels of opponent play which makes it ideal to be used as a backbone in my experiments.</p>
<h2 id="the-policy-function">The Policy Function</h2>
<p>We can think of the policy function as a model of the opponent. We will therefore be optimizing an algorithm to play against this model, rather than the optimal opponent.</p>
<p>The specifics of the policy function are as follows. The function takes two parameters, a board position <code>B</code> and an opponent skill level <code>L</code>. The parameter <code>L</code> is important because the <em>nature</em> of the opponent&rsquo;s suboptimal play is important. All levels of chess players play sub-optimally, but the types of mistakes a beginner or intermediate-level player will make are different from a grandmaster. The algorithm should exploit this fact. In practice, this algorithm should play in a much more &ldquo;trappy&rdquo; way than the methodical slow grind of a typical chess engine.</p>
<p>Formally, at move $i$ there will be $n_i$ possible moves. For each of the $n_i$ moves, we should calculate an evaluation weighted by the opponent&rsquo;s probability of playing responses to that move:</p>
<p style="text-align: center; font-size: 1.5em">
$ e_i(m_i) = p_L(M_{i+1}) \cdot r(M_{i+1}) $
</p>
<p>Where $M_{i+1}$ is the set of possible moves the opponent can make in response to move $m_i$, $p_L$ is the probability distribution of each move in $M_i$ being made, and $r$ is some real evaluation of the board position.</p>
<p>Let&rsquo;s use an example. Suppose that at some stage in the game the algorithm must choose between 2 possible moves. For simplicity, the opponent will have 4 possible replies after each of the two possible moves. After the algorithm makes the the first possible move, the probabilities look like this:</p>
<h3 id="opponent-move-distribution-at-position-1">Opponent move distribution at position 1</h3>
<table>
<thead>
<tr>
<th>Opponent Move</th>
<th>Probability of play</th>
<th>Resulting position evaluation</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.6</td>
<td>5</td>
</tr>
<tr>
<td>2</td>
<td>0.3</td>
<td>1</td>
</tr>
<tr>
<td>3</td>
<td>0.1</td>
<td>0.5</td>
</tr>
<tr>
<td>4</td>
<td>0.1</td>
<td>-1</td>
</tr>
</tbody>
</table>
<p style="text-align: center; font-size: 1.5em">
$ \begin{bmatrix} 0.6 \\ 0.3 \\ 0.1 \\ 0.1 \end{bmatrix}
\cdot \begin{bmatrix} 5 \\ 1 \\ 0.5 \\ -1 \end{bmatrix} = 3.25 $
</p>
<h3 id="opponent-move-distribution-at-position-2">Opponent move distribution at position 2</h3>
<p>And after the second possible move, the opponent move distribution looks like this:</p>
<table>
<thead>
<tr>
<th>Opponent Move</th>
<th>Probability of play</th>
<th>Resulting position evaluation</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.9</td>
<td>4</td>
</tr>
<tr>
<td>2</td>
<td>0.05</td>
<td>0.5</td>
</tr>
<tr>
<td>3</td>
<td>0.03</td>
<td>0</td>
</tr>
<tr>
<td>4</td>
<td>0.02</td>
<td>-2</td>
</tr>
</tbody>
</table>
<p style="text-align: center; font-size: 1.5em">
$ \begin{bmatrix} 0.9 \\ 0.05 \\ 0.03 \\ 0.02 \end{bmatrix}
\cdot \begin{bmatrix} 4 \\ 0.5 \\ 0 \\ -2 \end{bmatrix} = 3.585 $
</p>
<p><em>In these tables positions are evaluated from the algorithm&rsquo;s point of view, so higher evaluation=better. Assume the evaluation function is a standard evaluation from a chess engine e.g. Stockfish.</em></p>
<p>In this example it is clear that the algorithm should choose move 2, because it is highly likely that after making this move the algorithm will have a decisive advantage.</p>
<h2 id="checkmates">Checkmates</h2>
<p>All of this works quite well in the middle-game, where position evaluations are all within a reasonable range. But what happens when one of the opponent&rsquo;s moves leads to checkmate? How much value should we give to the possibility that an opponent might blunder into checkmate? The above calculation only works for finite numbers. The evaluation of a checkmate is +/- infinity. Instead, let&rsquo;s choose a large, finite number for checkmate.</p>
<p>The behavior of the algorithm depends heavily on our choice for this checkmate evaluation, the <code>CHECKMATE_WEIGHT</code>. If the number is too high, the algorithm will favor positions where the opponent <em>might</em> get checkmated, playing risky moves. If the number is too low, the algorithm will choose other branches which win less quickly. After a bit of trial and error, I settled on +10 for checkmate. This is enough for the algorithm to seek out these types of positions.</p>
<p>A further question is what to do once checkmate is inevitable. That is, what should the algorithm do once it will definitely win? One option is to continue to sample from the distribution, continuing to play the move which maximizes winning chances. At the beginning, this was my intuition. Unfortunately, this leads to many draws by repetition.</p>
<p>Instead, once the game is in a forced-win situation (&ldquo;mate in N moves&rdquo;) the algorithm will stop sampling from the distribution and simply play the moves which lead to checkmate. While this will inevitably lead to a win, it changes the character of the algorithm somewhat. Therefore, I have limited the lookahead to checkmates in the immediate next few moves (specifically, mate in 5 or fewer moves). Since the original goal was to create an algorithm which is used for training, this seems reasonable.</p>
<h2 id="games">Games</h2>
<p>I simulated 400 matches testing this opponent modeling approach, which I wrote about <a href="/posts/best-antimaia-games/">here</a>.</p>
]]></content>
        </item>
        
        <item>
            <title>Alphabet Chess</title>
            <link>https://lukesalamone.github.io/posts/alphabet-chess/</link>
            <pubDate>Fri, 10 Jun 2022 23:56:14 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/alphabet-chess/</guid>
            <description>TLDR: Alphabet chess is a chess variant that allows handicapping by mixing in a bit of poker into the beginning of the game. Moves must be played according to a secret word at the beginning of the game.
Chess has been played in different forms since the seventh century, and in its modern form since the nineteenth century. Opening theory, i.e. the study of the best moves to begin the game with, has been developing since then.</description>
            <content type="html"><![CDATA[<p><em><strong>TLDR: Alphabet chess is a chess variant that allows handicapping by mixing in a bit of poker into the beginning of the game. Moves must be played according to a secret word at the beginning of the game.</strong></em></p>
<p>Chess has been played in different forms since the seventh century, and in its modern form since the nineteenth century. Opening theory, i.e. the study of the best moves to begin the game with, has been developing since then.</p>
<h2 id="the-egg-opening">The EGG Opening</h2>
<p>I was inspired the other day after watching a <a href="https://www.youtube.com/watch?v=J6G3cP991Yc">video</a> by Eric Rosen called &ldquo;Quadruple Egg&rdquo;. Chess boards are usually notated from left to right with the letters A through H. (These columns are called &ldquo;files&rdquo;.) The &ldquo;egg&rdquo; opening, therefore, involves moving the E pawn, then the G pawn twice, spelling the word &ldquo;egg&rdquo;. It&rsquo;s an extremely unconventional opening, but how bad is it compared with unrestricted openings?</p>
<h2 id="what-are-the-best-ie-least-bad-alphabetic-openings">What are the best (i.e. least bad) alphabetic openings?</h2>
<p>We can call the Egg Opening one version of an alphabetical chess opening: an opening where the piece moved must start on a file corresponding to the next letter in a given word. In this definition, the pieces don&rsquo;t have to be pawns.</p>
<p>This raises an interesting question: what are the best and worst alphabetical openings? Under engine evaluation, they will all be losing, because the opponent is not limited in which piece they can move. However, as we will see, some are much worse than others.</p>
<h3 id="four-letters">Four letters</h3>
<p>English doesn&rsquo;t have an official dictionary, so I chose <a href="https://gist.github.com/paulcc/3799331">this list</a> of four letter words. From there, we need to eliminate all words with letters after H. This leaves us with <a href="https://gist.github.com/lukesalamone/eab96ddde6eb326b8e339c41f5f52bda">37 words</a>. Don&rsquo;t ask me what they all mean:</p>
<pre><code>abbe, abed, aced, ache, aged, agha,
baba, babe, bach, bade, bead, beef,
cafe, cage, ceca, cede, chad, chef,
dace, dada, dead, deaf, deed,
each, edda, edge, egad,
face, fade, feed,
gaff, gaga, gage, geed, ghee,
head, heed
</code></pre>
<p>A simple way of measuring how bad each of these is would be to evaluate them against Stockfish to see how badly we&rsquo;re losing after playing these moves. In this simulation, we play as white each time.</p>
<p><img alt="depth 20" src="/img/alpha_chess_disadvantage_depth_20.png"></p>
<p>This measurement approach has a problem, though: Stockfish is assuming that white can make any move, and playing black&rsquo;s moves accordingly. In reality, white&rsquo;s moves are extremely restricted. If black knew about this restriction, he would be much more aggressive.</p>
<p>Further, if black were to discover which word white is playing, he could punish white. And even if black doesn&rsquo;t know exactly which word white is playing, black could eliminate moves which definitely can&rsquo;t form English words. For example, if white&rsquo;s first move is H, the next move will definitely be E.</p>
<h2 id="alphabetical-chess">Alphabetical Chess</h2>
<p>What if both players were required to play their first four moves from dictionary words? And what if those words were pre-assigned but secret to each other? For example, if each player drew a card from a deck. In this game, if a player is unable to make a move with a piece of the correct letter, they lose.</p>
<p>There are a few interesting features of this chess variant:</p>
<ol>
<li>Your starting word will have a big impact on how the game will turn out.</li>
<li>Figuring out your opponent&rsquo;s word will give you a big advantage.</li>
<li>Checks and other attacks can be devastating. You may not be able to respond to the attack, and checks may end the game immediately.</li>
</ol>
<p>In that case, it isn&rsquo;t necessarily true that the best and worst words from above will still be the best and worst when pitted against another randomly chosen word. It&rsquo;s a rock-paper-scissors situation: depending on which word A you happen to get, there is another word B which best counters your word, and another word C which your word best counters.</p>
<p><img alt="alphabet chess heatmap" src="/img/alpha_chess_heatmap.png"></p>
<p>Here the y-axis shows the word which was played with the white pieces, and the x-axis shows the word played with black.</p>
<h2 id="cool-things-about-alphabet-chess">Cool Things About Alphabet Chess</h2>
<p>There are a few interesting benefits to this variant:</p>
<ol>
<li>
<p>Handicaps are quite easily built in. The longer your secret word is, the more restricted your play is and the more of a handicap you will have.</p>
</li>
<li>
<p>An element of gamesmanship and imperfect information. Players can win on the board with good moves, but also &ldquo;off-board&rdquo; by figuring out their opponent&rsquo;s secret word.</p>
</li>
<li>
<p>Prevents good players from using memorized openings.</p>
</li>
</ol>
<h2 id="play-alphabet-chess">Play Alphabet Chess</h2>
<p>If you&rsquo;d like to try it out with a friend and a chessboard, here&rsquo;s an online tool that&rsquo;ll pick a random word: <a href="https://alphachess.surge.sh">https://alphachess.surge.sh</a></p>
]]></content>
        </item>
        
        <item>
            <title>Paper Summary: COMET (Knowledge Graph Construction)</title>
            <link>https://lukesalamone.github.io/posts/knowledge-graph-construction/</link>
            <pubDate>Tue, 17 May 2022 17:47:25 +0700</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/knowledge-graph-construction/</guid>
            <description>Selected {subject, relation, object} tuples generated by COMET
Paper link: https://arxiv.org/abs/1906.05317
This paper describes COMET, a method of generating knowledge bases automatically. Previous work largely focused on encyclopedic knowledge, which has well-defined relationships. This paper, however, focuses on commonsense knowledge. In this paper the authors introduce a ‚Äúcommonsense transformer‚Äù which trains on a knowledge base consisting of tuples and a pre-trained language model. Their trained model generates new nodes in the knowledge graph and completes phrases based on edges in the existing graph.</description>
            <content type="html"><![CDATA[<figure><img src="/img/comet_knowledge_generation.png"
    alt="Selected {subject, relation, object} tuples generated by COMET"><figcaption>
      <p>Selected {subject, relation, object} tuples generated by COMET</p>
    </figcaption>
</figure>

<p>Paper link: <a href="https://arxiv.org/abs/1906.05317">https://arxiv.org/abs/1906.05317</a></p>
<p>This paper describes COMET, a method of generating knowledge bases automatically. Previous work largely focused on encyclopedic knowledge, which has well-defined relationships. This paper, however, focuses on commonsense knowledge. In this paper the authors introduce a ‚Äúcommonsense transformer‚Äù which trains on a knowledge base consisting of tuples and a pre-trained language model. Their trained model generates new nodes in the knowledge graph and completes phrases based on edges in the existing graph.</p>
<h2 id="training">Training</h2>
<p>The training data consists of tuples in the form of {subject, relation, object}, and the task is to generate the object given the subject and relation. The model architecture was the Vaswani GPT. The architecture is fairly straightforward for a transformer. The only difference of note is the representation of the knowledge tuple as a concatenated sequence of the words of each item of the tuple, along with a position embedding.</p>
<p>To train, the researchers train COMET with Atomic and ConceptNet as knowledge seed sets. The Atomic dataset contains 877k tuples of social commonsense knowledge. Experiments with Atomic used bleu-2 as an automatic evaluation metric. The researchers also used Amazon Mechanical Turk to identify whether the generated responses were reasonable.</p>
<p>The researchers also tested on ConceptNet. Relations were in the same {subject, relation, object} format. This dataset was evaluated using perplexity on the test set and accuracy of the generated positive samples on the pre-trained Bilinear AVG model.</p>
<h2 id="results">Results</h2>
<p>Atomic experiments showed that the model is able to generate coherent knowledge even when only 10% of the data in a domain is available. The researchers found that 91.7% of the greedily decoded ConceptNet examples were correct. The model seems to be producing fairly good knowledge tuples. However, sometimes the generated tuples are merely simplifications of previous knowledge.</p>
]]></content>
        </item>
        
        <item>
            <title>How to Create a Custom Pytorch Dataloader</title>
            <link>https://lukesalamone.github.io/posts/pytorch-dataloader/</link>
            <pubDate>Thu, 28 Apr 2022 18:22:07 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/pytorch-dataloader/</guid>
            <description>First, create a custom dataset class.
from torch.utils.data import Dataset, DataLoader class CustomDataset(Dataset): def __init__(self, features, labels): assert len(features) == len(labels) self.features = features self.labels = labels def __len__(self): return len(self.features) def __getitem__(self, idx): return self.features[idx], self.labels[idx] Next, create a custom dataloader where we specify the batch size.
features, labels = load_data() # features &amp;amp; labels must have equal lengths # e.g. features = [[1,2,3],[4,5,6]] # labels = [7,8] dataset = CustomDataset(features, labels) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) Finally, iterate over the dataloader during training.</description>
            <content type="html"><![CDATA[<p>First, create a custom dataset class.</p>
<pre><code class="language-python">from torch.utils.data import Dataset, DataLoader

class CustomDataset(Dataset):
  def __init__(self, features, labels):

    assert len(features) == len(labels)
    self.features = features
    self.labels = labels

  def __len__(self):
    return len(self.features)

  def __getitem__(self, idx):
    return self.features[idx], self.labels[idx]
</code></pre>
<p>Next, create a custom dataloader where we specify the batch size.</p>
<pre><code class="language-python">features, labels = load_data()

# features &amp; labels must have equal lengths
# e.g. features = [[1,2,3],[4,5,6]]
#      labels = [7,8]

dataset = CustomDataset(features, labels)
dataloader = DataLoader(dataset,
                        batch_size=batch_size,
                        shuffle=True)
</code></pre>
<p>Finally, iterate over the dataloader during training.</p>
<pre><code class="language-python">for epoch in range(num_epochs):
  for x, y in train_dataloader:
    # do stuff
</code></pre>
]]></content>
        </item>
        
        <item>
            <title>How to Zip and Unzip a tar.gz File</title>
            <link>https://lukesalamone.github.io/posts/how-to-tar-untar-file/</link>
            <pubDate>Wed, 30 Mar 2022 20:05:26 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/how-to-tar-untar-file/</guid>
            <description>If you want to extract a tar archive
tar -xf archive.tar.gz If you want to compress a directory
tar -czvf archive.tar.gz /path/to/directory That&amp;rsquo;s all.</description>
            <content type="html"><![CDATA[<p>If you want to extract a tar archive</p>
<pre><code class="language-console">tar -xf archive.tar.gz
</code></pre>
<p>If you want to compress a directory</p>
<pre><code class="language-console">tar -czvf archive.tar.gz /path/to/directory
</code></pre>
<p>That&rsquo;s all.</p>
]]></content>
        </item>
        
        <item>
            <title>Paper Summary: Defending Against Neural Fake News</title>
            <link>https://lukesalamone.github.io/posts/grover-paper-summary/</link>
            <pubDate>Sun, 19 Sep 2021 20:13:09 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/grover-paper-summary/</guid>
            <description>Defending Against Neural Fake News by Zellers et al. presents a model for controllable text generation called Grover. This model can be used to create highly believable computer-generated news articles. The authors present this paper as a method of detecting and preventing the spread of fake news. They claim their model is 92% accurate at detecting fake news stories, partially due to artifacts that generators include in the generated text.</description>
            <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1905.12616"><em>Defending Against Neural Fake News</em></a> by Zellers et al. presents a model for controllable text generation called Grover. This model can be used to create highly believable computer-generated news articles. The authors present this paper as a method of detecting and preventing the spread of fake news. They claim their model is 92% accurate at detecting fake news stories, partially due to artifacts that generators include in the generated text.</p>
<p>Grover functions in an adversarial manner:</p>
<ul>
<li>an ‚Äúadversary‚Äù generates synthetic stories</li>
<li>a ‚Äúdiscriminator‚Äù identifies fake stories</li>
</ul>
<p>Current generative models are fairly good at creating realistic-looking text, but largely lack the ability to be controlled via tunable parameters. In contrast, Grover models news stories as distributions like (<em>domain</em>, <em>date</em>, <em>authors</em>, <em>headline</em>, <em>body</em>) to be sampled from. Adversaries specify domain, date, and headline, and the generator creates the body and author as well as a more appropriate headline.</p>
<figure><img src="/img/grover-prob-distribution.png"
    alt="Grover samples from a joint distribution of the parts of a news article."><figcaption>
      <p>Grover samples from a joint distribution of the parts of a news article.</p>
    </figcaption>
</figure>

<p>Grover is built using a transformer architecture similar to BERT and GPT. Depending on the model size, the number of parameters varies from similar to GPT to on par with GPT-2. The authors use the RealNews corpus, resulting in 120 gigabytes of total file size. Training took 2 weeks on 256 TPU v3 cores.</p>
<figure><img src="/img/grover-vs-human.png"
    alt="Grover is actually better at writing synthetic articles than humans are according to authors."><figcaption>
      <p>Grover is actually better at writing synthetic articles than humans are according to authors.</p>
    </figcaption>
</figure>

<p>The results of the authors&rsquo; subjective experiments show that humans have a hard time identifying Grover-written propaganda. Using Mechanical Turk, articles were rated according to stylistic consistency, content sensibility, and overall trustworthiness. Grover, on the other hand, turns out to be a fairly good discriminator.</p>
<p>The authors also tested GPT2, BERT, and FastText on the task of classifying news as human or synthetic. The researchers set up the experiment in an unpaired setting (give a human/synthetic classification for a single article) and a paired setting (determine which is the human and which is the synthetic between 2 articles). Unsurprisingly, the paired setting was far easier. Also unsurprisingly, the larger models perform better at discriminating when paired with smaller generators.</p>
<figure><img src="/img/grover-discrimination-results.png"
    alt="For generators of the same size, discrimination accuracy was around 90-91%."><figcaption>
      <p>For generators of the same size, discrimination accuracy was around 90-91%.</p>
    </figcaption>
</figure>

<p>When comparing generator/discriminator pairs with the same numbers of parameters, classification accuracy was around 90-92%.
Grover tends to leave artifacts when generating text, and this fact may be part of the reason discriminators are so good at identifying synthetic text. For one, the authors identify ‚Äúexposure bias‚Äù as one of the reasons. The fact that Grover is never trained on generated text, only on human-authored articles seems to contribute to this. Perplexity also tends to vary over the length of the generated article, and depending on the sampling variance may fall out of the distribution of human language.</p>
]]></content>
        </item>
        
        <item>
            <title>Connect Jupyter to Remote</title>
            <link>https://lukesalamone.github.io/posts/connect-jupyter-to-remote/</link>
            <pubDate>Tue, 07 Sep 2021 09:10:56 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/connect-jupyter-to-remote/</guid>
            <description>Here&amp;rsquo;s how to connect to a remote Jupyter notebook.
Create an ssh tunnel to your remote machine:
ssh -L 8080:localhost:8080 user@12.34.56.78 # or use a .pem file to connect to ec2 ssh -L 8080:localhost:8080 -i &amp;quot;aws.pem&amp;quot; ec2-user@ec2-12-34-56-78.compute-1.amazonaws.com Start Jupyter on that machine in headless mode:
jupyter notebook --no-browser --port=8080 Use a browser to open one of the urls that Jupyter presents:
http://localhost:8080/?token=xyz</description>
            <content type="html"><![CDATA[<p>Here&rsquo;s how to connect to a remote Jupyter notebook.</p>
<p>Create an ssh tunnel to your remote machine:</p>
<pre><code>ssh -L 8080:localhost:8080 user@12.34.56.78

# or use a .pem file to connect to ec2
ssh -L 8080:localhost:8080 -i &quot;aws.pem&quot; ec2-user@ec2-12-34-56-78.compute-1.amazonaws.com
</code></pre>
<p>Start Jupyter on that machine in headless mode:</p>
<pre><code>jupyter notebook --no-browser --port=8080
</code></pre>
<p>Use a browser to open one of the urls that Jupyter presents:<br>
http://localhost:8080/?token=xyz</p>
]]></content>
        </item>
        
        <item>
            <title>What is Marginalization?</title>
            <link>https://lukesalamone.github.io/posts/what-is-marginalization/</link>
            <pubDate>Wed, 07 Jul 2021 14:23:12 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/what-is-marginalization/</guid>
            <description>In machine learning and statistics, marginalization simply means summing over a set of independent variables. For example, suppose an avid tennis player kept track of the number of days he played tennis over a period of time as well as the weather on that day:
weather sunny cloudy rainy totals play? yes 70 25 1 96 no 70 5 9 84 totals 140 30 10 180 (In this table we&amp;rsquo;re keeping track of the number of days.</description>
            <content type="html"><![CDATA[<p>In machine learning and statistics, marginalization simply means summing over a set of independent variables. For example, suppose an avid tennis player kept track of the number of days he played tennis over a period of time as well as the weather on that day:</p>
<style>
  .blue {
    background-color:#09f1;
  }
  .gray {
    background-color:#80808012;
  }
</style>
<table style="width:100%">
  <tr style="font-weight:bold">
    <th></th>
    <th></th>
    <th colspan="3" style="text-align:center">weather</th>
    <th></th>
  </tr>
  <tr style="font-weight:bold; text-align:center; background-color: inherit">
    <td></td>
    <th></th>
    <td>sunny</td>
    <td>cloudy</td>
    <td>rainy</td>
    <th>totals</th>
  </tr>
  <tr>
  	<td rowspan="2" style="font-weight:bold; text-align:right">play?</td>
    <td style="text-align:right">yes</td>
    <td class="blue">70</td>
    <td class="blue">25</td>
    <td class="blue">1</td>
    <td class="gray" style="font-weight:bold">96</td>
  </tr>
  <tr style="background-color: inherit;">
  	<td style="text-align:right">no</td>
  	<td class="blue">70</td>
    <td class="blue">5</td>
    <td class="blue">9</td>
    <td class="gray" style="font-weight:bold">84</td>
  </tr>
  <tr style="font-weight:bold">
  	<td colspan="2" style="text-align:right">totals</td>
  	<td class="gray">140</td>
    <td class="gray">30</td>
    <td class="gray">10</td>
    <td class="gray">180</td>
  </tr>
</table>
<p>(<em>In this table we&rsquo;re keeping track of the number of days. If you want probabilities, divide each value in the table by 180. But I think whole numbers are easier to think about so I&rsquo;m keeping them.</em>)</p>
<p>To marginalize one of the variables, we just sum one of the variables. For example, to marginalize the weather, we would sum each of the rows to find that 96/180=53% of the time tennis was played, and 47% of the time tennis was not played. Likewise, to marginalize the boolean variable of whether tennis was played, we just sum the columns: no matter whether tennis was played on that day, how many days was it sunny? 140.</p>
<p>Another way of saying this is the marginal distribution of sunny weather is the first column (containing 70 and 70). The marginal distribution of playing tennis is the first row (containing 70, 25, and 1).</p>
]]></content>
        </item>
        
        <item>
            <title>Colab: Connect to Google Drive</title>
            <link>https://lukesalamone.github.io/posts/connect-to-colab/</link>
            <pubDate>Wed, 30 Jun 2021 22:58:18 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/connect-to-colab/</guid>
            <description>Here&amp;rsquo;s how to connect your Google Colab notebook to your Drive directory:
from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) Follow the prompts from there. That is all.</description>
            <content type="html"><![CDATA[<p>Here&rsquo;s how to connect your Google Colab notebook to your Drive directory:</p>
<pre><code class="language-python">from google.colab import drive
drive.mount('/content/gdrive')
</code></pre>
<p>Follow the prompts from there. That is all.</p>
]]></content>
        </item>
        
        <item>
            <title>BERT vs GPT-2 Performance</title>
            <link>https://lukesalamone.github.io/posts/bert-vs-gpt2/</link>
            <pubDate>Mon, 21 Jun 2021 01:04:42 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/bert-vs-gpt2/</guid>
            <description>There are quite a few BERT vs GPT-2 breakdowns online, mostly focusing on the architectural differences between the two models. However, I am more interested in the performance differences between the two models, specifically their predictive capabilities. This blog post outlines the results of my experiments.
The code used in this experiment can be found on my Github
BERT The Devlin et al. model was released in November 2018. It is a transformer-based language model pretrained on masked input (also known as the cloze task).</description>
            <content type="html"><![CDATA[<p>There are quite a few BERT vs GPT-2 breakdowns online, mostly focusing on the architectural differences between the two models. However, I am more interested in the performance differences between the two models, specifically their predictive capabilities. This blog post outlines the results of my experiments.</p>
<p><a href="https://github.com/lukesalamone/gpt2-vs-bert">The code used in this experiment can be found on my Github</a></p>
<h2 id="bert">BERT</h2>
<p>The <a href="https://arxiv.org/pdf/1810.04805.pdf">Devlin et al. model</a> was released in November 2018. It is a transformer-based language model pretrained on masked input (also known as the <em>cloze</em> task). During pretraining, 15% of tokens are hidden from the model, and it is trained to predict the masked tokens. As a result, I was able to evaluate its ability to correctly predict a masked token at a random position in a fixed-size input.</p>
<p>I looked at the following varieties of BERT:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th># Parameters</th>
<th>Compare to</th>
</tr>
</thead>
<tbody>
<tr>
<td>bert-base-uncased</td>
<td>110 million</td>
<td>gpt2</td>
</tr>
<tr>
<td>bert-base-cased</td>
<td>109 million</td>
<td>gpt2</td>
</tr>
<tr>
<td>bert-large-uncased</td>
<td>336 million</td>
<td>gpt2-medium</td>
</tr>
<tr>
<td>bert-large-cased</td>
<td>335 million</td>
<td>gpt2-medium</td>
</tr>
</tbody>
</table>
<p>This table also includes corresponding GPT-2 models which have a similar number of parameters. <a href="https://huggingface.co/transformers/pretrained_models.html">Source</a></p>
<h2 id="gpt-2">GPT-2</h2>
<p>The <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Radford et al. model</a> hit the scene in February of 2019. Like BERT it is a transformer-based  model, and comes in various sizes ranging from 117M parameters up to 1.5B parameters (gpt2-xl). Because GPT-2 is an autoregressive model, experiments with this family of models perform one token of generation following input context, comparing with the target token for accuracy measurement.</p>
<p>Here we will be evaluating two flavors of this model:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th># Parameters</th>
<th>Compare to</th>
</tr>
</thead>
<tbody>
<tr>
<td>gpt2</td>
<td>117 million</td>
<td>bert-base</td>
</tr>
<tr>
<td>gpt2-medium</td>
<td>345 million</td>
<td>bert-large</td>
</tr>
</tbody>
</table>
<p>This table also includes corresponding BERT models which have a similar number of parameters. <a href="https://huggingface.co/transformers/pretrained_models.html">Source</a></p>
<h2 id="wikitext-token-prediction">Wikitext Token prediction</h2>
<p>To evaluate the models, I sampled 10,000 random sequences from <a href="https://paperswithcode.com/dataset/wikitext-2">Wikitext-2</a>.</p>
<p><strong>For BERT</strong>, a random sequence of 100 tokens is selected. Then, for each sequence, a random position within that sequence is selected and masked. BERT will be required to predict this token, so accuracy is measured as the percentage of the time which its masked token is predicted correctly.</p>
<p><strong>For GPT-2</strong>, a random sequence of 100 tokens is selected. Then, for each sequence, a random position within that sequence is selected. Because GPT-2 is autoregressive, it cannot attend to tokens on the right, so the sequence is truncated at the selected position. The sequence is then padded appropriately to maintain a fixed sequence length of 100.</p>
<p>Below we can see the performance of all 6 models on these tasks. The data has been smoothed by bucketing into groups of 5 positions at once (i.e. positions 0-4, 5-9, etc). You can see that performance of GPT-2 continues to rise as it is given additional context, while BERT models are relatively stable after being given around 5 tokens of context. Interestingly, BERT performance drops off quite steeply over the last 5-10 token positions.</p>
<script src="https://cdn.jsdelivr.net/npm/chart.js@3.4.0/dist/chart.min.js" type="text/javascript"></script>
<script src="/js/bert-vs-gpt2.js"></script>
<script src="/js/util.js"></script>
<div id="graph1"><canvas></canvas></div>
<p>When we zoom in on the final 10 positions, things start to get interesting. Both varieties of GPT-2 actually beat out all varieties of BERT at the final position.</p>
<div id="graph2"><canvas></canvas></div>
<h2 id="conclusion">Conclusion</h2>
<p>BERT and GPT-2 perform quite differently on the token prediction task depending on the position of the token being predicted. For a fixed sequence length of 100 tokens, BERT performs best when the masked token is between positions 5 and 95, while GPT-2 tends to continually improve as context length increases. Interestingly, when the final token in the sequence is to be predicted, BERT&rsquo;s performance falls off dramatically, while GPT-2 performance remains stable.</p>
]]></content>
        </item>
        
        <item>
            <title>How does GPT-2 Tokenize Text?</title>
            <link>https://lukesalamone.github.io/posts/gpt2-tokenization/</link>
            <pubDate>Thu, 17 Jun 2021 19:30:48 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/gpt2-tokenization/</guid>
            <description>Let&amp;rsquo;s explore how GPT-2 tokenizes text.
What is tokenization? It&amp;rsquo;s important to understand that GPT-2 doesn&amp;rsquo;t work with strings directly. Instead, it needs to tokenize the input string, which is essentially a process for converting the string into a list of numbers, or &amp;ldquo;tokens&amp;rdquo;. It is these tokens which are passed into the model during training or for inference. As a concrete example, let&amp;rsquo;s look at a few sample sentences:</description>
            <content type="html"><![CDATA[<p>Let&rsquo;s explore how GPT-2 tokenizes text.</p>
<h2 id="what-is-tokenization">What is tokenization?</h2>
<p>It&rsquo;s important to understand that GPT-2 doesn&rsquo;t work with strings directly. Instead, it needs to tokenize the input string, which is essentially a process for converting the string into a list of numbers, or &ldquo;tokens&rdquo;. It is these tokens which are passed into the model during training or for inference. As a concrete example, let&rsquo;s look at a few sample sentences:</p>
<pre><code class="language-python">tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokens1 = tokenizer('I love my dog')
</code></pre>
<p>When we look at <code>tokens1</code> we see there are 4 tokens:</p>
<pre><code class="language-python">{'input_ids': [40, 1842, 616, 3290], 'attention_mask': [1, 1, 1, 1]}
</code></pre>
<p>Here what we care about is the <code>'input_ids'</code> list. We can ignore the <code>'attention_mask'</code> for now. We can convert the tokens in <code>[40, 1842, 616, 3290]</code> back into strings using <code>tokenizer.decode</code>:</p>
<pre><code class="language-python">tokens1 = tokens1['input_ids']

[tokenizer.decode(x) for x in tokens1]
# prints ['I', ' love', ' my', ' dog']

[tokenizer.decode(x).strip().lower() for x in tokens1]
# prints ['i', 'love', 'my', 'dog']
</code></pre>
<p>This process allows us to recover the tokens as strings from the tokenizer. For dictionary lookups, we&rsquo;ll also lowercase the strings and remove the whitespace from them.</p>
<p>Now, let&rsquo;s see what happens when we do the same thing with more complex words:</p>
<pre><code class="language-python">tokens2 = tokenizer('My favorite color is chartreuse')['token_ids']
[tokenizer.decode(x).strip().lower() for x in tokens2]
# prints ['my', 'favorite', 'color', 'is', 'chart', 're', 'use']
</code></pre>
<p>Because &ldquo;chartreuse&rdquo; isn&rsquo;t in GPT-2&rsquo;s vocabulary, it is tokenized as &ldquo;chart&rdquo;, &ldquo;re&rdquo; and &ldquo;use&rdquo;.</p>
<h3 id="about-that-attention-mask">About that attention mask</h3>
<p>For brevity I glossed over what <code>attention_mask</code> does above. If you&rsquo;re interested in attention masks, <a href="/posts/what-are-attention-masks/">I have a blog post on that very topic</a>!</p>
<h2 id="english-words">English words</h2>
<p>Now it would be interesting to see how many tokens in GPT-2&rsquo;s vocabulary are actually English words. This is an imprecise metric since it depends heavily on which dictionary we use. (There is no single authoritative source of all English words.) I&rsquo;ll use several dictionaries and compare the results.</p>
<h3 id="enchant">Enchant</h3>
<p><a href="https://pyenchant.github.io/pyenchant/tutorial.html">PyEnchant</a> contains a python module <code>enchant</code> which we can use to check if a word is spelled correctly. It can also make spelling suggestions for incorrectly spelled words:</p>
<pre><code class="language-python">import enchant
d = enchant.request_dict(&quot;en_US&quot;)
d.check('Hello')
# prints True

d.check('Helo')
# prints False
</code></pre>
<h3 id="nltk-words">NLTK words</h3>
<p>The popular NLP library <a href="https://www.nltk.org/">NLTK</a> also contains a word list, accessible through its <code>corpus</code> module.</p>
<pre><code class="language-python">from nltk.corpus import words

nltk_words = set(words.words())
len(nltk_words)
# prints 235892
</code></pre>
<h3 id="english-350k">English 350k</h3>
<p>This list of words was taken from this <a href="https://github.com/dwyl/english-words/blob/master/words_alpha.txt">github repository</a>. It is a convenient list of lowercased words containing only letters. It seems to be the biggest of the word lists.</p>
<h3 id="lemmatization">Lemmatization</h3>
<p>We can bump our numbers up slightly through <a href="https://en.wikipedia.org/wiki/Lemmatisation">lemmatization</a>:</p>
<blockquote>
<p>In many languages, words appear in several inflected forms. For example, in English, the verb &rsquo;to walk&rsquo; may appear as &lsquo;walk&rsquo;, &lsquo;walked&rsquo;, &lsquo;walks&rsquo; or &lsquo;walking&rsquo;. The base form, &lsquo;walk&rsquo;, that one might look up in a dictionary, is called the lemma for the word. The association of the base form with a part of speech is often called a lexeme of the word.</p>
</blockquote>
<p>For our lemmatizer we will use <code>WordNetLemmatizer</code> from <code>nltk.stem.wordnet</code>.</p>
<h2 id="testing-gpt-2-tokens">Testing GPT-2 tokens</h2>
<p>So of the tokens which GPT-2 uses, how many are English words? We can break this down metric by the dictionary used.</p>
<table>
<thead>
<tr>
<th>Dictionary</th>
<th>% Words</th>
</tr>
</thead>
<tbody>
<tr>
<td>English370k ‚Ä†</td>
<td>72.92%</td>
</tr>
<tr>
<td>English370k</td>
<td>72.59%</td>
</tr>
<tr>
<td>Enchant ‚Ä†</td>
<td>60.48%</td>
</tr>
<tr>
<td>Enchant</td>
<td>60.17%</td>
</tr>
<tr>
<td>NLTK words ‚Ä†</td>
<td>57.07%</td>
</tr>
<tr>
<td>NLTK words</td>
<td>48.27%</td>
</tr>
</tbody>
</table>
<p><em>‚Ä† indicates words were lemmatized</em></p>
<p>So the English370k word list seems to capture the most tokens from the three dictionaries. Also note the mild impact of lemmatization: although it may bump some of the percentages up a bit, it&rsquo;s not enough for one dictionary to outperform another.</p>
<div id="pie"><canvas></canvas></div>
<p>Looking at the tokens which aren&rsquo;t in the dictionary, around 73% of them are non-word alphabetical strings. The final 27% is accounted for by symbols, numbers, and non-ascii character sequences (unicode characters from languages like Arabic, Korean, and Chinese). If we remove these, we end up with about 10k tokens containing only letters, which is around 21% of GPT-2&rsquo;s total vocabulary. I&rsquo;ve included this list in a <a href="https://gist.github.com/lukesalamone/22ce6f362db3bdd09eda3cc5cbf5576f">github gist</a> (duplicates removed).</p>
<h2 id="now-what">Now what?</h2>
<p>Looking at these non-word alphabetical strings, it&rsquo;s interesting to see how the Internet (as GPT-2 saw it) was encoded. Then again, it also contains a lot of proper nouns which wouldn&rsquo;t be in a normal dictionary like &ldquo;starbucks&rdquo;.</p>
<p>Other tokens are clearly vestiges of the scraping process which was used to gather text which GPT-2 trained on. Tokens like &ldquo;rawdownloadcloneembedreportprint&rdquo;, &ldquo;buyableinstoreandonline&rdquo;, &ldquo;randomredditorwithno&rdquo;, and &ldquo;itemthumbnailimage&rdquo; contain next to zero semantic value and the vocabulary space would probably have been better served with more meaningful tokens.</p>
<p>The following are the longest non-dictionary tokens found in GPT-2&rsquo;s vocabulary:</p>
<table>
<thead>
<tr>
<th>Token ID</th>
<th>String</th>
</tr>
</thead>
<tbody>
<tr>
<td>39177</td>
<td>ItemThumbnailImage</td>
</tr>
<tr>
<td>30210</td>
<td>guiActiveUnfocused</td>
</tr>
<tr>
<td>39755</td>
<td>isSpecialOrderable</td>
</tr>
<tr>
<td>31576</td>
<td>externalActionCode</td>
</tr>
<tr>
<td>39753</td>
<td>quickShipAvailable</td>
</tr>
<tr>
<td>39757</td>
<td>channelAvailability</td>
</tr>
<tr>
<td>36174</td>
<td>RandomRedditorWithNo</td>
</tr>
<tr>
<td>30899</td>
<td>cloneembedreportprint</td>
</tr>
<tr>
<td>40242</td>
<td>BuyableInstoreAndOnline</td>
</tr>
<tr>
<td>30906</td>
<td>rawdownloadcloneembedreportprint</td>
</tr>
</tbody>
</table>
<p>We may also be able to measure performance of GPT-2 on certain tasks based on how many of the tokens were dictionary words. It might be true, for example, that sentences with higher proportions of dictionary word tokens would perform better on sentence completion tasks.</p>
<script src="/js/chart.min.js" type="text/javascript"></script>
<script src="/js/gpt2-tokens-pie.js" type="text/javascript"></script>
]]></content>
        </item>
        
        <item>
            <title>What Are Attention Masks?</title>
            <link>https://lukesalamone.github.io/posts/what-are-attention-masks/</link>
            <pubDate>Tue, 15 Jun 2021 19:09:36 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/what-are-attention-masks/</guid>
            <description>TLDR: Attention masks allow us to send a batch into the transformer even when the examples in the batch have varying lengths. We do this by padding all sequences to the same length, then using the &amp;ldquo;attention_mask&amp;rdquo; tensor to identify which tokens are padding.
Here we use a batch with three samples padded from the left since we want to predict the next token on the right. (Padding on the right would probably predict another pad.</description>
            <content type="html"><![CDATA[<p>TLDR: Attention masks allow us to send a batch into the transformer even when the examples in the batch have varying lengths. We do this by padding all sequences to the same length, then using the &ldquo;attention_mask&rdquo; tensor to identify which tokens are padding.</p>
<figure><img src="/img/attention_mask.png"
    alt="Here we use a batch with three samples padded from the left since we want to predict the next token on the right. (Padding on the right would probably predict another pad.)"><figcaption>
      <p>Here we use a batch with three samples padded from the left since we want to predict the next token on the right. (Padding on the right would probably predict another pad.)</p>
    </figcaption>
</figure>

<p>If you want to perform inference with transformers one sequence at a time, you can ignore attention masks. The &ldquo;slow way&rdquo; will be sufficient for your needs.</p>
<h2 id="the-slow-way">The slow way</h2>
<p>We can perform inference with GPT-2 using sequences one at a time, but it&rsquo;s slow:</p>
<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')

context = tokenizer('It will rain in the', return_tensors='pt')

prediction = gpt2.generate(**context, max_length=10)
tokenizer.decode(prediction[0])
# prints 'It will rain in the morning, and the rain'
</code></pre>
<p>It&rsquo;s way faster to batch the inputs, which means adding their token vectors to the context and performing inference only once.</p>
<h2 id="the-un-slow-way">The un-slow way</h2>
<p>The cool way to perform inference on many samples is with batching. It&rsquo;s much faster but it&rsquo;s also slightly more complicated.</p>
<pre><code class="language-python">tokenizer.padding_side = &quot;left&quot;
tokenizer.pad_token = tokenizer.eos_token

sentences = [&quot;It will rain in the&quot;,
            &quot;I want to eat a big bowl of&quot;,
            &quot;My dog is&quot;]
inputs = tokenizer(sentences, return_tensors=&quot;pt&quot;, padding=True)

output_sequences = gpt2.generate(**inputs)

for seq in output_sequences:
    print(tokenizer.decode(seq))
</code></pre>
<p>What&rsquo;s happening here? And what does this have to do with attention masks? First let&rsquo;s explain padding, then take a look at the code line by line.</p>
<p>We feed tokens into transformer-based language models like GPT-2 and BERT for inference as <a href="https://pytorch.org/docs/stable/tensors.html">tensors</a>. A tensor is like a python list but with a few extra features and restrictions. Specifically, for a tensor of dimension 2+, all vectors in that dimension need to be the same length. For example,</p>
<pre><code class="language-python">from torch import tensor

tensor([[1,2], [3,4]])  # ok
tensor([[1,2], [3]])   # error!
</code></pre>
<p>When we tokenize an input, it it will be turned into a tensor containing sequence of integers, each corresponding to an item in the transformer&rsquo;s vocabulary. Here is an example tokenization in GPT-2:</p>
<table>
<thead>
<tr>
<th>String</th>
<th>Token ID</th>
</tr>
</thead>
<tbody>
<tr>
<td>It</td>
<td>1026</td>
</tr>
<tr>
<td>will</td>
<td>481</td>
</tr>
<tr>
<td>rain</td>
<td>6290</td>
</tr>
<tr>
<td>in</td>
<td>287</td>
</tr>
<tr>
<td>the</td>
<td>262</td>
</tr>
</tbody>
</table>
<p>Suppose we wanted to include a second sequence in our input:</p>
<table>
<thead>
<tr>
<th>String</th>
<th>Token ID</th>
</tr>
</thead>
<tbody>
<tr>
<td>My</td>
<td>3666</td>
</tr>
<tr>
<td>dog</td>
<td>3290</td>
</tr>
<tr>
<td>is</td>
<td>318</td>
</tr>
</tbody>
</table>
<p>Because these two sequences have different lengths, we can&rsquo;t just combine them in one tensor. Instead, we have to <em>pad</em> the shorter sequences with dummy tokens so that each sequence is the same length. And because we want the model to continue to add to the right side of our sequence, we will pad the left side of shorter sequences.</p>
<table>
<thead>
<tr>
<th>String</th>
<th>Token ID</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>&lt;pad&gt;</code></td>
<td>50256</td>
</tr>
<tr>
<td>My</td>
<td>3666</td>
</tr>
<tr>
<td>dog</td>
<td>3290</td>
</tr>
<tr>
<td>is</td>
<td>318</td>
</tr>
</tbody>
</table>
<p>This is where the attention mask comes in. The attention mask simply shows the transformer which tokens are padding, placing 0s in the positions of padding tokens and 1s in the positions of actual tokens. Now that we understand that, let&rsquo;s look at the code line by line.</p>
<pre><code class="language-python">tokenizer.padding_side = &quot;left&quot;
</code></pre>
<p>This line tells the tokenizer to begin padding from the left (default is right) because the logits of the rightmost token will be used to predict future tokens.</p>
<pre><code class="language-python">tokenizer.pad_token = tokenizer.eos_token
</code></pre>
<p>This line specifies which token we will use for padding. It doesn&rsquo;t matter which one you choose, but here we&rsquo;re choosing the &ldquo;end of sequence&rdquo; token.</p>
<pre><code class="language-python">sentences = [&quot;It will rain in the&quot;,
            &quot;I want to eat a big bowl of&quot;,
            &quot;My dog is&quot;]
</code></pre>
<p>These three sequences all have different lengths when tokenized, so should be a good test of our padding method.</p>
<pre><code class="language-python">inputs = tokenizer(sentences, return_tensors=&quot;pt&quot;, padding=True)
</code></pre>
<p>Now we tokenize. We&rsquo;re passing in the sentences from above, telling the tokenizer to use PyTorch tensors (rather than Tensorflow), and telling the tokenizer to add padding for us. We can print <code>inputs</code> here to confirm that, yes, tokenization is working as we thought:</p>
<pre><code class="language-python">{'input_ids': tensor([
    [50256, 50256, 50256,  1026,   481,  6290,   287,   262],
    [   40,   765,   284,  4483,   257,  1263,  9396,   286],
    [50256, 50256, 50256, 50256, 50256,  3666,  3290,   318]
  ]),
'attention_mask': tensor([
    [0, 0, 0, 1, 1, 1, 1, 1],
    [1, 1, 1, 1, 1, 1, 1, 1],
    [0, 0, 0, 0, 0, 1, 1, 1]
  ])}
</code></pre>
<p>As you can see, the first and third sequence include padding at the beginning, and the <code>attention_mask</code> parameter marks the position of this padding.</p>
<p>Now let&rsquo;s actually pass this input into the model to generate new text:</p>
<pre><code class="language-python">output_sequences = gpt2.generate(**inputs)
</code></pre>
<p>If you&rsquo;re unfamiliar with <code>**kwargs</code> syntax for function calls, this passes in the <code>inputs</code> dict as named parameters, using the keys as the parameter names and the values as the corresponding argument values. <a href="https://docs.python.org/3/tutorial/controlflow.html#keyword-arguments">Check the docs for more info</a>.</p>
<p>Finally, we just need to loop through each of the generated sequences and print out the result in human readable form, using the <code>decode()</code> function to convert token IDs to strings.</p>
<pre><code class="language-python">for seq in output_sequences:
    print(tokenizer.decode(seq))
</code></pre>
]]></content>
        </item>
        
        <item>
            <title>How Does Convolution Work?</title>
            <link>https://lukesalamone.github.io/posts/how-does-convolution-work/</link>
            <pubDate>Mon, 14 Jun 2021 21:05:06 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/how-does-convolution-work/</guid>
            <description>Convolutional neural networks have had breakthrough success in image recognition, natural language processing, and even board games like Chess and Go. But what&amp;rsquo;s really going on during convolution? Well, I think the easiest way to explain is with an interactive demo. Feel free to play around with the parameters below to see for yourself!
number: four three eight padding: kernel size: stride: speed: You can use the settings above to control the hyperparameters of the convolutional layer.</description>
            <content type="html"><![CDATA[<p>Convolutional neural networks have had breakthrough success in image recognition, natural language processing, and even board games like Chess and Go. But what&rsquo;s really going on during convolution? Well, I think the easiest way to explain is with an interactive demo. Feel free to play around with the parameters below to see for yourself!</p>
<script src="/js/util.js"></script>
<script src="/js/convolution-demo.js"></script>
<link rel="stylesheet" href="/css/convolution-demo.css" />
<div id="input-output">
  <div id="input-grid"></div>
  <div id="output-grid"></div>
</div>
<div id="controls">
  <table>
    <tr id="number">
      <td>number:</td>
      <td>
        <select>
          <option value="four">four</option>
          <option value="three">three</option>
          <option value="eight">eight</option>
        </select>
      </td>
    </tr>
    <tr id="padding">
      <td>padding: <span class="val"></span></td>
      <td><input type="range" min="0" max="2" value="0"></td>
    </tr>
    <tr id="kernelsize">
      <td>kernel size: <span class="val"></span></td>
      <td><input type="range" min="1" max="4" value="2"></td>
    </tr>
    <tr id="stride">
      <td>stride: <span class="val"></span></td>
      <td><input type="range" min="1" max="3" value="1"></td>
    </tr>
    <tr id="speed">
      <td>speed: <span class="val"></span></td>
      <td><input type="range" min="1" max="5" value="3"></td>
    </tr>
    <tr id="errors" style="display:none">
      <td colspan="2"></td>
    </tr>
  </table>
</div>
<p>You can use the settings above to control the hyperparameters of the convolutional layer.</p>
<p>As you&rsquo;ve probably figured out, the left side shows the input, a 20x20 handwritten digit, and the right side shows the output as it is being drawn. A convolutional layer will pass a filter over the input, selecting only a small group of pixels at a time.</p>
<p>Not shown: once the filter selects a window of pixels, it will compute a dot product with the pixel values and its own weights. In this demo the filter weights are assumed to be 1 for simplicity.</p>
<h2 id="more-on-the-hyperparameters">More on the hyperparameters</h2>
<p>Sometimes the settings will result in an invalid convolution. This happens if the kernel extends beyond the activation map. We can write a python function to check whether it will be valid:</p>
<pre><code class="language-python">def valid_convolution(input_size, kernel_size, padding, stride):
  if (input_size + 2*padding - kernel_size) % stride == 0:
    return True
  else:
    return False
</code></pre>
<h3 id="padding">Padding</h3>
<p>Increasing the padding will add extra pixels, typically zeros, around the edges of the input. This is done in order to ensure that the layer parameters will be valid. Additionally, it causes pixels which would have been on the edges to be more to the middle of the activation map.</p>
<h3 id="kernel-size">Kernel Size</h3>
<p>This refers to the &ldquo;sliding window&rdquo; which passes over the input. Choosing a larger kernel tends to increase the &ldquo;smudging&rdquo; which happens in the output, while a smaller kernel preserves more information for deeper layers. Sometimes kernel size is referred to as filter size.</p>
<h3 id="stride">Stride</h3>
<p>Stride refers to the distance the sliding window will move between steps. A stride of 1 moves the window one pixel to the right, while a stride of 2 moves the window 2 pixels. A larger stride can reduce output layer size at the expense of granularity.</p>
]]></content>
        </item>
        
        <item>
            <title>Python: Serve an HTML File</title>
            <link>https://lukesalamone.github.io/posts/python-serve-html/</link>
            <pubDate>Sun, 09 May 2021 15:06:11 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/python-serve-html/</guid>
            <description>If you want to serve some HTML with python run
python -m http.server 8000 Then navigate to http://localhost:8000.
This is not meant for production environments but will get you around CORS restrictions that would come from simply opening a local file in your browser.</description>
            <content type="html"><![CDATA[<p>If you want to serve some HTML with python run</p>
<pre><code class="language-console">python -m http.server 8000
</code></pre>
<p>Then navigate to <a href="http://localhost:8000/">http://localhost:8000</a>.</p>
<p>This is not meant for production environments but will get you around CORS restrictions that would come from simply opening a local file in your browser.</p>
]]></content>
        </item>
        
        <item>
            <title>How to Train and Run a Simple Language Model</title>
            <link>https://lukesalamone.github.io/posts/running-simple-language-model/</link>
            <pubDate>Fri, 16 Apr 2021 21:08:53 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/running-simple-language-model/</guid>
            <description>This article will show how to run a simple language model, KenLM. It&amp;rsquo;s not as powerful as transformer-based models like BERT or GPT-3, but depending on what you&amp;rsquo;re trying to accomplish it may be more than enough. This tutorial should take you about 15 minutes, including the time to run the scripts.
Let&amp;rsquo;s work backwards from where we&amp;rsquo;re trying to get to. When you&amp;rsquo;ve finished, you should be able to run the following script:</description>
            <content type="html"><![CDATA[<p>This article will show how to run a simple language model, KenLM. It&rsquo;s not as powerful as transformer-based models like BERT or GPT-3, but depending on what you&rsquo;re trying to accomplish it may be more than enough. This tutorial should take you about 15 minutes, including the time to run the scripts.</p>
<p>Let&rsquo;s work backwards from where we&rsquo;re trying to get to. When you&rsquo;ve finished, you should be able to run the following script:</p>
<pre><code class="language-python">import kenlm

path = 'path/to/model.arpa'
lm = kenlm.LanguageModel(path)

sentence = &quot;I am not superstitious but I am a little stitious&quot;
print(model.score(sentence))
</code></pre>
<p>The first step will be to build KenLM. Then, we will build the ARPA file which KenLM uses to evaluate.</p>
<h2 id="building-kenlm">Building KenLM</h2>
<p>First, clone this repository:</p>
<p><code>git clone git@github.com:kpu/kenlm.git</code></p>
<p>Now we need to build the KenLM toolkit. Run the following to build:</p>
<pre><code>mkdir -p build
cd build
cmake ..
make -j 4
</code></pre>
<p>Now we just need to provide the model with a <code>.arpa</code> ngram language model file. So let&rsquo;s get one.</p>
<h2 id="building-an-ngram-language-model-from-wikitext-2">Building an ngram language model from Wikitext-2</h2>
<p>First, let&rsquo;s clone a repository which will build an ARPA file. This repository builds the ngram file from <a href="https://paperswithcode.com/dataset/wikitext-2">Wikitext-2</a>, a common dataset used in natural language processing.</p>
<pre><code>git clone git@github.com:daandouwe/ngram-lm.git
cd ngram-lm
mkdir data
./get-data.sh
mkdir arpa
./main.py --order 3 --interpolate --save-arpa --name wiki-interpolate
</code></pre>
<p>Once that has finished, you&rsquo;ll have new <code>.arpa</code> in the <code>arpa</code> directory you created. This script took the longest to run on my machine. Be patient, your computer is busy reading all of Wikipedia.</p>
<h2 id="all-together-now">All Together Now</h2>
<p>Now we&rsquo;re finally ready to evaluate a sentence with the language model.</p>
<pre><code class="language-python">import kenlm

path = 'path/to/model.arpa'
lm = kenlm.LanguageModel(path)

sentence = &quot;I am not superstitious but I am a little stitious&quot;
print(model.score(sentence))
</code></pre>
<p>Which prints something like</p>
<pre><code>----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
****************************************************************************************************
-24.47921371459961
</code></pre>
<p>Now if you&rsquo;re interested in a bit more information about what&rsquo;s going on, you can add this bit at the bottom:</p>
<pre><code class="language-python">words = ['&lt;s&gt;'] + sentence.split() + ['&lt;/s&gt;']
for i, (prob, length, oov) in enumerate(lm.full_scores(sentence)):
  print(f'{prob} {length}: {&quot; &quot;.join(words[i+2-length:i+2])}')
  if oov:
    print(f'\t&quot;{words[i+1]}&quot; is an OOV')

for w in words:
  if not w in lm:
    print(f'&quot;{w}&quot; is an OOV')
</code></pre>
<p>Which adds this to your output:</p>
<pre><code>-3.1138248443603516 2: &lt;s&gt; I
-1.1560251712799072 3: &lt;s&gt; I am
-1.1645264625549316 3: I am not
-4.912360191345215 1: superstitious
-4.504511833190918 1: but
-2.2214112281799316 2: but I
-1.1531075239181519 3: but I am
-1.2614283561706543 3: I am a
-0.9001830816268921 3: am a little
-1.2325057983398438 3: a little stitious
	&quot;stitious&quot; is an OOV
-2.8593297004699707 2: stitious &lt;/s&gt;
&quot;stitious&quot; is an OOV
</code></pre>
<p>To the left of each term is the log (base 10) probability of each term occurring. For the first term, <code>&lt;s&gt; I</code> means start of sentence followed by &ldquo;I&rdquo;, which the model has assigned a log probability of -3.11. That&rsquo;s around 0.00078. You might think it&rsquo;s strange that a sentence beginning with &ldquo;I&rdquo; is so unlikely but we are using Wikitext-2. Wikitext-2 is Wikipedia articles. Not a lot of sentences on Wikipedia begin with &ldquo;I&rdquo;.</p>
<p>Notice that &ldquo;stitious&rdquo; is an OOV (out of vocabulary) term here. Clearly the language model doesn&rsquo;t appreciate humor. We&rsquo;ll have to tackle that next time.</p>
]]></content>
        </item>
        
        <item>
            <title>What is Temperature in NLP?üê≠</title>
            <link>https://lukesalamone.github.io/posts/what-is-temperature/</link>
            <pubDate>Fri, 02 Apr 2021 00:50:38 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/what-is-temperature/</guid>
            <description>Temperature is a parameter used in natural language processing models to increase or decrease the &amp;ldquo;confidence&amp;rdquo; a model has in its most likely response.
In my opinion, the most intuitive way of understanding how temperature affects model outputs is to play with it yourself. If you&amp;rsquo;re interested in the mathematical details, I&amp;rsquo;ve included them below, but I won&amp;rsquo;t be offended if you just want to play around with the slider üòÉ .</description>
            <content type="html"><![CDATA[<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      },
      extensions: ["AMSmath.js", "AMSsymbols.js"]
    }
  }
});
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<p>Temperature is a parameter used in natural language processing models to increase or decrease the &ldquo;confidence&rdquo; a model has in its most likely response.</p>
<p>In my opinion, the most intuitive way of understanding how temperature affects model outputs is to play with it yourself. If you&rsquo;re interested in the mathematical details, I&rsquo;ve included them below, but I won&rsquo;t be offended if you just want to play around with the slider üòÉ .</p>
<script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.7.2/Chart.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
<script src="/js/temperature-demo.js"></script>
<p><canvas id="myChart" style="background-color: #0000"></canvas>
<span>Temperature (Œ∏):
<input type="range" min="1" max="1000" value="250" class="slider" id="myRange">
<span id="temperature">25.0</span></p>
<h2 id="whats-going-on">What&rsquo;s going on?</h2>
<p>Suppose we have a language model which predicts the last word in the sentence &ldquo;The mouse ate the _____&rdquo;. Given the previous words in the sentence and its prior training, our language model will try to fill in the blank with a reasonable final token. Suppose those raw outputs are as follows:</p>
<table>
<thead>
<tr>
<th>token</th>
<th>logit</th>
</tr>
</thead>
<tbody>
<tr>
<td>cat</td>
<td>3</td>
</tr>
<tr>
<td>cheese</td>
<td>70</td>
</tr>
<tr>
<td>pizza</td>
<td>40</td>
</tr>
<tr>
<td>cookie</td>
<td>65</td>
</tr>
<tr>
<td>fondue</td>
<td>55</td>
</tr>
<tr>
<td>banana</td>
<td>10</td>
</tr>
<tr>
<td>baguette</td>
<td>15</td>
</tr>
<tr>
<td>cake</td>
<td>12</td>
</tr>
</tbody>
</table>
<p>These outputs make sense. A mouse probably eats cheese, but <a href="https://en.wikipedia.org/wiki/If_You_Give_a_Mouse_a_Cookie">mice are also known to eat cookies</a>. A mouse probably wouldn&rsquo;t eat a baguette unless it was <a href="https://imgur.com/a/484AEO3">a French mouse</a>.</p>
<p>Since these are the raw outputs of the model, they won&rsquo;t sum to 100. To normalize these values, we typically use <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>:</p>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<p style="text-align: center; font-size: 1.5em">
$ œÉ(z_i) = {e^{z_i} \over \sum_{j=0}^N e^{z_j}} $
</p>
<p>When modulating with temperature, we introduce an additional temperature variable Œ∏ which affects the softmax distribution. A higher temperature Œ∏ &ldquo;excites&rdquo; previously low probability outputs. A lower temperature Œ∏ lowers the smaller outputs relative to the largest outputs. To accomplish this, we replace each z<sub>i</sub> in the formula above with the quotient z<sub>i</sub>/Œ∏:</p>
<p style="text-align: center; font-size: 1.5em">
$ œÉ(z_i) = {e^{z_i \over Œ∏} \over \sum_{j=0}^N e^{z_j \over Œ∏}} $
</p>
<p>Higher temperatures make the model more &ldquo;creative&rdquo; which can be useful when generating prose, for example. Lower temperatures make the model more &ldquo;confident&rdquo; which can be useful in applications like question answering.</p>
]]></content>
        </item>
        
        <item>
            <title>What is Perplexity?</title>
            <link>https://lukesalamone.github.io/posts/perplexity/</link>
            <pubDate>Thu, 01 Apr 2021 12:14:49 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/perplexity/</guid>
            <description>TLDR: NLP metric ranging from 1 to infinity. Lower is better.
In natural language processing, perplexity is the most common metric used to measure the performance of a language model. To calculate perplexity, we use the following formula:
$ perplexity = e^z $ where
$ z = -{1 \over N} \sum_{i=0}^N ln(P_{n}) $ Typically we use base e when calculating perplexity, but this is not required. Any base will do, so sometimes the formula will use base 2 or base 10, along with logarithms to the corresponding base.</description>
            <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      },
      extensions: ["AMSmath.js", "AMSsymbols.js"]
    }
  }
});
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<p><strong>TLDR: NLP metric ranging from 1 to infinity. Lower is better.</strong></p>
<p>In natural language processing, perplexity is the most common metric used to measure the performance of a language model. To calculate perplexity, we use the following formula:</p>
<p style="text-align: center; font-size: 1.5em">
$ perplexity = e^z $
</p>
<p style="text-align:center">where</p>
<p style="text-align: center; font-size: 1.5em">
$ z = -{1 \over N} \sum_{i=0}^N ln(P_{n}) $
</p>
<p>Typically we use base <code>e</code> when calculating perplexity, but this is not required. Any base will do, so sometimes the formula will use base 2 or base 10, along with logarithms to the corresponding base.</p>
<h2 id="example">Example</h2>
<p>Imagine that we have a language model which generates the following sequence of tokens:</p>
<p><code>&lt;start&gt;</code> <code>jack</code> <code>and</code> <code>jill</code> <code>went</code> <code>up</code> <code>the</code> <code>hill</code></p>
<p>And suppose that the conditional probabilities for each of the tokens are as follows:</p>
<table>
<thead>
<tr>
<th>token</th>
<th>probability</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>&lt;start&gt;</code></td>
<td>15%</td>
</tr>
<tr>
<td><code>jack</code></td>
<td>5%</td>
</tr>
<tr>
<td><code>and</code></td>
<td>12%</td>
</tr>
<tr>
<td><code>jill</code></td>
<td>18%</td>
</tr>
<tr>
<td><code>went</code></td>
<td>25%</td>
</tr>
<tr>
<td><code>up</code></td>
<td>40%</td>
</tr>
<tr>
<td><code>the</code></td>
<td>33%</td>
</tr>
<tr>
<td><code>hill</code></td>
<td>50%</td>
</tr>
</tbody>
</table>
<p>For the purposes of calculating perplexity it doesn&rsquo;t matter how the sequence was generated. It may be using an n-gram model or an LSTM or a transformer. All that matters is the probabilities the model assigns to each of the tokens. To calculate perplexity, we calculate the logarithm of each of the values above:</p>
<table>
<thead>
<tr>
<th>token</th>
<th>P</th>
<th>ln(P)</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>&lt;start&gt;</code></td>
<td>15%</td>
<td>-1.897</td>
</tr>
<tr>
<td><code>jack</code></td>
<td>5%</td>
<td>-2.996</td>
</tr>
<tr>
<td><code>and</code></td>
<td>12%</td>
<td>-2.120</td>
</tr>
<tr>
<td><code>jill</code></td>
<td>18%</td>
<td>-1.715</td>
</tr>
<tr>
<td><code>went</code></td>
<td>25%</td>
<td>-1.386</td>
</tr>
<tr>
<td><code>up</code></td>
<td>40%</td>
<td>-0.916</td>
</tr>
<tr>
<td><code>the</code></td>
<td>33%</td>
<td>-1.109</td>
</tr>
<tr>
<td><code>hill</code></td>
<td>50%</td>
<td>-0.693</td>
</tr>
</tbody>
</table>
<p>Summing the logs, we get -12.832. Since there are 8 tokens, we divide -12.832 by 8 to get -1.604. Negating that allows us to calculate the final perplexity:</p>
<p style="text-align: center; font-size: 1.5em">
$ perplexity = e^{1.604} = 4.973 $
</p>
<p>Therefore the perplexity of this sequence is about 4.973.</p>
]]></content>
        </item>
        
        <item>
            <title>S3 Bucket Url</title>
            <link>https://lukesalamone.github.io/posts/s3-bucket-url/</link>
            <pubDate>Wed, 10 Mar 2021 03:03:53 -0600</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/s3-bucket-url/</guid>
            <description>Assuming your bucket is publicly accessible, the url of your S3 bucket will be
http://[bucket-name].s3-website-[region].amazonaws.com For example for &amp;ldquo;mybucket&amp;rdquo; in &amp;ldquo;us-east-1&amp;rdquo; your url will be
http://mybucket.s3-website-us-east-1.amazonaws.com </description>
            <content type="html"><![CDATA[<p>Assuming your bucket is publicly accessible, the url of your S3 bucket will be</p>
<pre><code>http://[bucket-name].s3-website-[region].amazonaws.com
</code></pre>
<p>For example for &ldquo;mybucket&rdquo; in &ldquo;us-east-1&rdquo; your url will be</p>
<pre><code>http://mybucket.s3-website-us-east-1.amazonaws.com
</code></pre>
]]></content>
        </item>
        
        <item>
            <title>About My Quick Reference Articles</title>
            <link>https://lukesalamone.github.io/posts/why-how-to/</link>
            <pubDate>Sun, 07 Mar 2021 14:44:37 -0600</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/why-how-to/</guid>
            <description>I&amp;rsquo;ve created a few quick-reference articles and it might not be clear why. There are a few reasons:
These articles are mainly a reference for me. I find myself searching the same things over and over, looking for the purple link, scrolling through the article, then copy &amp;amp; pasting code. I&amp;rsquo;d rather not go through the hassle. These articles aim to solve that problem. I aim to keep the answers above the fold.</description>
            <content type="html"><![CDATA[<p>I&rsquo;ve created a few quick-reference articles and it might not be clear why. There are a few reasons:</p>
<ol>
<li>These articles are mainly a reference for me. I find myself searching the same things over and over, looking for the purple link, scrolling through the article, then copy &amp; pasting code. I&rsquo;d rather not go through the hassle. These articles aim to solve that problem.</li>
<li>I aim to keep the answers <a href="https://en.wikipedia.org/wiki/Above_the_fold#In_web_design">above the fold</a>. I don&rsquo;t want to have to scroll down to find the answer. I almost never read the surrounding prose when I am in &ldquo;coding mode&rdquo;.</li>
<li>I don&rsquo;t have ads or popups on my blog. I will never ask people to sign up for a newsletter or login to read more. I also don&rsquo;t use pictures unless there&rsquo;s a good reason. <a href="https://www.google.com/search?q=ai+thinking+robot+stock+photo&tbm=isch">The &ldquo;thinking AI robot stock photo&rdquo; industry is definitely a bubble.</a></li>
<li>Writing these things out explicitly helps me to remember them. Paradoxically, this may make these how-to pages less useful to me, but maybe someone else will find them useful.</li>
</ol>
<p>These quick reference articles don&rsquo;t explain much because I don&rsquo;t need an explanation of what is going on. I just need a chunk of working code. There are other websites which have far more comprehensive guides covering how to do things. They cover all of the fundamentals of how things are done. But I don&rsquo;t need that, I just want a 30 second reference with a working chunk of code.</p>
]]></content>
        </item>
        
        <item>
            <title>Python: Read &amp; Write Json</title>
            <link>https://lukesalamone.github.io/posts/read-write-json/</link>
            <pubDate>Sun, 07 Mar 2021 14:05:27 -0600</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/read-write-json/</guid>
            <description>Often it is useful to save python data to json files. The following code will demonstrate how that can be done.
&amp;ldquo;God bless JSON!&amp;rdquo; ~ a soon to be famous programmer
import json data = {&#39;a&#39;: 1, &#39;b&#39;:&#39;hello&#39;, &#39;c&#39;:False} filename = &#39;awesome_data.json&#39; # write data to file with open(filename, &#39;w&#39;) as f: json.dump(data, f) # read json from file with open(filename, &#39;r&#39;) as f: data = json.load(f) print(data) # prints {&#39;a&#39;: 1, &#39;b&#39;:&#39;hello&#39;, &#39;c&#39;:False} </description>
            <content type="html"><![CDATA[<p>Often it is useful to save python data to json files. The following code will demonstrate how that can be done.</p>
<blockquote>
<p>&ldquo;God bless JSON!&rdquo; ~ a soon to be famous programmer</p>
</blockquote>
<pre><code class="language-python">import json

data = {'a': 1, 'b':'hello', 'c':False}
filename = 'awesome_data.json'

# write data to file
with open(filename, 'w') as f:
  json.dump(data, f)


# read json from file
with open(filename, 'r') as f:
  data = json.load(f)


print(data)
# prints {'a': 1, 'b':'hello', 'c':False}
</code></pre>
]]></content>
        </item>
        
        <item>
            <title>Autoencoding Stock Prices</title>
            <link>https://lukesalamone.github.io/posts/build-an-autoencoder/</link>
            <pubDate>Sun, 07 Mar 2021 01:31:51 -0600</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/build-an-autoencoder/</guid>
            <description>Autoencoding stock prices as found in Heaton et al., 2016
So you want to build an autoencoder? Great! This article will demonstrate how to build an autoencoder and use it to measure stock prices against an index. This technique is described in more technical terms here.
Once we&amp;rsquo;ve trained the autoencoder, we can use it to measure how well each component follows the other members of the index. This can be useful for finding deeper insights into an index, and doesn&amp;rsquo;t require a priori knowledge of the index price or the weighting of its components.</description>
            <content type="html"><![CDATA[<figure><img src="/img/autoencoder.png"
    alt="Autoencoding stock prices as found in Heaton et al., 2016"><figcaption>
      <p>Autoencoding stock prices as found in Heaton et al., 2016</p>
    </figcaption>
</figure>

<p>So you want to build an autoencoder? Great! This article will demonstrate how to build an autoencoder and use it to measure stock prices against an index. This technique is described in more technical terms <a href="https://arxiv.org/pdf/1602.06561.pdf">here</a>.</p>
<p>Once we&rsquo;ve trained the autoencoder, we can use it to measure how well each component follows the other members of the index. This can be useful for finding deeper insights into an index, and doesn&rsquo;t require a priori knowledge of the index price or the weighting of its components. Note, this is only one metric which one could use to determine how well one member of the group follows the group overall. Another might be <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson Correlation</a>.</p>
<h2 id="github-repository">Github repository</h2>
<p>To follow along with the code in this tutorial, please download the <a href="https://github.com/lukesalamone/stock-data-autoencoder">corresponding repository on Github</a>:</p>
<pre><code>git clone git@github.com:lukesalamone/stock-data-autoencoder.git
cd stock-data-autoencoder
pip install -r requirements.txt
</code></pre>
<h1 id="what-is-an-autoencoder">What is an autoencoder?</h1>
<p>An autoencoder is a neural network which encodes information back to itself. The structure of the network is such that the input layers (the &ldquo;encoder&rdquo;) will be large compared to the hidden layers (the &ldquo;code&rdquo;), forcing the network to compress information inside its hidden layers.</p>
<p>The idea of our autoencoder is that we would like to encode stock price information back to itself while discarding trends that aren&rsquo;t important. To do this, we will feed our network stock price data and ask the network to return those prices to us as outputs. Component stocks which are important to the index will be preserved well and thus be highly accurate, while components which are less important will not be well-preserved. We will measure the performance of the network on each component using mean squared error.</p>
<h1 id="the-model">The model</h1>
<p>We will use an autoencoder with a number of inputs and outputs equal to the number of component stocks in our index. For this exercise, we will use the <a href="https://en.wikipedia.org/wiki/S%26P_500">S&amp;P 500 index</a> which contains 505 components. This means our input and output size will be 505. We will also use a hidden layer with 5 units.</p>
<pre><code class="language-python">class StonksNet(nn.Module):
 def __init__(self, size):
   super().__init__()
     self.fc1 = nn.Linear(in_features=size, out_features=5)
     self.out = nn.Linear(in_features=5, out_features=size)

 def forward(self, x: Tensor) -&gt; Tensor:
   x = F.relu(self.fc1(x))
   x = F.relu(self.out(x))
   return x
</code></pre>
<h1 id="the-data">The data</h1>
<p>We will use daily stock prices downloaded using <a href="https://pypi.org/project/yfinance/">yfinance</a>. This data is readily available online and I recommend downloading it for yourself. We will use data between January 1, 1991 to January 1, 2021 (30 years of data).</p>
<p>To download the S&amp;P 500 stock data please run <code>gather_stocks.py</code> from the project directory:</p>
<pre><code>python gather_stocks.py
</code></pre>
<p>This will download all 505 components into the <code>stock_data</code> directory. Data will also be cleaned such that each component has the same number of days, which will be important when feeding it into the model.</p>
<h1 id="training-the-model">Training the model</h1>
<p>The model itself is a simple feed-forward neural network. As such, we use a standard training loop to train the model. We don&rsquo;t expect the loss to ever fall to zero during training since it is impossible for the network to perfectly encode and decode so many inputs into so few hidden code units. Some information will inevitably be lost. In my training, validation losses bottomed out at around 4000, but yours may be different depending on the initialization of your autoencoder.</p>
<p><img alt="validation loss" src="/img/autoencoder_validation_losses.png"></p>
<h1 id="ranking-components">Ranking components</h1>
<p>Finally we&rsquo;re ready to rank the components of the S&amp;P 500 for &ldquo;closeness&rdquo;. After running <code>python train_model.py</code> you will see the best and worst components as scored by the autoencoder. Here were my results, yours may be different.</p>
<pre><code>best 5 results:
DRE: 16.66
LNT: 37.27
MU: 38.88
HOLX: 43.18
CERN: 47.46

worst 5 results:
HUM: 105244.19
SHW: 108542.73
LMT: 113654.48
C: 357073.88
NVR: 10955169.00
</code></pre>
<h1 id="future-research">Future research</h1>
<p>Upon inspection, it appears that better results might be achieved if we normalize the stock data before training. It appears that stocks with higher prices and higher volatility tended to perform worse than those with tight price ranges. In a way this is expected, since the autoencoder will naturally have a harder time modeling large values with a limited set of hidden units. However, normalizing the prices into similar ranges might be an interesting exercise to see if we can squeeze even more out of the model.</p>
]]></content>
        </item>
        
        <item>
            <title>Python: Formatting a string</title>
            <link>https://lukesalamone.github.io/posts/python-format-string/</link>
            <pubDate>Wed, 24 Feb 2021 21:22:42 -0600</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/python-format-string/</guid>
            <description>There are three main ways to format strings in python:
name = &#39;Luke&#39; food = &#39;pizza&#39; # old style &amp;quot;My name is %s and I like %s.&amp;quot; % (name, food) # str.format() &amp;quot;My name is {0} and I like {1}.&amp;quot;.format(name, food) # f-strings f&amp;quot;My name is {name} and I like {food}.&amp;quot; </description>
            <content type="html"><![CDATA[<p>There are three main ways to format strings in python:</p>
<pre><code class="language-python">name = 'Luke'
food = 'pizza'

# old style
&quot;My name is %s and I like %s.&quot; % (name, food)

# str.format()
&quot;My name is {0} and I like {1}.&quot;.format(name, food)

# f-strings
f&quot;My name is {name} and I like {food}.&quot;
</code></pre>
]]></content>
        </item>
        
        <item>
            <title>Siamese Neural Networks (Video)</title>
            <link>https://lukesalamone.github.io/posts/siamese-nn-video/</link>
            <pubDate>Thu, 17 Dec 2020 11:22:43 -0600</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/siamese-nn-video/</guid>
            <description>The following is a transcript of the above video
In this paper, the authors present a novel neural network architecture to enable audio search via sounds humans are able to make, for example humming and whistling. This is an important capability when searching through audio for a specific sound.
Motivation Imagine you have hundreds of unlabeled sound effects on your computer, and you are looking for a specific one. It could be very tedious to listen to every single one until you can find the right sound.</description>
            <content type="html"><![CDATA[<div style="text-align:center">
  <iframe src="https://player.vimeo.com/video/491725663" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>
</div>
<p><em>The following is a transcript of the above video</em></p>
<p>In this paper, the authors present a novel neural network architecture to enable audio search via sounds humans are able to make, for example humming and whistling. This is an important capability when searching through audio for a specific sound.</p>
<h2 id="motivation">Motivation</h2>
<p>Imagine you have hundreds of unlabeled sound effects on your computer, and you are looking for a specific one. It could be very tedious to listen to every single one until you can find the right sound. Even if the sounds do have some kind of word labels, it could be hard to pinpoint exactly which words to search for. A lot of sounds don‚Äôt exactly lend themselves to text descriptors, so finding the right sound can be difficult with a text search.</p>
<p>This paper contains three main contributions:</p>
<p>First, it introduces a new neural network architecture for matching imitated sounds with a sound corpus, the semi-siamese convolutional neural network.</p>
<p>Second, the researchers built a second architecture which utilizes transfer learning from other audio tasks in an attempt for better performance.</p>
<p>Third, the researchers visualized and sonified input patterns which excited the neurons in different layers.</p>
<p>Both neural networks outperform the state of the art systems as we will see later on.</p>
<h2 id="data">Data</h2>
<p>To train the siamese model, the authors used a dataset called VocalSketch, which contains sounds from 4 broad categories: acoustic instruments, commercial synthesizers, everyday, and single synthesizer notes. The dataset also contains a number of human vocal imitations of each of the sounds.
For each of the 4 categories in VocalSketch, the researchers selected half of the sounds in each category along with corresponding vocal imitations as the training and validation set, and the other half for their test set.</p>
<p>Since each of the categories other than Everyday contained 40 sounds, 20 sounds would be for training and 20 for test and validation. The everyday category contained 120 sounds, so that category had 60 in training and 60 in test.
Each sound in the dataset has 10 corresponding human vocal imitations. In the training set, 7 of them for each sound were selected for training, and the remaining 3 were used for validation. Overall, the researchers used this dataset to create 840 positive and 840 negative pairs for training, and 360 positive and negative pairs for validation.</p>
<p>I thought it was interesting that the researchers opted not to use balanced categories of 20 sounds for each category. There isn‚Äôt a comment in the paper about the reasoning behind this but it may be due to the difficulty of categorizing this class of sounds.</p>
<p>The transfer learning model required pretraining the two towers on two additional datasets before training the full network on VocalSketch.</p>
<p>For the vocal imitation tower, they used a dataset called VoxForge. From this dataset the researchers selected 8 thousand samples for each of 7 different languages. They used a 70 - 30 split for training and testing, and achieved a 69.8% accuracy, which seems pretty good for a 7 class classification task.</p>
<p>For the second transfer learning tower, the environmental sound classification tower, they used a dataset called UrbanSound8k. This dataset contains 8732 sound samples in 10 different classes, things like car horns, jackhammers, and street music. The researchers used 10 fold cross validation when pretraining on this dataset and achieved a 70.2% accuracy over the dataset.</p>
<p>Note that for pre-training the two towers, the researchers used a slightly modified neural network architecture, appending two fully connected layers to categorize the results into the necessary number of classes.</p>
<p>So how are sounds fed into the neural networks? You‚Äôre probably familiar with the way that convolutional neural networks work with images. Typically the input for each image is of a shape width by height by color depth. Creating an input when working with sound is similar. The width of the audio file in this case is time, and the height is the frequencies during that time step. This is similar to the output of a spectrogram.</p>
<p>In order to be fed into the network, audio must first undergo a preprocessing step. The specific preprocessing involved varied between the networks, but generally involves downsampling the audio and splitting it by frequency band. This resulted in an input which resembles a spectrogram image.</p>
<h2 id="methodology">Methodology</h2>
<p>The heart of this problem is an architecture the researchers dubbed siamese style or semi siamese neural networks. A true siamese neural network consists of two identical neural network towers with the same weights, and is used similar to the way that hashing is used to match similar inputs. A siamese style network is similar to a siamese neural network, but may have different weights in one of its towers.</p>
<p>The first network the researchers called IMINET, which included a true siamese neural network as one of its configurations. This network consisted of two convolutional network towers, a concatenation step, and a fully-connected network with three more layers to compute a similarity score between 0 and 1. The convolutional neural networks in IMINET had the same structure, even though in some configurations their weights were not the same. They consisted of four convolutional layers with pool layers following convolutional layers 1 and 2.</p>
<p>The researchers detailed the parameters they used for each of the convolutional layers, specifically the number of filters and the receptive field size. Each of the filters in a layer learns to detect various characteristics of the input, and the receptive field is the part of the input that the filter is able to see. The max pooling layers output the maximum value among all of their inputs, reducing the number of inputs for the next layer in the network.</p>
<p>The researchers experimented with three different configurations for the convolutional network towers: a tied configuration, where both towers would share the exact same weights and biases; an untied configuration, where the two towers were required to share no weights and biases at all; and a partially tied configuration, where weights and biases were shared for convolutional layers 3 and 4 but not 1 and 2. Because the untied and partially tied configurations are not truly siamese neural networks, the researchers called these configurations semi siamese.</p>
<p>After both inputs pass through the convolutional towers, they are concatenated together into one input vector and fed into a fully connected network with three layers. This network‚Äôs job is to compute a similarity score between 0 and 1 for the two inputs. Both of the first two layers used ReLU activation. The final layer was a single neuron which used sigmoid activation to squash its input into an output between 0 and 1.</p>
<p>This architecture achieved state of the art results in sound retrieval which I will discuss in a moment along with the rest of the findings of this paper.</p>
<p>The second neural network developed by researchers was called TL-IMINET. This network was very similar to the first network, but this time the researchers tried using transfer learning to achieve better performance. The researchers hypothesized that the vocal imitation task shares many characteristics with language identification, and that sound recognition shares many similarities with environmental sound classification. This would allow networks which were pre-trained on these tasks to require only fine tuning to be adapted to this task.</p>
<p>The network architectures for the language identification and environmental classification tasks were slightly different from those used in IMINET and are shown here. Note that the towers are also different in architecture from each other.</p>
<p>The researchers also experimented with fusion strategies between different models. For IMINET, the similarity likelihoods for all three configurations were multiplied together to achieve a combined score. They also experimented with combining IMINET with the previous state of the art model. Since that model computes a cosine distance between the input sound and a candidate sound, this output was converted into a likelihood using softmax, and that softmax was multiplied by the output of IMINET. The transfer learning model TL-IMINET was also combined with the state of the art model in a similar way by computing the softmax and multiplying by the output of TL-IMINET. These fusion strategies ended up improving the performance of each of the models quite a bit.</p>
<h2 id="experiments-and-findings">Experiments and Findings</h2>
<p>To measure the performance of these networks, recall that the output of the networks was a number between 0 and 1 indicating how similar the network believed the two inputs were to each other. For example, two very similar inputs might have a similarity rating of 0.9, while two dissimilar inputs might have a similarity rating of 0.1. After gathering the similarity of the human vocalization sound to each of its potential matches, the matches were ranked according to their similarity score.</p>
<p>The authors then used a metric called mean reciprocal rank, which is a number between 0 and 1 indicating how well the algorithm ranked the sounds. For example, a mean reciprocal rank of 0.5 suggests that the target sound is ranked second among all possible matches on average.</p>
<p>Here are the performances of the various network configurations when measured by mean reciprocal rank. The researchers highlighted several insights which could be drawn from their results.</p>
<p>First, it seems that tied configurations performed the best among all configurations of IMINET. This runs contrary to the researchers‚Äô expectations that untied configurations would outperform tied configurations.</p>
<p>Second, the tied configuration outperformed the previous state of the art benchmark in two categories of sounds: commercial synthesizers and everyday. It performed worse than the state of the art for acoustic instruments, and was about the same performance for the single synthesizer category.</p>
<p>Third, IMINET achieved even better performance in most categories by using an ensemble of different configurations.</p>
<p>Fourth, even without pretraining, the TL-IMINET model performed better than the untied configuration of IMINET for all categories except commercial synthesizers. This is interesting because the only difference between these two models is the network structure of the convolutional towers.</p>
<p>And finally, the pre-trained TL-IMINET model outperformed the previous state of the art model by quite a bit in all categories, but the best performing configuration overall was TL-IMINET fused with the previous state of the art model.</p>
<p>One of the most interesting experiments was the visualization of the input patterns which activate neurons in each of the layers. This was done by performing gradient ascent of the neuron activation with respect to the input from a random initialization state. Visualizing the convolutional layers showed that the first layer tends to learn local features like edges, intermediate layer neurons learn more complex features like texture and direction, and the deepest layer recognizes concentrations of frequency ranges. The visualizations also helped to confirm that pretraining indeed helped the networks to learn more detail. The patterns from the pretrained vocal imitation tower were sharper than those in the naive IMINET towers.</p>
<h2 id="takeaways">Takeaways</h2>
<p>There are a few key takeaways from this research. The first is that the transfer learners had much better performance than the naive non-transfer learner, as evidenced by the fact that TL-IMINET performed better than IMINET for most categories even though neither model was pretrained. The research also showed that ensemble methods can outperform any single model on its own. IMINET performed better when used in combination with its different configurations, and combining it with the state of the art model performed better than either model on its own. Finally, visualizing the inputs can help to confirm that the network is learning the correct things, and helps to provide insights as to what types of sound properties are important.</p>
]]></content>
        </item>
        
        <item>
            <title>Managing Python Environments</title>
            <link>https://lukesalamone.github.io/posts/managing-python-environments/</link>
            <pubDate>Sat, 24 Oct 2020 17:45:41 -0500</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/managing-python-environments/</guid>
            <description>Need to switch between python versions often? Use pyenv.
Installing pyenv # install pyenv curl https://pyenv.run | bash # check pyenv install location which pyenv Install another python version # see a list of available python versions pyenv install --list # check installed python versions pyenv versions # installs python 3.7.5 pyenv install 3.7.5 Switch python versions # use python 3.7.5 everywhere on your machine pyenv global 3.7.5 # use python 3.</description>
            <content type="html"><![CDATA[<p>Need to switch between python versions often? Use <a href="https://github.com/pyenv/pyenv"><code>pyenv</code></a>.</p>
<h3 id="installing-pyenv">Installing pyenv</h3>
<pre><code class="language-bash"># install pyenv
curl https://pyenv.run | bash

# check pyenv install location
which pyenv
</code></pre>
<h3 id="install-another-python-version">Install another python version</h3>
<pre><code class="language-bash"># see a list of available python versions
pyenv install --list

# check installed python versions
pyenv versions

# installs python 3.7.5
pyenv install 3.7.5
</code></pre>
<h3 id="switch-python-versions">Switch python versions</h3>
<pre><code class="language-bash"># use python 3.7.5 everywhere on your machine
pyenv global 3.7.5

# use python 3.7.5 in current directory
pyenv local 3.7.5

# use python 3.7.5 in current shell session
pyenv shell 3.7.5
</code></pre>
]]></content>
        </item>
        
        <item>
            <title>What is the Hardest Hangman Word?</title>
            <link>https://lukesalamone.github.io/posts/hardest-hangman-word/</link>
            <pubDate>Tue, 21 Jul 2020 17:34:05 +0800</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/hardest-hangman-word/</guid>
            <description>It seems like a simple enough question. Which word should you choose so that it takes your opponent the most guesses to discover it? Should you choose a long word to use up your opponent&amp;rsquo;s guesses? Or perhaps a short word with obscure letters? In this document I look into this question. But first, a bit of background.
If you&amp;rsquo;re not familiar with the rules of hangman, it is a guessing game played between two people.</description>
            <content type="html"><![CDATA[<p><img alt="Example hangman game" src="https://i.imgur.com/p33HisS.png"></p>
<p>It seems like a simple enough question. Which word should you choose so that it takes your opponent the most guesses to discover it? Should you choose a long word to use up your opponent&rsquo;s guesses? Or perhaps a short word with obscure letters? In this document I look into this question. But first, a bit of background.</p>
<p>If you&rsquo;re not familiar with the rules of hangman, it is a guessing game played between two people. Player A chooses a secret word, and tells player B the length of the secret word. Player B guesses letters which she thinks might be in the word. If she chooses a correct letter, player A reveals the locations of <em>each instance</em> of the guessed letter. However, if player B guesses an incorrect letter, this counts as a &ldquo;strike&rdquo; against her. After an agreed-upon number of strikes, player B loses.</p>
<h1 id="an-algorithmic-approach">An algorithmic approach</h1>
<p>A few years ago, I created a &ldquo;hangman solver&rdquo; for the popular paper and pencil game. This game assessed each game analytically, to determine a list of possible words given the clues available. The algorithm works as follows: at the beginning of the game, we know the length of the secret word, which narrows our dictionary considerably. Then, for each letter in the alphabet, count the number of words available which contain that letter.</p>
<p>Suppose our dictionary consisted of a random list of 50 four-letter words as follows:</p>
<pre><code class="language-javascript">[&quot;pull&quot;, &quot;dipt&quot;, &quot;dorp&quot;, &quot;poky&quot;, &quot;jism&quot;, &quot;cues&quot;, &quot;hood&quot;, &quot;drag&quot;,
&quot;inky&quot;, &quot;mhos&quot;, &quot;kerf&quot;, &quot;jess&quot;, &quot;mete&quot;, &quot;lues&quot;, &quot;wipe&quot;, &quot;kane&quot;,
&quot;tiro&quot;, &quot;keys&quot;, &quot;jape&quot;, &quot;lime&quot;, &quot;sees&quot;, &quot;sass&quot;, &quot;demo&quot;, &quot;ilia&quot;,
&quot;mink&quot;, &quot;dips&quot;, &quot;hove&quot;, &quot;jees&quot;, &quot;that&quot;, &quot;pops&quot;, &quot;isle&quot;, &quot;teas&quot;,
&quot;dens&quot;, &quot;dogy&quot;, &quot;pink&quot;, &quot;sizy&quot;, &quot;cole&quot;, &quot;pact&quot;, &quot;thaw&quot;, &quot;lead&quot;,
&quot;mile&quot;, &quot;dodo&quot;, &quot;litu&quot;, &quot;scup&quot;, &quot;colt&quot;, &quot;soma&quot;, &quot;seat&quot;, &quot;dewy&quot;,
&quot;pits&quot;, &quot;mojo&quot;]
</code></pre>
<p>This would result in letter counts as follows:</p>
<table>
<thead>
<tr>
<th>letter</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td>16</td>
</tr>
<tr>
<td>b</td>
<td>7</td>
</tr>
<tr>
<td>c</td>
<td>15</td>
</tr>
<tr>
<td>d</td>
<td>5</td>
</tr>
<tr>
<td>e</td>
<td>26</td>
</tr>
<tr>
<td>f</td>
<td>1</td>
</tr>
<tr>
<td>g</td>
<td>5</td>
</tr>
<tr>
<td>h</td>
<td>3</td>
</tr>
<tr>
<td>i</td>
<td>10</td>
</tr>
<tr>
<td>j</td>
<td>1</td>
</tr>
<tr>
<td>k</td>
<td>6</td>
</tr>
<tr>
<td>l</td>
<td>14</td>
</tr>
<tr>
<td>m</td>
<td>6</td>
</tr>
<tr>
<td>n</td>
<td>6</td>
</tr>
<tr>
<td>o</td>
<td>14</td>
</tr>
<tr>
<td>p</td>
<td>7</td>
</tr>
<tr>
<td>r</td>
<td>8</td>
</tr>
<tr>
<td>s</td>
<td>15</td>
</tr>
<tr>
<td>t</td>
<td>13</td>
</tr>
<tr>
<td>u</td>
<td>4</td>
</tr>
<tr>
<td>w</td>
<td>1</td>
</tr>
<tr>
<td>x</td>
<td>1</td>
</tr>
<tr>
<td>y</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>In this case, the letter E is found in 26 words, the most of any letter. Therefore our algorithm should pick E since it is the safest guess. Once we guess a letter correctly, this gives important positional information which can filter the word list even further.</p>
<p>This process is repeated, each time picking the most likely letter, given the constraints we know. If we know some of the letters in the secret word, we can eliminate any words that don&rsquo;t have those letters in those positions. If we have guessed a letter incorrectly, we know that letter isn&rsquo;t in the secret word and can eliminate all words which have that letter. You can see a python implementation of this algorithm <a href="https://gist.github.com/lukesalamone/a815cda5e427b28db78e0caafdbbc0f3#file-hangman_solver-py">here</a>.</p>
<p>For this experiment I used the <a href="http://www.blogmybrain.com/words-with-friends-cheat/words.txt">Zynga dictionary</a>, the same dictionary used in the game Words With Friends. The dictionary contains 173,000 words. It does not include any proper nouns or profanities.</p>
<h1 id="live-experiment">Live experiment</h1>
<p>Below you can experiment with this algorithm among 4 letter words. Enter a secret word, and the steps used to uncover the secret word will be displayed below. (Need inspiration? Try comparing <em>jazz</em> to <em>rock</em> or <em>blue</em> to <em>grey</em>.)</p>
<div id="demo1" style="margin:20px;background:#ddd;padding:20px;font-family:monospace">
    <script src="/js/hangman-demo1.js"></script>
<pre><code>&lt;div style=&quot;width:100%; text-align:center&quot;&gt;
    &lt;input placeholder=&quot;enter 4 letter word&quot;&gt;&lt;/input&gt;
    &lt;button class=&quot;play&quot;&gt;play word&lt;/button&gt;
    &lt;button class=&quot;reset&quot;&gt;reset&lt;/button&gt;
&lt;/div&gt;
&lt;div class=&quot;output&quot; style=&quot;margin: 20px 0&quot;&gt;&lt;/div&gt;
</code></pre>
</div>
<p>Notice how some words have much higher difficulty than others? This is due to the fact that some words have many &ldquo;siblings&rdquo; which differ by only one letter. For example the pattern &ldquo;.ays&rdquo; could match the letters c, d, f, h, j, k, l, n, or r. Knowing the last 3 letters gives us no indication of which of these nine letters it will be.</p>
<p>One apparent shortcoming of the above calculation is that it assumes that letters with equal probability will be picked in alphabetical order, and therefore letters last in the alphabet will be picked last. Although this has the benefit of creating a deterministic algorithm which will always return the same result for the same word, in real life we don&rsquo;t know in which order people will pick letters. In actuality the words <em>days</em>, <em>jays</em> and <em>rays</em> have equal difficulty each is equally likely to be the secret word.</p>
<h1 id="preliminary-results">Preliminary results</h1>
<p>With this preliminary caveat in mind, we can still calculate the difficulty of every word in the dictionary. If we assume that in a situation where multiple letters are equally probable, our opponent will break the tie using alphabetical ordering, which hangman words are the hardest to guess? Here are the top 17:</p>
<table>
<thead>
<tr>
<th>word</th>
<th>difficulty</th>
</tr>
</thead>
<tbody>
<tr>
<td>zill</td>
<td>19</td>
</tr>
<tr>
<td>zills</td>
<td>18</td>
</tr>
<tr>
<td>yill</td>
<td>18</td>
</tr>
<tr>
<td>zin</td>
<td>17</td>
</tr>
<tr>
<td>zax</td>
<td>17</td>
</tr>
<tr>
<td>yills</td>
<td>17</td>
</tr>
<tr>
<td>will</td>
<td>17</td>
</tr>
<tr>
<td>vox</td>
<td>17</td>
</tr>
<tr>
<td>mem</td>
<td>17</td>
</tr>
<tr>
<td>zins</td>
<td>16</td>
</tr>
<tr>
<td>yuck</td>
<td>16</td>
</tr>
<tr>
<td>yin</td>
<td>16</td>
</tr>
<tr>
<td>wills</td>
<td>16</td>
</tr>
<tr>
<td>vill</td>
<td>16</td>
</tr>
<tr>
<td>oak</td>
<td>16</td>
</tr>
<tr>
<td>jazz</td>
<td>16</td>
</tr>
<tr>
<td>foy</td>
<td>16</td>
</tr>
<tr>
<td>(27 more)</td>
<td>15</td>
</tr>
</tbody>
</table>
<h1 id="previous-research">Previous research</h1>
<p>I should note that <a href="https://web.archive.org/web/20100815092214/http://blog.wolfram.com/2010/08/13/25-best-hangman-words/">previous research by Jon McLoone in 2010</a> has explored the same topic, although he used slightly different methodology and a smaller 90,000 word dictionary. His algorithm was not deterministic, and so does not always pick the most frequent letter available. Instead, his algorithm picks a letter with a probability proportional to the frequency with which it occurs in candidate words. For example, if we refer to the letter frequencies of the 50 word 4-letter dictionary above, <em>j</em> appears in just 1 word, while <em>e</em> appears in 26 of them. In this case, since the sum of the numbers in the frequency table is 188, McLoone&rsquo;s algorithm would pick <em>z</em> in 1 out of every 188 first guesses, and <em>e</em> in 26 of them.  Although it might seem that this strategy is not optimal, it does avoid the deterministic results shown above.</p>
<p>Additionally, McLoone chose to remain faithful to the logic of the hangman game, opting to end the games after a given number of mistakes, and recording the probability that a given word was not discovered after the game ended. So an 8-game means that the game was ended after 8 mistakes, and a 13-game after 13 mistakes. Using this methodology, he found the hardest hangman words were as follows:</p>
<p><img alt="Jon McLoone results" src="https://i.imgur.com/N5ginNw.gif"></p>
<h1 id="future-research">Future research</h1>
<p>Now, I think that we can improve upon our results a bit. Rather than calculating difficulty deterministically, we can instead randomize the ordering which letters will be picked in. This should dramatically reduce some of the outliers from above, bringing &ldquo;rays&rdquo; down from a difficulty of 14 to something more reasonable.</p>
<p>Such a stochastic calculation will require simulating millions of games. For my 173,000 word dictionary, simply simulating 10 games would involve 1.7 million games. Fortunately, this operation is highly parallelizable. It should be possible to split the dictionary into 100 or even 1000 pieces and derive the results for each piece simultaneously.</p>
<p>A simulation of this algorithm is shown below, with a graph of the average number of mistakes the new randomized algorithm incurs before discovering your secret word. The simulation is set up to play 500 rounds of games, and the final average is displayed at the top.</p>
<div id="demo2" style="margin:20px;background:#ddd;padding:20px;font-family:monospace">
    <script src="/js/chart.min.js"></script>
    <script src="/js/hangman-demo2.js"></script>
<pre><code>&lt;div style=&quot;width:100%; text-align:center&quot;&gt;
    &lt;input placeholder=&quot;enter 4 letter word&quot;&gt;&lt;/input&gt;
    &lt;button class=&quot;play&quot;&gt;play word&lt;/button&gt;
    &lt;button class=&quot;reset&quot;&gt;reset&lt;/button&gt;
&lt;/div&gt;
&lt;div class=&quot;messages&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;canvas-holder&quot; style=&quot;display:none;width:500px;height:500px&quot;&gt;
    &lt;canvas&gt;&lt;/canvas&gt;
&lt;/div&gt;
</code></pre>
</div>
]]></content>
        </item>
        
        <item>
            <title>Estimating Pi with a Monte Carlo Simulation</title>
            <link>https://lukesalamone.github.io/posts/monte-carlo/</link>
            <pubDate>Thu, 09 Jul 2020 15:40:14 +0800</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/monte-carlo/</guid>
            <description>A Monte Carlo simulation is a method of estimating events or quantities which are difficult or computationally infeasible to derive a closed-form solution to. The value of the mathematical constant Pi is a good example of this: although it is possible to calculate the exact value of Pi, a good estimate is easily demonstrated with just a few lines of code.
Monte Carlo simulations work when the input can be drawn from a random probability distribution, and the outcome can be derived deterministically from the input.</description>
            <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<p>A Monte Carlo simulation is a method of estimating events or quantities which are difficult or computationally infeasible to derive a closed-form solution to. The value of the mathematical constant Pi is a good example of this: although it is possible to calculate the exact value of Pi, a good estimate is easily demonstrated with just a few lines of code.</p>
<p>Monte Carlo simulations work when the input can be drawn from a random probability distribution, and the outcome can be derived deterministically from the input. In the case of calculating Pi, this can be modeled geometrically. The <em>random distribution</em> is all points within the square, and the outcome is whether a selected point lies within the circle inside of the square. We know that for a square circumscribed about a circle,</p>
<p><code>$ A_{circle} = \pi r ^ 2, A_{square} = (2r) ^ 2 = 4r ^ 2 $</code></p>
<p><code>$ { A_{circle}  \over  A_{square} } = { \pi r^2  \over 4r ^ 2 } = { \pi \over 4 } $</code></p>
<p><code>$ { 4 ({ A_{circle}  \over  A_{square} }) } = \pi $</code></p>
<p>If we notice that the probability that a randomly placed dot will fall within the circle is the same as the ratio of their areas (i.e. the circle takes up about 78% of the area of the square, so a random dot has about a 78% chance of landing inside the circle), then multiplying that probability by 4 gives Pi. By placing dots randomly, we play out that probability in real-time. As to whether a given dot lies within the circle, we simply use the Pythagorean theorum to calculate its distance from the origin:</p>
<p><code>$ \sqrt{ x^2 + y^2 } &lt; 1 $</code></p>
<p>So all dots greater than 1 unit from the origin are outside the circle.</p>
<p>Below is a simulation of the derivation of the value of Pi. Click &ldquo;start simulation&rdquo; to see for yourself. <strong>Warning:</strong> this simulation may become slow once many dots are drawn on the screen.</p>
<div id="sim1" style="font-family: monospace; background-color: #444; margin: 20px 0">
    <script src="/js/chart.min.js" type="text/javascript"></script>
    <script src="/js/monte-carlo.js" type="text/javascript"></script>
<pre><code>&lt;div style=&quot;padding: 20px; text-align: center&quot;&gt;
    &lt;button class=&quot;start&quot;&gt;start simulation&lt;/button&gt;
    &lt;button class=&quot;stop&quot; disabled&gt;stop simulation&lt;/button&gt;
&lt;/div&gt;
&lt;div style=&quot;width:500px; height:500px&quot;&gt;
    &lt;canvas&gt;&lt;/canvas&gt;
&lt;/div&gt;
&lt;div style=&quot;color: #fff; margin-left: 20px;&quot;&gt;
    &lt;p style=&quot;margin:0&quot; id=&quot;num-points&quot;&gt;Number of points: 0&lt;/p&gt;
    &lt;p style=&quot;margin:0&quot; id=&quot;pct-inside&quot;&gt;% inside circle: 0&lt;/p&gt;
    &lt;p style=&quot;margin:0&quot; id=&quot;pi&quot;&gt;Approximate value of pi: 0&lt;/p&gt;
&lt;/div&gt;
</code></pre>
</div>
<p>Next, I was interested in the way that this simulation would play out once the number of points became large. Intuitively, this should approach Pi, but doing so requires that the random numbers generated by browsers be evenly distributed.</p>
<div id="sim2" style="font-family: monospace; background-color: #444; margin: 20px 0">
    <div style="padding: 20px; text-align: center">
        <button class="start">start simulation</button>
        <button class="stop" disabled>stop simulation</button>
    </div>
    <div style="width:600px; height:700px">
        <canvas></canvas>
    </div>
    <div style="color: #fff; margin-left: 20px;">
        <p style="margin:0" class="num-points">Number of points: 0</p>
        <p style="margin:0" class="pi">Approximate value of pi: 0</p>
        <p style="margin:0" class="pct-error">% error: 0</p>
    </div>
</div>
]]></content>
        </item>
        
        <item>
            <title>Creating an AI for Gomoku</title>
            <link>https://lukesalamone.github.io/posts/gomoku2049/</link>
            <pubDate>Tue, 19 May 2020 14:28:57 +0800</pubDate>
            
            <guid>https://lukesalamone.github.io/posts/gomoku2049/</guid>
            <description>Gomoku is a strategy game similar to tic tac toe, but played on a larger board and with the goal of getting 5 in a row rather than 3. Since the game has perfect information and has simple rules, I thought it would be a fun exercise in creating a game AI. In February 2020 I decided to code up Gomoku2049. The game is a demonstration of MiniMax, which is an algorithm for finding the move which minimizes the opponent‚Äôs best moves.</description>
            <content type="html"><![CDATA[<p>Gomoku is a strategy game similar to tic tac toe, but played on a larger board and with the goal of getting 5 in a row rather than 3. Since the game has perfect information and has simple rules, I thought it would be a fun exercise in creating a game AI.
In February 2020 I decided to code up Gomoku2049. The game is a demonstration of MiniMax, which is an algorithm for finding the move which minimizes the opponent‚Äôs best moves. This article is an overview of the game‚Äôs technical highlights.</p>
<p><a href="http://gomoku2049.s3-website-us-east-1.amazonaws.com/">Click here to try out the game!</a></p>
<h2 id="minimax-with-alpha-beta-pruning">Minimax with alpha-beta pruning</h2>
<figure><img src="/img/game-tree.png"
    alt="In the tree above, the current game is shown on the left, green to move. If green fails to block orange‚Äôs 3 in a row now, orange cannot be stopped."><figcaption>
      <p>In the tree above, the current game is shown on the left, green to move. If green fails to block orange‚Äôs 3 in a row now, orange cannot be stopped.</p>
    </figcaption>
</figure>

<p>The Minimax algorithm represents every game as a tree of moves, with the current game position at the root of the tree. The algorithm is recursive with exponential time complexity and can have a very high branching factor: after the first move there are 225‚Äì1=224 possible moves. Because it is not feasible to evaluate all possible games to completion, Minimax calculation is usually limited to a fixed depth, after which the algorithm evaluates terminal leaf nodes using the gameover function and the static evaluator.</p>
<p>After each human move (known as ‚Äúplies‚Äù), Minimax assigns a score to each of the possible reply moves. By convention, the AI will score favorable moves with a positive score, and unfavorable moves with a negative score. The move corresponding to the highest score is then selected. In other words, the AI is called the ‚Äúmaximizer‚Äù. Likewise, the human is known as the minimizer. To determine the score of each possible move, the minimax algorithm will recursively either maximize or minimize the possible moves available. After a given depth, the evaluation will stop, and return either an infinite value (+‚àû for an AI win, -‚àû for human win) or a finite evaluation of the state of the board. This static evaluation can be rather expensive, but luckily even a rough approximation is effective.</p>
<p>In practice, in addition to a depth limitation, this minimax algorithm also reduces the branching factor by limiting the squares which will be evaluated to those which are adjacent to squares which have been played. Given the fact that a disconnected ‚Äúisland‚Äù square cannot immediately lead to a win, this seems to be a reasonable simplification.
At the leaf nodes of the tree, either the game is over (the human has won or the computer has won) or the board needs to be evaluated with regards to who is winning.</p>
<h2 id="alpha-beta-pruning">Alpha-beta pruning</h2>
<p>Alpha-beta pruning is an improvement on the minimax algorithm, reducing the number of branches and leaf nodes which need to be evaluated. This is achieved by ‚Äúpruning‚Äù unnecessary branches, ignoring them because the parent minimizer/maximizer would never choose it. For a maximizer (whose parent is a minimizer), this will occur if the parent minimizer has already seen a lower evaluation than a number the maximizing child sees.</p>
<h2 id="static-evaluator">Static evaluator</h2>
<p>This function is used to evaluate a board position with regards to which player is winning, and by how much. The MiniMax algorithm will then choose the highest value for itself, while minimizing the options for its opponent. For gomoku, it was important to derive an evaluation function which could be calculated quickly, and which builds towards the final desired result of 5 squares in a row. Note that such a function would necessarily be isomorphic in four directions: vertical, horizontal, and on both diagonals.</p>
<p>My initial thought was that this would be extremely computationally expensive. There are many permutations of selected squares which can lead to a win, and many which do not. For example, XX ‚Äî ‚Äî OOO ‚Äî ‚Äî XX with O to move will lead to a win for O, but with X to move will not. However, I convinced myself that any static evaluation which built towards a win would find winning nodes at sufficient depth, so finding extremely detailed evaluation was less important than a general approximation.</p>
<p>Building from this thought, I decided to count the number of 4-in-a-rows (4s) and give them a high score, along with the 3s and 2s. Each in-a-row would be given an exponentially increasing ‚Äúreward‚Äù, so that 4s scores much higher than 2√ó2s. For example, the payout function might be f(n) = 2^N for 2, 3, and 4 so that f(4) = 16 and 2√óf(2) = 8. This ensures the desired result, that the optimal configuration of N squares is Ns.</p>
<p>Eventually, I determined that it was sufficient to simply count 2s with overlaps, since allowing double counts would still favor longer sequences of squares, but would not require separate checks for each length. Therefore, if 2s was rewarded 1, then XXX would be rewarded 2, and XXXX would be rewarded 3. This means that 4s is still more the most efficient configuration of four squares, since XX ‚Äî XX only evaluates to 2.</p>
<h2 id="gameover-function">Gameover function</h2>
<p>This function simply needs to return true if the game is over and a player has won. After the simplifications to the static evaluator, the gameover function behaves almost identically. Instead of counting 2s, we check for the presence of 5s.</p>
<h2 id="bitmasks">Bitmasks</h2>
<p>Here is where the fun begins. I realized that a very efficient way of representing a game board was with a sequence of bits, where 1 represented an occupied square, and 0 represented an unoccupied square. A game state would therefore only require a bit sequence for each player (the game engine would prevent overlapping bits). For a 15√ó15=225 square board, each player‚Äôs occupied squares could be represented with a number 225 bits in length. Although Javascript Numbers are only 53 bits long, Javascript has a newer primitive, BigInt, which can store numbers of arbitrary length.</p>
<p>The biggest benefit of representing the game board this way is that it facilitates bitwise operations, which drastically reduces the time complexity for the static evaluator and gameover functions.</p>
<figure><img src="/img/bitmask.gif"
    alt="Here the mask is shown in white, and the actual squares occupied are shown in orange. With each step in the bitmask check, the board and the mask are bitwise ANDed together, a very fast operation which reduces the computational complexity required in the static evaluator and gameover function."><figcaption>
      <p>Here the mask is shown in white, and the actual squares occupied are shown in orange. With each step in the bitmask check, the board and the mask are bitwise ANDed together, a very fast operation which reduces the computational complexity required in the static evaluator and gameover function.</p>
    </figcaption>
</figure>

<h2 id="about-bigint">About BigInt</h2>
<p>The BigInt primitive is a <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/BigInt">newer built-in type</a> in Javascript, and as such is unsupported in some browsers. In particular, Internet Explorer and Safari do not have BigInt as a primitive. Although there are polyfills available for BigInt, they do not have the same performance as the native type. I decided that as a demonstration of the Minimax algorithm, supporting all browsers was not a priority.</p>
<h2 id="web-workers">Web Workers</h2>
<p>Most people know that Javascript is single threaded. It is, except when it isn‚Äôt. Web Workers are a way of multithreading in the browser, which in this context is pretty important because it helps to avoid freezing the user interface. In this game, the board state is handed off to a Web Worker thread, which computes the best move and returns it to the main thread. Progress is reported back periodically to the main thread as well, which is shown in a progress bar underneath the Gomoku2049 logo.</p>
<p>Theoretically, I could have taken further advantage of multithreading when creating this game. Each branch in the decision tree can be parallelized, allowing for simultaneous computation of each node‚Äôs value. For example, a new thread could be used to evaluate each of the AI‚Äôs possible moves. Unfortunately, the number of possible moves for the AI can be quite high later in the game, and browsers limit the number of Web Workers allowed (Chrome allows 60, Firefox allows 20, etc.) so instead of spawning a new worker for each top level branch, threads would need to be spawned from a shared thread pool.</p>
<p><a href="https://github.com/lukesalamone/gomoku-2049">The full source code for this game can be found here.</a></p>
]]></content>
        </item>
        
    </channel>
</rss>
