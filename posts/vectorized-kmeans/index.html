<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="keywords" content=", machine-learning, numpy, performance" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://lukesalamone.github.io/posts/vectorized-kmeans/" />

<meta property="og:title" content=""/>
<meta property="og:image" content=""/>
<meta property="og:description" content=""/>

<script>
    if(location.hostname !== 'localhost') {
        
            fetch('https://d1tkdcmshl91xi.cloudfront.net?p=' + window.location.href.match(/\/([^/]+)\/$/)[1]);
        
    }
</script>


    <title>
        
            Vectorized K-Means Clustering :: Luke Salamone&#39;s Blog 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="../../main.664d1df93a1a25c7c8d09147fb0b5e685b4ebe309ab122277733d326beea3022.css">







  <meta itemprop="name" content="Vectorized K-Means Clustering">
  <meta itemprop="description" content="K-means clustering (previous discussion) is an unsupervised learning algorithm which assigns points to one of K different clusters based on the distance of that point to a centroid. You can see a demo (in two dimensions) below. Centroids are colored white.
start General algorithm The basic K-means algorithm is fairly simple and has two steps, repeated until convergence (i.e. when no points change cluster):
assign points to the cluster corresponding to closest centroid update the centroid locations to the mean of all points assigned to the associated cluster import numpy as np &#39;&#39;&#39; General setup: we will demo 5000 points, each with two dimensions.">
  <meta itemprop="datePublished" content="2024-02-04T23:39:29-07:00">
  <meta itemprop="dateModified" content="2024-02-04T23:39:29-07:00">
  <meta itemprop="wordCount" content="1064">
  <meta itemprop="image" content="https://lukesalamone.github.io/">
  <meta itemprop="keywords" content="Machine-Learning,Numpy,Performance">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://lukesalamone.github.io/">
  <meta name="twitter:title" content="Vectorized K-Means Clustering">
  <meta name="twitter:description" content="K-means clustering (previous discussion) is an unsupervised learning algorithm which assigns points to one of K different clusters based on the distance of that point to a centroid. You can see a demo (in two dimensions) below. Centroids are colored white.
start General algorithm The basic K-means algorithm is fairly simple and has two steps, repeated until convergence (i.e. when no points change cluster):
assign points to the cluster corresponding to closest centroid update the centroid locations to the mean of all points assigned to the associated cluster import numpy as np &#39;&#39;&#39; General setup: we will demo 5000 points, each with two dimensions.">



    <meta property="og:url" content="https://lukesalamone.github.io/posts/vectorized-kmeans/">
  <meta property="og:site_name" content="Luke Salamone&#39;s Blog">
  <meta property="og:title" content="Vectorized K-Means Clustering">
  <meta property="og:description" content="K-means clustering (previous discussion) is an unsupervised learning algorithm which assigns points to one of K different clusters based on the distance of that point to a centroid. You can see a demo (in two dimensions) below. Centroids are colored white.
start General algorithm The basic K-means algorithm is fairly simple and has two steps, repeated until convergence (i.e. when no points change cluster):
assign points to the cluster corresponding to closest centroid update the centroid locations to the mean of all points assigned to the associated cluster import numpy as np &#39;&#39;&#39; General setup: we will demo 5000 points, each with two dimensions.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-02-04T23:39:29-07:00">
    <meta property="article:modified_time" content="2024-02-04T23:39:29-07:00">
    <meta property="article:tag" content="Machine-Learning">
    <meta property="article:tag" content="Numpy">
    <meta property="article:tag" content="Performance">
    <meta property="og:image" content="https://lukesalamone.github.io/">






    <meta property="article:published_time" content="2024-02-04 23:39:29 -0700 -0700" />








    </head>

    <body class="dark-theme">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="../../" style="text-decoration: none;">
  <div class="logo">
    <div class="logo-holder-dark">
      <img src="../../img/logo.png" alt="Blog Home" />
    </div>
    <div class="logo-holder-light">
      <img src="../../img/logo2.png" alt="Blog Home" />
    </div>
  </div>
</a>


        <span class="header__right">
            
            <div id="header_search">
              <input type="text" placeholder="Search here" />
            </div>
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        5 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="https://lukesalamone.github.io/posts/vectorized-kmeans/">Vectorized K-Means Clustering</a>
      </h1>

      

      <div class="post-content">
        <p>K-means clustering (<a href="../../posts/kmeans-clustering/">previous discussion</a>) is an unsupervised learning algorithm which assigns points to one of K different clusters based on the distance of that point to a centroid. You can see a demo (in two dimensions) below. Centroids are colored white.</p>
<script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.3/dist/chart.umd.min.js"></script>
<script src="../../js/kmeans_demo.js"></script>
<div id='demo'>
<button>start</button>
<canvas id="myChart" style="background-color: #0000"></canvas>
</div>
<h2 id="general-algorithm">General algorithm</h2>
<p>The basic K-means algorithm is fairly simple and has two steps, repeated until convergence (i.e. when no points change cluster):</p>
<ol>
<li>assign points to the cluster corresponding to closest centroid</li>
<li>update the centroid locations to the mean of all points assigned to the associated cluster</li>
</ol>
<pre><code class="language-python">import numpy as np

'''
General setup: we will demo 5000 points, each with two 
dimensions. They will be clustered into 5 clusters.
'''

N, k, d = 5000, 5, 2
points = np.random.random((N,d))
centroids = np.random.random((k,d))

while True:

  # assign each point to closest cluster
  clusters = assign_clusters(points, centroids)

  # recalculate centroids
  new_centroids = update_centroids(points, clusters, k)

  # if centroids haven't changed, we're done
  if centroids_equal(new_centroids, centroids):
    break

  # otherwise, update centroids
  centroids = new_centroids
</code></pre>
<h2 id="vectorizing-the-algorithm">Vectorizing the algorithm</h2>
<p>Now that we&rsquo;ve established the general algorithm, we might consider how it can be vectorized. In essence, this means we&rsquo;ll avoid writing for-loops in python. In numpy, this means cython will handle for-loops. For environments with a GPU, we can use PyTorch, JAX or Tensorflow to take advantage of parallel processing that GPUs excel at.</p>
<h3 id="how-broadcasting-works">How broadcasting works</h3>
<p>Before going further it would be good to pause and introduce broadcasting. The <a href="https://numpy.org/devdocs/user/basics.broadcasting.html">numpy guide</a> is much more comprehensive, but the gist is that because numpy needs tensors to have the same shape before performing certain arithmetic operations on them, numpy will automatically try to align their shapes. Importantly, this is done without making copies of the data. The steps for broadcasting are as follows:</p>
<ol start="0">
<li>A scalar will simply take the shape of the vector it&rsquo;s being broadcasted to.</li>
<li>If one vector has fewer dimensions, left-pad its shape with 1s.</li>
<li>Starting from the right, check dimension compatibility. If they are equal or one of them is 1, the dimensions are compatible.</li>
</ol>
<p>For example,</p>
<pre><code class="language-python"># scalars are the simplest
(np.ones((2,2)) * 3).shape # (2,2)

# (3,) is left padded to (1,3)
(np.ones((3)) * np.ones((2,3))).shape # (2,3)

# if dimensions equal or one of them is 1, they are compatible
(np.ones((2,3,1)) * np.ones((2,1,4))).shape # (2,3,4)

# error! [1,1] * [1,1,1] doesn't work
(np.ones((2)) * np.ones(3)) # incompatible shapes!
</code></pre>
<p>For more information, check out the Numpy guide: <a href="https://numpy.org/devdocs/user/basics.broadcasting.html">https://numpy.org/devdocs/user/basics.broadcasting.html</a>.</p>
<h3 id="assign_clusters">assign_clusters()</h3>
<p>Ok, back to K-means. Let&rsquo;s start with <code>assign_clusters()</code>. A point is assigned to the cluster corresponding to the centroid which is closest.</p>
<p>The general idea is that for each point we&rsquo;d like to know its distance to each centroid. We can then take the argmin of those distances to identify the correct cluster and return a result with shape <code>(N,)</code> (N = # of points). Distance will be computed using euclidean distance i.e. subtract, square, and sum.</p>
<p>First we expand the dimensions of the points from <code>(N,d)</code> to <code>(N,1,d)</code>. We also expand the dimensions of the centroids to <code>(1,k,d)</code>. (This happens automatically via broadcasting.) When we take the difference, we are left with a vector with shape <code>(N,k,d)</code>. For each point in <code>N</code>, this yields a vector with <code>k</code> rows and <code>d</code> columns.</p>
<p>Now that we have the differences, we square them, and take the sum across the last dimension. This yields a vector with shape <code>(N,k)</code>.</p>
<p>For actual euclidean distance we would then take the square root of the result, but since we just care about the relative distances we can skip this step.</p>
<p>Finally, we just take the <code>argmin</code> of the last dimension, i.e. for each row, the index of the smallest distance. Now we have our <code>(N,)</code> result.</p>
<pre><code class="language-python">def assign_clusters(points, centroids):
  # (5000,2) -&gt; (5000,1,2)
  points = np.expand_dims(points, 1)

  # (5000,1,2) - (5,2)
  # with broadcasting becomes (5000,5,2)
  diff = points - centroids

  diff = diff ** 2

  # (5000,5,2) -&gt; (5000,5)
  diff = diff.sum(-1)

  # (5000,5) -&gt; (5000,)
  diff = diff.argmin(-1)

  return diff
</code></pre>
<h3 id="update_centroids">update_centroids()</h3>
<p>To update the centroids, we simply take the mean of the points within that cluster. To do this, we will create a vector with shape <code>(N,k,d)</code> again, and mask out  points not in the cluster.</p>
<p>We start by creating an <code>N x k</code> mask, where 1s correspond to the n-th point being in the k-th cluster. We will also temporarily add a dimension at the end to make this <code>N x k x 1</code>.</p>
<p>Adding a dimension to the points vector for proper broadcasting, we can then create the <code>N x k x d</code> vector we planned above. Summing this across the first dimension yields a <code>k x d</code> vector, representing the sums of all of the points within each of the <code>k</code> clusters.</p>
<p>Now we just need to divide by the number of points in each cluster, which we can get by summing the <code>N x k</code> mask we created earlier across the first dimension, yielding <code>k</code> counts.</p>
<p>At this point, we can divide the sum by the counts to get the means.</p>
<pre><code class="language-python">def update_centroids(points, clusters, k):
    N = clusters.shape[0]

    # create an N x k mask, where each row contains a 1
    # in the position corresponding to its cluster
    mask = np.zeros((N, k))
    mask[np.arange(N), clusters] = 1

    # add a dimension in the middle so that 
    # multiplication will broadcast
    # (5000,2) -&gt; (5000,1,2)
    points = np.expand_dims(points, 1)

    # (5000,5,1) * (5000,1,2) results in a (5000,5,2) vector
    # for each of the 5000 points, we have a 5x2 vector where only
    # one of the five rows will have non-zero values
    masked_points = np.expand_dims(mask, -1) * points

    # sum across points
    # (5000, 5, 2) -&gt; (5, 2)
    sums = masked_points.sum(0)

    # sum across points
    counts = mask.sum(0)

    # avoid divide by zero
    counts[counts == 0] = 1

    # divide sum by count to get means
    return sums / np.expand_dims(counts, -1)
</code></pre>
<h3 id="centroids_equal">centroids_equal()</h3>
<p>As a last implementation, we need to check whether the centroids have changed. For this we can just call <code>np.array_equal()</code> on the old and new centroids vectors.</p>
<pre><code class="language-python">def centroids_equal(vec1, vec2):
  return np.array_equal(vec1, vec2)
</code></pre>
<h2 id="comparing-vectorization-vs-loops">Comparing vectorization vs loops</h2>
<p>To compare the vectorized implementation with the non-vectorized implementation, I ran each 100 times and took the mean clock time to converge. Note that because the points are random, K-means may take different numbers of iterations to converge.</p>
<p>The vectorized implementation (green) runs in significantly less time, between 7 and 21 times faster.</p>
<p><canvas id="groupedBarChart" width="800" height="400"></canvas></p>
<script>
        const vecSpeeds = {
          100: 1/0.00029814304085448387,
          1000: 1/0.002743517281487584,
          10000: 1/0.038284409288316966,
          100000: 1/0.5512656086706557
        };

        const loopSpeeds = {
          100: 1/0.0023364973906427624,
          1000: 1/0.04799017548095435,
          10000: 1/0.720272407180164,
          100000: 1/10.535213513069321
        };

        let mult = [7.637348888, 14.978150694, 21.25242236];

        const labels = Object.keys(vecSpeeds);
        const vecData = Object.values(vecSpeeds);
        const loopData = Object.values(loopSpeeds);

        const data = {
            labels: labels,
            datasets: [{
                label: 'Vectorized',
                data: vecData,
                backgroundColor: '#00ff0022',
                borderColor: '#00ff00',
                borderWidth: 1
            }, {
                label: 'Non-vectorized',
                data: loopData,
                backgroundColor: '#8a2be222',
                borderColor: '#8a2be2',
                borderWidth: 1
            }]
        };

        const config = {
            type: 'bar',
            data: data,
            options: {
              scales: {
                  y: {
                      type: 'logarithmic',
                      beginAtZero: true,
                      title: {
                          display: true,
                          text: 'runs/sec (log scale)'
                      }
                  },
                  x: {
                    title: {
                      display: true,
                      text: 'number of points'
                    }
                  }
              },
              plugins: {
                title: {
                    display: true,
                    text: 'K-means speeds (k=5)',
                    font: {
                        size: 16
                    },
                    padding: {
                        top: 10,
                        bottom: 30
                    }
                }
              }
            }
        };

        const myChart = new Chart(
            document.getElementById('groupedBarChart'),
            config
        );
</script>
      </div>
    </article>

    <hr />

    <div class="post-info">
      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://lukesalamone.github.io/tags/machine-learning/">machine-learning</a></span>
        <span class="tag"><a href="https://lukesalamone.github.io/tags/numpy/">numpy</a></span>
        <span class="tag"><a href="https://lukesalamone.github.io/tags/performance/">performance</a></span>
        
    </p>

      

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        1064 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2024-02-04 22:39
        

        
          
        
      </p>
    </div>

    
      <div class="pagination">
        <div class="pagination__title">
          <span class="pagination__title-h">Read other posts</span>
          <hr />
        </div>

        <div class="pagination__buttons">
          
            <span class="button previous">
              <a href="https://lukesalamone.github.io/posts/learning-the-haystack/">
                <span class="button__icon">←</span>
                <span class="button__text">Learning the Haystack</span>
              </a>
            </span>
          

          
            <span class="button next">
              <a href="https://lukesalamone.github.io/posts/chess-blunders/">
                <span class="button__text">What is a blunder in chess?</span>
                <span class="button__icon">→</span>
              </a>
            </span>
          
        </div>
      </div>
    


    
  </main>
  <div id="shadow-search" class="posts">
    <div class="posts-group">
      <div class="post-year">2022</div>
      <ul class="posts-list"></ul>
    </div>
  </div>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2024 Luke Salamone</span>&nbsp;|&nbsp;
            <span><a href="https://lukesalamone.com/" target="_blank">lukesalamone.com</a>&nbsp;|&nbsp;<a href="https://github.com/lukesalamone/" target="_blank">github</a></span>
        </div>
    </div>
</footer>

            
        </div>

        




<script type="text/javascript" src="../../bundle.min.fdc5aa536e3e114f1858ac9d803de25aba14817a1fa56e188943bfc00702be87ecb0b477b68f6868a13772d5fd57341d069d167527d85cf7fab246f9aafba8cd.js" integrity="sha512-/cWqU24&#43;EU8YWKydgD3iWroUgXofpW4YiUO/wAcCvofssLR3to9oaKE3ctX9VzQdBp0WdSfYXPf6skb5qvuozQ=="></script>



    </body>
</html>
