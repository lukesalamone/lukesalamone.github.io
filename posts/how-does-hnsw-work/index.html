<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="keywords" content=", machine-learning, retrieval, vectors" />
<meta name="google-site-verification" content="sPwRw-1pCCEcm2EbqDJiJzJEnze1K7FkQzY9xLreBJQ" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://lukesalamone.github.io/posts/how-does-hnsw-work/" />

<meta property="og:title" content=""/>
<meta property="og:image" content=""/>
<meta property="og:description" content=""/>

<script>
    if(location.hostname !== 'localhost') {
        
            fetch('https://d1tkdcmshl91xi.cloudfront.net?p=' + window.location.href.match(/\/([^/]+)\/$/)[1]);
        
    }
</script>


    <title>
        
            How does HNSW work? :: Luke Salamone&#39;s Blog 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="../../main.664d1df93a1a25c7c8d09147fb0b5e685b4ebe309ab122277733d326beea3022.css">







  <meta itemprop="name" content="How does HNSW work?">
  <meta itemprop="description" content="Suppose we have a vector database with a billion items in it (the haystack). And suppose we are looking for K vectors, the needles which maximize some similarity function. (In the case of cosine similarity or euclidean distance, we may be maximizing 1-distance(x,y).) And also suppose that we’d like to do this quickly.
Naive and semi-naive approaches One approach might be to compare every vector and take the argmax. In that case, for vectors of length D our runtime will be 1 billion x D.">
  <meta itemprop="datePublished" content="2024-05-20T13:38:01-07:00">
  <meta itemprop="dateModified" content="2024-05-20T13:38:01-07:00">
  <meta itemprop="wordCount" content="850">
  <meta itemprop="image" content="https://lukesalamone.github.io/">
  <meta itemprop="keywords" content="Machine-Learning,Retrieval,Vectors">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://lukesalamone.github.io/">
  <meta name="twitter:title" content="How does HNSW work?">
  <meta name="twitter:description" content="Suppose we have a vector database with a billion items in it (the haystack). And suppose we are looking for K vectors, the needles which maximize some similarity function. (In the case of cosine similarity or euclidean distance, we may be maximizing 1-distance(x,y).) And also suppose that we’d like to do this quickly.
Naive and semi-naive approaches One approach might be to compare every vector and take the argmax. In that case, for vectors of length D our runtime will be 1 billion x D.">



    <meta property="og:url" content="https://lukesalamone.github.io/posts/how-does-hnsw-work/">
  <meta property="og:site_name" content="Luke Salamone&#39;s Blog">
  <meta property="og:title" content="How does HNSW work?">
  <meta property="og:description" content="Suppose we have a vector database with a billion items in it (the haystack). And suppose we are looking for K vectors, the needles which maximize some similarity function. (In the case of cosine similarity or euclidean distance, we may be maximizing 1-distance(x,y).) And also suppose that we’d like to do this quickly.
Naive and semi-naive approaches One approach might be to compare every vector and take the argmax. In that case, for vectors of length D our runtime will be 1 billion x D.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-05-20T13:38:01-07:00">
    <meta property="article:modified_time" content="2024-05-20T13:38:01-07:00">
    <meta property="article:tag" content="Machine-Learning">
    <meta property="article:tag" content="Retrieval">
    <meta property="article:tag" content="Vectors">
    <meta property="og:image" content="https://lukesalamone.github.io/">






    <meta property="article:published_time" content="2024-05-20 13:38:01 -0700 PDT" />








    </head>

    <body class="dark-theme">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="../../" style="text-decoration: none;">
  <div class="logo">
    <div class="logo-holder-dark">
      <img src="../../img/logo.png" alt="Blog Home" />
    </div>
    <div class="logo-holder-light">
      <img src="../../img/logo2.png" alt="Blog Home" />
    </div>
  </div>
</a>


        <span class="header__right">
            
            <div id="header_search">
              <input type="text" placeholder="Search here" />
            </div>
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        4 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="https://lukesalamone.github.io/posts/how-does-hnsw-work/">How does HNSW work?</a>
      </h1>

      

      <div class="post-content">
        <p>Suppose we have a vector database with a billion items in it (the <em>haystack</em>). And suppose we are looking for K vectors, the <em>needles</em> which maximize some similarity function. (In the case of cosine similarity or euclidean distance, we may be maximizing <code>1-distance(x,y)</code>.) And also suppose that we&rsquo;d like to do this quickly.</p>
<h2 id="naive-and-semi-naive-approaches">Naive and semi-naive approaches</h2>
<p>One approach might be to compare every vector and take the argmax. In that case, for vectors of length D our runtime will be <code>1 billion x D</code>.</p>
<pre><code class="language-python">def search(needle, haystack, arg_k=1):
    # normalize needle and haystack vectors
    needle /= np.linalg.norm(needle)
    haystack /= np.linalg.norm(haystack, axis=1, keepdims=True)
    
    # compute cosine similarities
    similarities = np.dot(haystack, needle)
    
    # get the indexes of the top k elements
    idx_topk = np.argpartition(a, -k)[-k:]

    # return haystack vector with highest cosine similarity
    return haystack[idx_topk]
</code></pre>
<p>In practice, there are some optimizations we can try which can increase the practical speed of this operation:</p>
<ul>
<li>We can parallelize computations on-device with <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a> or <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">BLAS</a>.</li>
<li>We can parallelize computations on-device with GPU.</li>
<li>We can parallelize across cores for machines with multiple cores.</li>
<li>We can parallelize across devices. For example, partitioning the database across 10 clusters, finding a &ldquo;winner&rdquo; from each cluster, and then repeating the search across the 10 winners.</li>
</ul>
<p>There are also some relatively simple but &ldquo;destructive&rdquo; techniques we can also try:</p>
<ul>
<li>We can try pruning the dataset in some way beforehand. For example, maybe we don&rsquo;t need to consider old or unpopular vectors.</li>
<li>We can try reducing the dimensionality of the vectors with something like PCA.</li>
<li>We can try quantizing the vectors e.g. <code>f64</code> -&gt; <code>f16</code> reduces memory requirements 4x. (Usually this requires quantization-aware training.)</li>
</ul>
<h2 id="hierarchical-navigable-small-worlds-hnsw">Hierarchical Navigable Small Worlds (HNSW)</h2>
<p>Sometimes linear time is still too slow. In that case, we can consider an <em>approximate</em> solution, where some acceptable percent of vectors returned are actually not in the top K nearest neighbors. This is called approximate nearest neighbors (ANN). If we have a large system, it&rsquo;s possible to <a href="https://sbert.net/examples/applications/retrieve_rerank/">use another more precise model to rerank</a> the vectors results anyways. In that case, we just need to set K large enough to reliably include the vectors we really need.</p>
<p>ANN algorithms typically come in three flavors: graph based approaches, space partitioning based approaches, and hash based approaches. If your dataset is large enough, you might add compression (e.g. product quantization) on top of it.</p>
<h3 id="intuition">Intuition</h3>
<p><a href="https://arxiv.org/pdf/1603.09320">HNSW</a> is a bit of a Six Degrees of Kevin Bacon approach. I&rsquo;ll give what I think this is a fairly good intuition for how it works.</p>
<p>Suppose you have 1,000 friends and you&rsquo;d like to know which five of them live closest to a landmark like the Golden Gate Bridge. You don’t have everyone&rsquo;s exact address, but each friend keeps a list of their close friends&rsquo; addresses. You might start with a friend who lives in the US. From there, you check their friends to find who lives closest to the target, and then search among their friends for someone even closer. By using a hierarchical approach, you don&rsquo;t need to search through everyone, just those likely to be closest.</p>
<p>In the analogy, the Golden Gate Bridge is the &ldquo;needle&rdquo; and the friends are the &ldquo;haystack&rdquo;. This is a pretty efficient method, but of course it is possible to miss a node if their friend wasn&rsquo;t linked to an earlier friend.</p>
<figure><img src="../../img/hnsw.png"
    alt="HNSW uses a multi-layered graph for efficient nearest neighbor search."><figcaption>
      <p>HNSW uses a multi-layered graph for efficient nearest neighbor search.</p>
    </figcaption>
</figure>

<h3 id="searching">Searching</h3>
<p>Searching is fairly straightforward. Start from an entry point (maybe a random point) in the top layer. For each of the neighbors of the entry point, compute the distance between the query and that point. If any of the neighbors are closer to the query, hop to that neighbor and repeat the process. Select the closest point and go to the next layer down. We keep searching until we reach layer 0.</p>
<p>To compute the K nearest neighbors during search (with K &gt; 1) we simply maintain a min heap, and add all nodes considered during the search. The number of nodes returned in this search (K) is also known as <code>ef_search</code> in the HNSW paper.</p>
<h3 id="addition">Addition</h3>
<p>To add a vector X, we start by computing the highest layer <code>L</code> the node will appear in. <code>L</code> is randomly selected using an exponentially decaying probability. (All nodes appear in level 0, but can have skip links to higher levels.) Next, starting from the top layer, we traverse the graph structure as if we were searching for X.</p>
<p>Once we reach layer <code>L</code> from earlier, we begin to make connections. Connections can be made with previously found nearest neighbors or nearest neighbors on the current layer. The number of nearest neighbors to consider connections with is controlled by the <code>ef_construction</code> parameter. The number of links to actually create is controlled by the parameter <code>M</code>.</p>
<h4 id="performance-considerations">Performance considerations</h4>
<ul>
<li>HNSW can require a large amount of memory. This can be reduced with product quantization, a lossy vector compression method.</li>
<li>Adding new elements is slow. There is a tradeoff between index quality and time required to build the index controlled by <code>ef_construction</code>.</li>
</ul>
<p>More information about HNSW parameters can be found <a href="https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md">here</a>.</p>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://lukesalamone.github.io/tags/machine-learning/">machine-learning</a></span>
        <span class="tag"><a href="https://lukesalamone.github.io/tags/retrieval/">retrieval</a></span>
        <span class="tag"><a href="https://lukesalamone.github.io/tags/vectors/">vectors</a></span>
        
    </p>

      

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        850 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2024-05-20 13:38
        

        
          
        
      </p>
    </div>

    
      <div class="pagination">
        <div class="pagination__title">
          <span class="pagination__title-h">Read other posts</span>
          <hr />
        </div>

        <div class="pagination__buttons">
          
            <span class="button previous">
              <a href="https://lukesalamone.github.io/posts/sparse-autoencoder/">
                <span class="button__icon">←</span>
                <span class="button__text">What are Sparse Autoencoders?</span>
              </a>
            </span>
          

          
            <span class="button next">
              <a href="https://lukesalamone.github.io/posts/learning-the-haystack/">
                <span class="button__text">Learning the Haystack</span>
                <span class="button__icon">→</span>
              </a>
            </span>
          
        </div>
      </div>
    


    
  </main>
  <div id="shadow-search" class="posts">
    <div class="posts-group">
      <div class="post-year">2022</div>
      <ul class="posts-list"></ul>
    </div>
  </div>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2024 Luke Salamone</span>&nbsp;|&nbsp;
            <span><a href="https://lukesalamone.com/" target="_blank">lukesalamone.com</a>&nbsp;|&nbsp;<a href="https://github.com/lukesalamone/" target="_blank">github</a></span>
        </div>
    </div>
</footer>

            
        </div>

        




<script type="text/javascript" src="../../bundle.min.fdc5aa536e3e114f1858ac9d803de25aba14817a1fa56e188943bfc00702be87ecb0b477b68f6868a13772d5fd57341d069d167527d85cf7fab246f9aafba8cd.js" integrity="sha512-/cWqU24&#43;EU8YWKydgD3iWroUgXofpW4YiUO/wAcCvofssLR3to9oaKE3ctX9VzQdBp0WdSfYXPf6skb5qvuozQ=="></script>



    </body>
</html>
