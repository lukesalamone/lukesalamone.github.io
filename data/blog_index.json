{"earth-antipodes": {"title": "The Other End of the Earth", "text": "The Other End of the Earth White areas show points of earth on land whose antipode is also on land. This is only about 8.6 of all of earth s surface. If you want to fly across the Pacific Ocean you ll have to board an airplane and fly around 12 hours. It s pretty slow. A much faster route would be to go directly through the center of the earth. Digging to China was a common expression I heard growing up with the implication that the opposite side of the globe is somewhere in Asia. For the US that s actually not true. The opposite end of the earth for most of the US is actually somewhere west of Australia. For North America there are actually very few places whose antipode the point on the exact opposite side of the globe is on dry land. If you could dig a hole through the center of the earth from the US you d pop out in the Pacific Ocean. It is actually possible to dig to China though. Most of South America excluding most of Brazil has an antipode in east Asia. In fact several of the world s biggest cities have antipodes on dry land. I ll get to this later. Calculating Antipodes Given the latitude and longitude of a point on earth it s pretty easy to calculate its antipode def antipode lat lon lat 0 lat lon 180 lon if lon 0 else lon 180 return lat lon Dry land Once I have the antipode of a particular latitude and longitude I want to know if it s on dry land or not. The process of figuring out what s at a particular latitude longitude point is called reverse geocoding and there are a few nice tools for doing this for dry land. These tools will give fairly good information about the country a point is in. But for now all I care about is whether a particular point is on land or water not what country it s in. I wasn t able to find a great tool online that does this so I opted for a more brute force method. I used this picture from Wikipedia which represents almost all water as the same color of blue. Based on that I can create a boolean mask to indicate oceans and lakes and reverse project pixels from that image to geographic coordinates. Simple enough. Pixels to Geocoordinates Once you have the x y coordinates they need to be converted to longitude and latitude. Latitude is how far north or south you are on the globe and ranges from 90 degrees at the North Pole to 90 degrees at the South Pole. On a flat map this is represented by a y coordinate. Longitude represents how far east or west you are from the Prime Meridian which runs through Greenwich UK and ranges from 180 to 180 both of which represent the same meridian. How a latitude longitude point is projected onto the globe depends on the map projection. In this case we re using Equirectangular projection so we just need to reverse that operation to get the x y coordinates. The formula for doing so is height is the height of the image def lat2y lat return int 90 lat height 180 width is the width of the image def lon2x lon return int lon 180 width 360 the reverse operations of the above def y2lat y return 90 180 y height def x2lon x return 360 x width 180 Pixel Antipodes Now we can calculate the antipodes of a pixel. def pixel antipode y x lat lon antipode y2lat y x2lon x return lat2y lat lon2x lon Antipodal Cities If we look at a list of the world s largest cities which of them have antipodes on land In the top 20 only four cities do. city population millions lat lon antipode country Shanghai 25.5 31.2 121.5 Argentina Beijing 19.6 39.9 116.4 Argentina Manila 13.5 14.6 121.0 Brazil Tianjin 13.2 39.1 117.2 Argentina", "tags": "python geography", "time": "2022-11-23", "name": "earth-antipodes"}, "knowledge-graph-construction": {"title": "Paper Summary: COMET (Knowledge Graph Construction)", "text": "Paper Summary COMET Knowledge Graph Construction Selected subject relation object tuples generated by COMET Paper link https arxiv.org abs 1906.05317 This paper describes COMET a method of generating knowledge bases automatically. Previous work largely focused on encyclopedic knowledge which has well defined relationships. This paper however focuses on commonsense knowledge. In this paper the authors introduce a commonsense transformer which trains on a knowledge base consisting of tuples and a pre trained language model. Their trained model generates new nodes in the knowledge graph and completes phrases based on edges in the existing graph. Training The training data consists of tuples in the form of subject relation object and the task is to generate the object given the subject and relation. The model architecture was the Vaswani GPT. The architecture is fairly straightforward for a transformer. The only difference of note is the representation of the knowledge tuple as a concatenated sequence of the words of each item of the tuple along with a position embedding. To train the researchers train COMET with Atomic and ConceptNet as knowledge seed sets. The Atomic dataset contains 877k tuples of social commonsense knowledge. Experiments with Atomic used bleu 2 as an automatic evaluation metric. The researchers also used Amazon Mechanical Turk to identify whether the generated responses were reasonable. The researchers also tested on ConceptNet. Relations were in the same subject relation object format. This dataset was evaluated using perplexity on the test set and accuracy of the generated positive samples on the pre trained Bilinear AVG model. Results Atomic experiments showed that the model is able to generate coherent knowledge even when only 10 of the data in a domain is available. The researchers found that 91.7 of the greedily decoded ConceptNet examples were correct. The model seems to be producing fairly good knowledge tuples. However sometimes the generated tuples are merely simplifications of previous knowledge.", "tags": "transformer nlp", "time": "2022-05-17", "name": "knowledge-graph-construction"}, "gpt2-tokenization": {"title": "How does GPT-2 Tokenize Text?", "text": "How does GPT 2 Tokenize Text Let s explore how GPT 2 tokenizes text. What is tokenization It s important to understand that GPT 2 doesn t work with strings directly. Instead it needs to tokenize the input string which is essentially a process for converting the string into a list of numbers or tokens . It is these tokens which are passed into the model during training or for inference. As a concrete example let s look at a few sample sentences tokenizer GPT2Tokenizer.from pretrained gpt2 tokens1 tokenizer I love my dog When we look at tokens1 we see there are 4 tokens input ids 40 1842 616 3290 attention mask 1 1 1 1 Here what we care about is the input ids list. We can ignore the attention mask for now. We can convert the tokens in 40 1842 616 3290 back into strings using tokenizer.decode tokens1 tokens1 input ids tokenizer.decode x for x in tokens1 prints I love my dog tokenizer.decode x .strip .lower for x in tokens1 prints i love my dog This process allows us to recover the tokens as strings from the tokenizer. For dictionary lookups we ll also lowercase the strings and remove the whitespace from them. Now let s see what happens when we do the same thing with more complex words tokens2 tokenizer My favorite color is chartreuse token ids tokenizer.decode x .strip .lower for x in tokens2 prints my favorite color is chart re use Because chartreuse isn t in GPT 2 s vocabulary it is tokenized as chart re and use . About that attention mask For brevity I glossed over what attention mask does above. If you re interested in attention masks I have a blog post on that very topic English words Now it would be interesting to see how many tokens in GPT 2 s vocabulary are actually English words. This is an imprecise metric since it depends heavily on which dictionary we use. There is no single authoritative source of all English words. I ll use several dictionaries and compare the results. Enchant PyEnchant contains a python module enchant which we can use to check if a word is spelled correctly. It can also make spelling suggestions for incorrectly spelled words import enchant d enchant.request dict en US d.check Hello prints True d.check Helo prints False NLTK words The popular NLP library NLTK also contains a word list accessible through its corpus module. from nltk.corpus import words nltk words set words.words len nltk words prints 235892 English 350k This list of words was taken from this github repository. It is a convenient list of lowercased words containing only letters. It seems to be the biggest of the word lists. Lemmatization We can bump our numbers up slightly through lemmatization In many languages words appear in several inflected forms. For example in English the verb to walk may appear as walk walked walks or walking . The base form walk that one might look up in a dictionary is called the lemma for the word. The association of the base form with a part of speech is often called a lexeme of the word. For our lemmatizer we will use WordNetLemmatizer from nltk.stem.wordnet. Testing GPT 2 tokens So of the tokens which GPT 2 uses how many are English words We can break this down metric by the dictionary used. Dictionary Words English370k 72.92 English370k 72.59 Enchant 60.48 Enchant 60.17 NLTK words 57.07 NLTK words 48.27 indicates words were lemmatized So the English370k word list seems to capture the most tokens from the three dictionaries. Also note the mild impact of lemmatization although it may bump some of the percentages up a bit it s not enough for one dictionary to outperform another. Looking at the tokens which aren t in the dictionary around 73 of them are non word alphabetical strings. The final 27 is accounted for by symbols numbers and non ascii character sequences unicode characters from languages like Arabic Korean and Chinese . If we remove these we end up with about 10k tokens containing only letters which is around 21 of GPT 2 s total vocabulary. I ve included this list in a github gist duplicates removed . Now what Looking at these non word alphabetical strings it s interesting to see how the Internet as GPT 2 saw it was encoded. Then again it also contains a lot of proper nouns which wouldn t be in a normal dictionary like starbucks . Other tokens are clearly vestiges of the scraping process which was used to gather text which GPT 2 trained on. Tokens like rawdownloadcloneembedreportprint buyableinstoreandonline randomredditorwithno and itemthumbnailimage contain next to zero semantic value and the vocabulary space would probably have been better served with more meaningful tokens. The following are the longest non dictionary tokens found in GPT 2 s vocabulary Token ID String 39177 ItemThumbnailImage 30210 guiActiveUnfocused 39755 isSpecialOrderable 31576 externalActionCode 39753 quickShipAvailable 39757 channelAvailability 36174 RandomRedditorWithNo 30899 cloneembedreportprint 40242 BuyableInstoreAndOnline 30906 rawdownloadcloneembedreportprint We may also be able to measure performance of GPT 2 on certain tasks based on how many of the tokens were dictionary words. It might be true for example that sentences with higher proportions of dictionary word tokens would perform better on sentence completion tasks.", "tags": "nlp gpt2", "time": "2021-06-17", "name": "gpt2-tokenization"}, "perplexity": {"title": "What is Perplexity?", "text": "What is Perplexity TLDR NLP metric ranging from 1 to infinity. Lower is better. In natural language processing perplexity is the most common metric used to measure the performance of a language model. To calculate perplexity we use the following formula perplexity e z where z 1 over N sum i 0 N ln P n Typically we use base e when calculating perplexity but this is not required. Any base will do so sometimes the formula will use base 2 or base 10 along with logarithms to the corresponding base. Example Imagine that we have a language model which generates the following sequence of tokens start jack and jill went up the hill And suppose that the conditional probabilities for each of the tokens are as follows token probability start 15 jack 5 and 12 jill 18 went 25 up 40 the 33 hill 50 For the purposes of calculating perplexity it doesn t matter how the sequence was generated. It may be using an n gram model or an LSTM or a transformer. All that matters is the probabilities the model assigns to each of the tokens. To calculate perplexity we calculate the logarithm of each of the values above token P ln P start 15 1.897 jack 5 2.996 and 12 2.120 jill 18 1.715 went 25 1.386 up 40 0.916 the 33 1.109 hill 50 0.693 Summing the logs we get 12.832. Since there are 8 tokens we divide 12.832 by 8 to get 1.604. Negating that allows us to calculate the final perplexity perplexity e 1.604 4.973 Therefore the perplexity of this sequence is about 4.973.", "tags": "nlp statistics", "time": "2021-04-01", "name": "perplexity"}, "rolling-my-own-blog-search": {"title": "Rolling My Own Blog Search", "text": "Rolling My Own Blog Search I ve found myself hitting ctrl f on this blog enough that I figured it s about time to add some search functionality to it. While there are certainly prefab solutions out there this task is simple enough and fairly instructive. I had a few requirements though The search needs to be fast useful and aesthetically pleasing. Search in the browser. Standing up a server is a lot of extra work. It s also overkill since I only have about 30 articles so far. Semantic search I did some experiments with small neural networks deployed using ONNX but ultimately they didn t seem to be a good fit for this blog. The search experience was not quite as snappy as I d have liked it to be and while I was able to get the model under 10MB it still added a good amount of bloat to the page size. Further it wasn t clear to me that the search results were significantly better and in some cases they were worse. In any case the advantages were not enough to justify the added page size. I did learn a lot with this experiment which probably deserves a blog post of its own at some point. I might revisit semantic matching at a later point as well. BM25 Rather than loading a neural network I decided instead to index the text of my blog and use BM25 to rank documents. One difficulty in this was finding the actual algorithm which didn t include a ton of extra code. I was only interested in the function itself not all of the extra boilerplate code floating around. In the end I broke down and rewrote it myself. Tokenizing Text BM25 works by scoring document tokens against query tokens. Documents with more relevant tokens should be scored higher than those with fewer relevant tokens. This raises the question of how to convert a query string into a group of tokens. Initially I simply split by word. That is hello my friend hello my friend Although this works pretty well it has the downside that different forms of a word will be missed. For example friends will not match with friend . To solve this problem I decided to use fixed token sizes of three characters instead hello my friend hel ell llo lo my my fr fri rie ien end In this way friend will be tokenized very similarly to friends so the penalty for mismatches between forms of words will be minimized. Subjectively I felt that this yielded a much more fluid and versatile ranking as well. Multi factor scoring My blog posts have title body and tags. I could simply concatenate them all together and score the combined string and it would work pretty well. But this seems to throw away the fact that the title and tags may have more concentrated information than the article body does. They deserve to be weighted differently. I decided to weight the title body tags components as 30 50 20 in the final ranking.", "tags": "search nlp", "time": "2022-11-09", "name": "rolling-my-own-blog-search"}, "s3-bucket-url": {"title": "S3 Bucket Url", "text": "S3 Bucket Url Assuming your bucket is publicly accessible the url of your S3 bucket will be http bucket name .s3 website region .amazonaws.com For example for mybucket in us east 1 your url will be http mybucket.s3 website us east 1.amazonaws.com", "tags": "aws quick reference", "time": "2021-03-10", "name": "s3-bucket-url"}, "python-serve-html": {"title": "Python: Serve an HTML File", "text": "Python Serve an HTML File If you want to serve some HTML with python run python m http.server 8000 Then navigate to http localhost 8000. This is not meant for production environments but will get you around CORS restrictions that would come from simply opening a local file in your browser.", "tags": "python quick reference webdev", "time": "2021-05-09", "name": "python-serve-html"}, "chatgpt-mnist": {"title": "Can ChatGPT Recognize Handwritten Digits?", "text": "Can ChatGPT Recognize Handwritten Digits TLDR No. No it cannot. This was admittedly a fairly stupid experiment on the face of it. ChatGPT is a decoder only model. It shouldn t be able to perform an image recognition task. But then again a decoder only model wouldn t have been my first choice for translation or summarization either. In my experience ChatGPT has created translations which are at least as coherent and idiomatic as Google Translate if not more so. So I thought why not give it a shot Process MNIST is a computer vision dataset containing thousands of handwritten digits along with their actual labels. Images are represented as 28x28 tensors where each element of the tensor represents a intensity pixel intensity between 0 and 1. Traditionally this is formulated into a classification problem with the goal of choosing the correct class out of the 10. To simplify the input I flattened the tensor and translated the pixel intensity values to the range 0 10. This allowed me to use fewer tokens at the expense of some of the gradations between light and dark. I gathered 10 random examples from each of the 10 classes 100 total images and fed them into gpt 3.5 turbo with the following prompt The following is a flattened representation of an image of a handwritten digit. The image was 28x28 but has been flattened to 1x784. Each number represents a pixel intensity from 0 10. Please tell me which digit the following pixel list represents Followed by the list of 784 pixel intensities. Results ChatGPT scored 11 100. Barely better than guessing. A summary of the sessions can be found at this gist. A visual summary is below with the blue signifying correct guesses. Note although the image shows the digits sorted they were prompted in random order. It is interesting that only the 0s and the 7s were correctly identified. Let s take a look at the distribution of ChatGPT s guesses. Guess Count 0 85 1 0 2 3 3 1 4 1 5 4 6 1 7 5 8 0 9 0 Oh. It got 8 10 zeros correct because it guessed zero 85 of the time.", "tags": "chatgpt computer vision", "time": "2023-07-30", "name": "chatgpt-mnist"}, "hardest-hangman-word": {"title": "What is the Hardest Hangman Word?", "text": "What is the Hardest Hangman Word Table of Contents It seems like a simple enough question. Which word should you choose so that it takes your opponent the most guesses to discover it Should you choose a long word to use up your opponent s guesses Or perhaps a short word with obscure letters In this document I look into this question. But first a bit of background. If you re not familiar with the rules of hangman it is a guessing game played between two people. Player A chooses a secret word and tells player B the length of the secret word. Player B guesses letters which she thinks might be in the word. If she chooses a correct letter player A reveals the locations of each instance of the guessed letter. However if player B guesses an incorrect letter this counts as a strike against her. After an agreed upon number of strikes player B loses. An algorithmic approach A few years ago I created a hangman solver for the popular paper and pencil game. This game assessed each game analytically to determine a list of possible words given the clues available. The algorithm works as follows at the beginning of the game we know the length of the secret word which narrows our dictionary considerably. Then for each letter in the alphabet count the number of words available which contain that letter. Suppose our dictionary consisted of a random list of 50 four letter words as follows pull dipt dorp poky jism cues hood drag inky mhos kerf jess mete lues wipe kane tiro keys jape lime sees sass demo ilia mink dips hove jees that pops isle teas dens dogy pink sizy cole pact thaw lead mile dodo litu scup colt soma seat dewy pits mojo This would result in letter counts as follows letter count a 16 b 7 c 15 d 5 e 26 f 1 g 5 h 3 i 10 j 1 k 6 l 14 m 6 n 6 o 14 p 7 r 8 s 15 t 13 u 4 w 1 x 1 y 4 In this case the letter E is found in 26 words the most of any letter. Therefore our algorithm should pick E since it is the safest guess. Once we guess a letter correctly this gives important positional information which can filter the word list even further. This process is repeated each time picking the most likely letter given the constraints we know. If we know some of the letters in the secret word we can eliminate any words that don t have those letters in those positions. If we have guessed a letter incorrectly we know that letter isn t in the secret word and can eliminate all words which have that letter. You can see a python implementation of this algorithm here. For this experiment I used the Zynga dictionary the same dictionary used in the game Words With Friends. The dictionary contains 173 000 words. It does not include any proper nouns or profanities. Live experiment Below you can experiment with this algorithm among 4 letter words. Enter a secret word and the steps used to uncover the secret word will be displayed below. Need inspiration Try comparing jazz to rock or blue to grey. div style width 100 text align center input placeholder enter 4 letter word input button class play play word button button class reset reset button div div class output style margin 20px 0 div Notice how some words have much higher difficulty than others This is due to the fact that some words have many siblings which differ by only one letter. For example the pattern .ays could match the letters c d f h j k l n or r. Knowing the last 3 letters gives us no indication of which of these nine letters it will be. One apparent shortcoming of the above calculation is that it assumes that letters with equal probability will be picked in alphabetical order and therefore letters last in the alphabet will be picked last. Although this has the benefit of creating a deterministic algorithm which will always return the same result for the same word in real life we don t know in which order people will pick letters. In actuality the words days jays and rays have equal difficulty each is equally likely to be the secret word. Preliminary results With this preliminary caveat in mind we can still calculate the difficulty of every word in the dictionary. If we assume that in a situation where multiple letters are equally probable our opponent will break the tie using alphabetical ordering which hangman words are the hardest to guess Here are the top 17 word difficulty zill 19 zills 18 yill 18 zin 17 zax 17 yills 17 will 17 vox 17 mem 17 zins 16 yuck 16 yin 16 wills 16 vill 16 oak 16 jazz 16 foy 16 27 more 15 Previous research I should note that previous research by Jon McLoone in 2010 has explored the same topic although he used slightly different methodology and a smaller 90 000 word dictionary. His algorithm was not deterministic and so does not always pick the most frequent letter available. Instead his algorithm picks a letter with a probability proportional to the frequency with which it occurs in candidate words. For example if we refer to the letter frequencies of the 50 word 4 letter dictionary above j appears in just 1 word while e appears in 26 of them. In this case since the sum of the numbers in the frequency table is 188 McLoone s algorithm would pick z in 1 out of every 188 first guesses and e in 26 of them. Although it might seem that this strategy is not optimal it does avoid the deterministic results shown above. Additionally McLoone chose to remain faithful to the logic of the hangman game opting to end the games after a given number of mistakes and recording the probability that a given word was not discovered after the game ended. So an 8 game means that the game was ended after 8 mistakes and a 13 game after 13 mistakes. Using this methodology he found the hardest hangman words were as follows Future research Now I think that we can improve upon our results a bit. Rather than calculating difficulty deterministically we can instead randomize the ordering which letters will be picked in. This should dramatically reduce some of the outliers from above bringing rays down from a difficulty of 14 to something more reasonable. Such a stochastic calculation will require simulating millions of games. For my 173 000 word dictionary simply simulating 10 games would involve 1.7 million games. Fortunately this operation is highly parallelizable. It should be possible to split the dictionary into 100 or even 1000 pieces and derive the results for each piece simultaneously. A simulation of this algorithm is shown below with a graph of the average number of mistakes the new randomized algorithm incurs before discovering your secret word. The simulation is set up to play 500 rounds of games and the final average is displayed at the top. div style width 100 text align center input placeholder enter 4 letter word input button class play play word button button class reset reset button div div class messages span span div div class canvas holder style display none width 500px height 500px canvas canvas div", "tags": "statistics monte carlo python javascript", "time": "2020-07-21", "name": "hardest-hangman-word"}, "build-an-autoencoder": {"title": "Autoencoding Stock Prices", "text": "Autoencoding Stock Prices Autoencoding stock prices as found in Heaton et al. 2016 So you want to build an autoencoder Great This article will demonstrate how to build an autoencoder and use it to measure stock prices against an index. This technique is described in more technical terms here. Once we ve trained the autoencoder we can use it to measure how well each component follows the other members of the index. This can be useful for finding deeper insights into an index and doesn t require a priori knowledge of the index price or the weighting of its components. Note this is only one metric which one could use to determine how well one member of the group follows the group overall. Another might be Pearson Correlation. Github repository To follow along with the code in this tutorial please download the corresponding repository on Github git clone git github.com lukesalamone stock data autoencoder.git cd stock data autoencoder pip install r requirements.txt What is an autoencoder An autoencoder is a neural network which encodes information back to itself. The structure of the network is such that the input layers the encoder will be large compared to the hidden layers the code forcing the network to compress information inside its hidden layers. The idea of our autoencoder is that we would like to encode stock price information back to itself while discarding trends that aren t important. To do this we will feed our network stock price data and ask the network to return those prices to us as outputs. Component stocks which are important to the index will be preserved well and thus be highly accurate while components which are less important will not be well preserved. We will measure the performance of the network on each component using mean squared error. The model We will use an autoencoder with a number of inputs and outputs equal to the number of component stocks in our index. For this exercise we will use the S P 500 index which contains 505 components. This means our input and output size will be 505. We will also use a hidden layer with 5 units. class StonksNet nn.Module def init self size super . init self.fc1 nn.Linear in features size out features 5 self.out nn.Linear in features 5 out features size def forward self x Tensor Tensor x F.relu self.fc1 x x F.relu self.out x return x The data We will use daily stock prices downloaded using yfinance. This data is readily available online and I recommend downloading it for yourself. We will use data between January 1 1991 to January 1 2021 30 years of data . To download the S P 500 stock data please run gather stocks.py from the project directory python gather stocks.py This will download all 505 components into the stock data directory. Data will also be cleaned such that each component has the same number of days which will be important when feeding it into the model. Training the model The model itself is a simple feed forward neural network. As such we use a standard training loop to train the model. We don t expect the loss to ever fall to zero during training since it is impossible for the network to perfectly encode and decode so many inputs into so few hidden code units. Some information will inevitably be lost. In my training validation losses bottomed out at around 4000 but yours may be different depending on the initialization of your autoencoder. Ranking components Finally we re ready to rank the components of the S P 500 for closeness . After running python train model.py you will see the best and worst components as scored by the autoencoder. Here were my results yours may be different. best 5 results DRE 16.66 LNT 37.27 MU 38.88 HOLX 43.18 CERN 47.46 worst 5 results HUM 105244.19 SHW 108542.73 LMT 113654.48 C 357073.88 NVR 10955169.00 Future research Upon inspection it appears that better results might be achieved if we normalize the stock data before training. It appears that stocks with higher prices and higher volatility tended to perform worse than those with tight price ranges. In a way this is expected since the autoencoder will naturally have a harder time modeling large values with a limited set of hidden units. However normalizing the prices into similar ranges might be an interesting exercise to see if we can squeeze even more out of the model.", "tags": "neural networks pytorch stock trading", "time": "2021-03-06", "name": "build-an-autoencoder"}, "qualitative-analysis-chess": {"title": "A new type of chess tournament", "text": "A new type of chess tournament This is part 2 of a paper I wrote for Ken Forbus Qualitative Reasoning course adapted for this blog. You can find a printable version of the paper here and part 1 here. In the previous post I discussed the history of chess engines and why they don t think like we think. Trading interpretability for computation cycles ultimately led to the engines we have today fairly alien in nature and perhaps less pedagogically useful because of it. At the time though the goal was to beat human grandmasters by any means necessary a great engineering feat that the field had been working on for decades. This post contains two related proposals. The first is a chess engine tournament unique in the type of engine which will be permitted to enter and likely to succeed. Importantly the vast majority of engines currently holding the highest performance ratings will likely not be effective. The second proposal is the outlines of a chess engine that is likely to be successful in this tournament taking advantage of the highly qualitative nature of chess position evaluation. Although it is unlikely to perform as strongly against top performing engines there are several distinct advantages of such an engine. In short there is likely to be a great deal of educational value as well as financial incentive driving the construction of highly successful qualitative chess engines. Qualitative chess analysis Qualitatively there are many aspects to a chess game that may be captured. Let s take a look at the way a grandmaster analyzes a position. It will become quite apparent that at the highest levels the qualitative aspects of position analysis dominate over quantitative aspects i.e. the number and value of each piece . In the selected lecture Grandmaster Varuzhan Akobian details a game he played previously. At a key moment of the game Akobian sacrificed his rook for a key pawn in the center of the board. The resulting position is reproduced for convenience in Figure 1. Figure 1. Quantitative analysis would posit that black is winning due to his extra rook for white s knight and pawn. However qualitative analysis provides a more complete picture of black s predicament. His analysis starts at 25 15 in the video I would like you to spend a minute or two just to give me the evaluation of this position. It may not seem that clear because I m down the exchange. Novice chess players are taught that chess pieces have quantitative values which may come into consideration when exchanging one piece for another. These values are measured in terms of pawns. Knights and bishops are generally understood to be worth 3 pawns rooks are worth 5 and queens are worth 9. I have a knight and a pawn for a rook. Rook is valued 5 knight and a pawn is 4. It may seem like I m down a pawn here. But what do you think is the proper evaluation of this position Basically white is very active. There are a few other things we can mention about white s position that it s very strong. What else is very strong White s king is very safe he cannot attack me. But how about the black king Do you think the black king is very safe No. For example I could put my queen here the e4 square then I have a battery Remember when we have a queen and a bishop on the same diagonal we call that a battery. And suddenly if I can deflect this queen black queen on the g7 square I will just go queen takes pawn checkmate His dark square bishop is basically trapped behind his own pawns so it s ineffective. . . . My bishop is very active. . . . And one more thing that you can mention. Passed pawn exactly And it s a very strong passed pawn because with a knight on d6 very quickly the pawn will turn into a queen . . . . How much advantage does white have here Big advantage slight advantage maybe winning . . . We re not going to use Houdini a chess engine Houdini will probably say black is slightly worse. But in practical play I would be very comfortable to play this against anybody and pretty comfortable I can win this position for white. Note that quantitative analysis is almost entirely absent from GM Akobian s evaluation. Towards the beginning he mentions that he has sacrificed his rook for a knight and a pawn and consequently is at a material deficit. However he quickly discards this shallow evaluation going so far as to label his subsequent qualitative evaluation as the proper evaluation. GM Akobian goes on to mention several other qualitative features of the position which are difficult to assign quantitative value to. Firstly the activity of his pieces means that it is much easier to play the position as white because his pieces are on better squares including some deep in black s half of the board. The lack of activity is mentioned later on noting that black s bishop is essentially trapped behind his own pieces. King safety is another difficult thing to quantify. In the given position it is difficult to find a way that black can even check the white king. Moving the d8 rook to b1 will take 2 moves and even then the b1 square is guarded by the bishop on d3. So the white king is indeed quite safe. In contrast black king is quite vulnerable guarded mainly by the black queen who is herself vulnerable to deflection or direct attack. Deflection is a chess tactic which involves distracting an opponent s piece which plays an important defensive role. For example a piece which is defending two pieces simultaneously may be deflected by capturing one of the defended pieces. GM Akobian emphasizes the weakness of black s king by sketching out a simple game winning checkmate plan arrange the bishop and queen in a battery which attacks the h7 pawn deflect the black queen and deliver checkmate with the queen by taking the h7 pawn. Although it is not immediately clear how to implement the plan this type of simple plan creates a well defined long term threat that black must contend with. Another threat he mentions is encompassed by white s passed pawn on c5. This pawn may become a queen which would become an insurmountable advantage for white. Therefore this threat is another long term vulnerability for black. A passed pawn is a pawn which cannot be stopped or attacked by an opponent s pawns. This occurs when there are no opponent pawns in the file vertical column of the pawn as well as the file to the left and the right if applicable. For example a pawn in the C file is a passed pawn if there are no opponent pawns in the B C or D files. A pawn in the H file is passed if there are no opponent pawns in the G or H files. Finally note that GM Akobian does not assign a quantitative value to the board position but rather a very comfortable to win assessment. Very little of this analysis involves quantities but rather qualitative situations which must be dealt with. Consequently it seems that qualitative reasoning is an ideal tool which a chess engine might use. Qualitative analysis is more human The nature of expert level perception described by GM Akobian was studied directly in a 1973 paper by Chase and Simon. Participants at three different levels of chess ability a master level player an experienced club level player and a novice were asked to complete two chess related cognitive tasks. The first was a perception task requiring him to reproduce a chess position on an adjacent board as quickly as possible with the model board in plain view. The second task was a memory task requiring participants to reproduce a position from memory after viewing it for only 5 seconds. Importantly the perceptual study attended to chess players tendencies to chunk the board position as they reproduced positions tending to remember groups of interrelated pieces. These pieces tended to have relationships which the authors characterized in five ways a piece attacks another a piece defends another two pieces are adjacent two pieces are the same color two pieces are the same type. The results of the study found that the C S and null relations are low because subjects are placing pieces which usually have multiple relations. Thus from the within glance relations it appears that subjects are noticing the pawn structure clusters of pieces of the same color and attack and defense relations over small spatial distances. In other words it seems likely that human players use the qualitative relationships between pieces to remember the board. A qualitative chess engine It is unlikely that a qualitative chess engine will be able to entirely do away with the basic structural algorithm involved in chess calculations i.e. minimax. We would like our qualitative engine to calculate in a way most similar to humans and thus will require some level of ply depth to the calculations. However a qualitative engine will have a much stronger sense of the flow of the game and will thus explore fewer branches. Rather than considering each position as discrete a qualitative engine should note how each move guides the evolution of the chess board position. It is important to note that a qualitative chess engine may not be the most computationally efficient a factor which was the primary motivation during the period of time when top chess engines needed to be run on supercomputers and every ounce of performance needed to be squeezed out of the machine. A qualitative engine should instead favor explainability over performance whenever possible. Instead an engine should produce a good explanation of which moves were considered and why a particular move was chosen. More modern qualitative research can improve upon Wilkins knowledge based PARADISE approach. It is important to recognize that his knowledge base is quite similar in nature to the FAC component of the retrieval model presented by Forbus et al. in 1995 but does not take advantage of the performance speedups presented there. Because of the high number of positional examples available online there is a huge opportunity for a performant analogical retrieval system at present. The MAC FAC retrieval system could pay huge performance dividends in retrieval if applied to this problem. Specifically the Lichess database referenced above contains 1 737 489 chess puzzles as of the time of writing. A chess puzzle is simply a chess board position in which players are encouraged to find the best move. Each puzzle relates to one or more chess themes e.g. mate in 1 pin discovered attack etc. analogous to Wilkins concepts outlined above. Each puzzle also includes the best move to make in the position. Some research will need to be done to derive meaning from this best move relating it by analogy to the current position being evaluated by the engine. Qualitative spatial calculi may also be used to construct more psychologically plausible models of chess positions than simply noting which pieces occupy which squares seeking to emulate the models suggested by Chase and Simon. Chess pieces have intricate relationships which can be captured and which change whenever a piece moves to another square. Importantly however not all relationships are affected by the movement of a single chess piece suggesting that performance gains may be realized by recomputing only those relationships which have changed. It is likely that low level piece relationships may give rise to higher level relationships and tactics. For example the concept of capturing the defender arises from the concept of attacking a piece A which defends B which works when pieces A and B are attacked by pieces C and D of opposing colors. And in the case of Figure 2 below a defender may be deflected to win the piece it is defending. Defensive relationships may be thought of in a chain or directed graph with each piece defending another and the safety of a piece being considered in relation to its connection to a defensive group. Figure 2. White to move. From this image many basic piece relationships are apparent with only 8 pieces left on the board. Importantly the black king is defending the black queen. The black queen is also being attacked by the white queen and is attacking the white queen. White s queen is undefended a state sometimes referred to as hanging. These relationships reveal the opportunity for a tactic. White can simultaneously move his rook to attack black s king danger levels and take advantage of the defensive connection between the black king and queen with the move rook to b8. This forces black to move his king removing the defense of his queen. In this position black s queen can be freely captured by white s queen. Due to the mobility advantages of a queen over a rook this is a favorable move sequence for white. Applications of a qualitative chess engine There are many benefits to reopening the pursuit of qualitative reasoning in chess. The first and most clear value proposition is that qualitative reasoning is likely to serve as a more plausible model for how humans think about the game. This is evidenced by the fact that as Chase and Simon found chess players do not see the whole board at once but rather in chunks of interrelated pieces. Even if the details of human mental models differ slightly from the implementation of a qualitative reasoning engine it will be able to provide a traceable account of its decision making process an important step towards explainability. As we saw in part 1 current top chess engines reason about chess in ways that are quite contrary to human intuition. Stockfish uses full width search considering each move in each position without prejudice and assigning numerical values to each position. As we saw from the analysis from GM Akobian qualitative evaluations are far more meaningful to humans. ther chess engines approach chess in an even more alien way. Specifically it is unlikely that any engine which makes heavy use of neural evaluation functions will model human derived organic strategies in ways which chess players will recognize. At the far end of this spectrum is the fully neural Maia chess engine but even Lc0 s Monte Carlo tree search precludes consideration for cognitive plausibility. Qualitative chess engines which are able to better reproduce the types of chess reasoning used by top human chess players are also likely to serve as better pedagogical tools for those interested in studying chess. This applies at every level from beginner to grandmaster. The skill level of such a chess engine would be quite easily tunable simply by disabling more advanced knowledge from the knowledge base. This is a far more natural method of handicapping than the search depth limitations used in current chess engines. Each piece of knowledge becomes a tunable parameter to the engine. As students learn concepts the corresponding representations in the knowledge base could be enabled allowing for gradual learning in a far more accessible way. In fact it is likely true that a qualitative chess engine could outperform human grandmasters who often teach chess to others in this respect. Finally it is likely that a qualitative engine would become a key component of a first line of defense against cheating in chess. Most cheating is performed by using assistance from a chess engine during online games with unsuspecting opponents. Consulting a functionally omniscient computer program can thus provide a cheater with a theoretically insurmountable advantage. In an interview with the Perpetual Chess Podcast Chris Callahan of the popular chess website LiChess.org stated that the majority of employees of the website work primarily to detect cheaters and yet the problem still persists. By exploiting the difference between conventional full width engines like Stockfish and a qualitative evaluation those working to detect cheaters will be better equipped to detect suspicious moves. However qualitative chess engines are unlikely to be able to completely replace human moderation. Nerf the Engine A computer chess tournament be held between chess engines can encourage the type of reasoning and gameplay which resembles human games. However because we are not interested in the best overall chess engine but one which can reason like a human might the rules of the tournament will be adjusted in several key ways to discourage brute force computational methods. We already know that calculating millions of positions can find the optimal move. But what happens when an engine is limited to e.g. 1000 positions Because we expect few entrants in early iterations of this special tournament engineering an automatic enforcement mechanism for the limitations stipulated in this document are likely to be unnecessary. Engine compliance may simply be verified through manual inspection. Future iterations may include further safeguards potentially separating the position evaluation function and directly counting the number of invocations while arbitrating the tournament to directly verify compliance. Position limitation Firstly competing chess engines will be limited in the number of board positions they can evaluate during any one move. Because human grandmasters evaluate around 100 positions before making a move the tournament arbitration system will artificially impose this limitation on all competing engines. This cap immediately creates an issue for full width chess engines because of chess high branching factor. Were an engine to evaluate each possible move it would perform quite poorly in board positions with many possible plies and replies available rarely reaching a depth of more than 2 or 3. As a result any engine which naively assesses a chess board would perform quite poorly in this setup. The practical upshot of the position limitation is that the engine will be incentivized to gather as much relevant information about a position as possible rather than optimizing for the maximum number of positions. Position saliency Additionally engines will be required to implement scheduling logic which takes the time remaining into consideration. While this creates the immediate problem of how an engine should allocate its time it creates the ancillary challenge of evaluating a position s quiescence. Positions which are quiet and have few forcing moves require less evaluation than positions in which there are many non forcing moves. This requirement immediately motivates a qualitative chess engine to recognize the futility of falling prey to the Horizon Effect. The Horizon Effect causes engines to waste many position calculations pursuing delaying moves which amount to hopeless rabbit trails. Instead an engine should recognize that quiescence has to do with the relationships between pieces. Humans understand this and can quite quickly see the futility of a move and terminate their search. A qualitative analysis which takes this factor into consideration will be able to save a great deal of position calculations behaving more like a human player. Conclusion Given that computers have achieved and sustained superhuman capabilities in the domain of chess the next frontier is not in building increasingly strong engines but harnessing the present computational power to reason about the game in ways that humans do. Qualitative reasoning can provide novel and intuitive ways to reason about previously seen moves and think about the game.", "tags": "chess qualitative reasoning ai", "time": "2022-10-08", "name": "qualitative-analysis-chess"}, "bert-vs-gpt2": {"title": "BERT vs GPT-2 Performance", "text": "BERT vs GPT 2 Performance There are quite a few BERT vs GPT 2 breakdowns online mostly focusing on the architectural differences between the two models. However I am more interested in the performance differences between the two models specifically their predictive capabilities. This blog post outlines the results of my experiments. The code used in this experiment can be found on my Github BERT The Devlin et al. model was released in November 2018. It is a transformer based language model pretrained on masked input also known as the cloze task . During pretraining 15 of tokens are hidden from the model and it is trained to predict the masked tokens. As a result I was able to evaluate its ability to correctly predict a masked token at a random position in a fixed size input. I looked at the following varieties of BERT Model Parameters Compare to bert base uncased 110 million gpt2 bert base cased 109 million gpt2 bert large uncased 336 million gpt2 medium bert large cased 335 million gpt2 medium This table also includes corresponding GPT 2 models which have a similar number of parameters. Source GPT 2 The Radford et al. model hit the scene in February of 2019. Like BERT it is a transformer based model and comes in various sizes ranging from 117M parameters up to 1.5B parameters gpt2 xl . Because GPT 2 is an autoregressive model experiments with this family of models perform one token of generation following input context comparing with the target token for accuracy measurement. Here we will be evaluating two flavors of this model Model Parameters Compare to gpt2 117 million bert base gpt2 medium 345 million bert large This table also includes corresponding BERT models which have a similar number of parameters. Source Wikitext Token prediction To evaluate the models I sampled 10 000 random sequences from Wikitext 2. For BERT a random sequence of 100 tokens is selected. Then for each sequence a random position within that sequence is selected and masked. BERT will be required to predict this token so accuracy is measured as the percentage of the time which its masked token is predicted correctly. For GPT 2 a random sequence of 100 tokens is selected. Then for each sequence a random position within that sequence is selected. Because GPT 2 is autoregressive it cannot attend to tokens on the right so the sequence is truncated at the selected position. The sequence is then padded appropriately to maintain a fixed sequence length of 100. Below we can see the performance of all 6 models on these tasks. The data has been smoothed by bucketing into groups of 5 positions at once i.e. positions 0 4 5 9 etc . You can see that performance of GPT 2 continues to rise as it is given additional context while BERT models are relatively stable after being given around 5 tokens of context. Interestingly BERT performance drops off quite steeply over the last 5 10 token positions. When we zoom in on the final 10 positions things start to get interesting. Both varieties of GPT 2 actually beat out all varieties of BERT at the final position. Conclusion BERT and GPT 2 perform quite differently on the token prediction task depending on the position of the token being predicted. For a fixed sequence length of 100 tokens BERT performs best when the masked token is between positions 5 and 95 while GPT 2 tends to continually improve as context length increases. Interestingly when the final token in the sequence is to be predicted BERT s performance falls off dramatically while GPT 2 performance remains stable.", "tags": "nlp bert gpt2", "time": "2021-06-20", "name": "bert-vs-gpt2"}, "why-how-to": {"title": "About My Quick Reference Articles", "text": "About My Quick Reference Articles I ve created a few quick reference articles and it might not be clear why. There are a few reasons These articles are mainly a reference for me. I find myself searching the same things over and over looking for the purple link scrolling through the article then copy pasting code. I d rather not go through the hassle. These articles aim to solve that problem. I aim to keep the answers above the fold. I don t want to have to scroll down to find the answer. I almost never read the surrounding prose when I am in coding mode . I don t have ads or popups on my blog. I will never ask people to sign up for a newsletter or login to read more. I also don t use pictures unless there s a good reason. The thinking AI robot stock photo industry is definitely a bubble. Writing these things out explicitly helps me to remember them. Paradoxically this may make these how to pages less useful to me but maybe someone else will find them useful. These quick reference articles don t explain much because I don t need an explanation of what is going on. I just need a chunk of working code. There are other websites which have far more comprehensive guides covering how to do things. They cover all of the fundamentals of how things are done. But I don t need that I just want a 30 second reference with a working chunk of code.", "tags": "quick reference", "time": "2021-03-07", "name": "why-how-to"}, "what-is-marginalization": {"title": "What is Marginalization?", "text": "What is Marginalization In machine learning and statistics marginalization simply means summing over a set of independent variables. For example suppose an avid tennis player kept track of the number of days he played tennis over a period of time as well as the weather on that day weather sunny cloudy rainy totals play yes 70 25 1 96 no 70 5 9 84 totals 140 30 10 180 In this table we re keeping track of the number of days. If you want probabilities divide each value in the table by 180. But I think whole numbers are easier to think about so I m keeping them. To marginalize one of the variables we just sum one of the variables. For example to marginalize the weather we would sum each of the rows to find that 96 180 53 of the time tennis was played and 47 of the time tennis was not played. Likewise to marginalize the boolean variable of whether tennis was played we just sum the columns no matter whether tennis was played on that day how many days was it sunny 140. Another way of saying this is the marginal distribution of sunny weather is the first column containing 70 and 70 . The marginal distribution of playing tennis is the first row containing 70 25 and 1 .", "tags": "statistics", "time": "2021-07-07", "name": "what-is-marginalization"}, "how-to-tar-untar-file": {"title": "How to Zip and Unzip a tar.gz File", "text": "How to Zip and Unzip a tar.gz File If you want to extract a tar archive tar xf archive.tar.gz If you want to compress a directory tar czvf archive.tar.gz path to directory That s all.", "tags": "tar archive compress uncompress untar", "time": "2022-03-30", "name": "how-to-tar-untar-file"}, "pytorch-dataloader": {"title": "How to Create a Custom Pytorch Dataloader", "text": "How to Create a Custom Pytorch Dataloader First create a custom dataset class. from torch.utils.data import Dataset DataLoader class CustomDataset Dataset def init self features labels assert len features len labels self.features features self.labels labels def len self return len self.features def getitem self idx return self.features idx self.labels idx Next create a custom dataloader where we specify the batch size. features labels load data features labels must have equal lengths e.g. features 1 2 3 4 5 6 labels 7 8 dataset CustomDataset features labels dataloader DataLoader dataset batch size batch size shuffle True Finally iterate over the dataloader during training. for epoch in range num epochs for x y in train dataloader do stuff", "tags": "pytorch machine learning python", "time": "2022-04-28", "name": "pytorch-dataloader"}, "gomoku2049": {"title": "Creating an AI for Gomoku", "text": "Creating an AI for Gomoku Gomoku is a strategy game similar to tic tac toe but played on a larger board and with the goal of getting 5 in a row rather than 3. Since the game has perfect information and has simple rules I thought it would be a fun exercise in creating a game AI. In February 2020 I decided to code up Gomoku2049. The game is a demonstration of MiniMax which is an algorithm for finding the move which minimizes the opponent s best moves. This article is an overview of the game s technical highlights. Click here to try out the game Minimax with alpha beta pruning In the tree above the current game is shown on the left green to move. If green fails to block orange s 3 in a row now orange cannot be stopped. The Minimax algorithm represents every game as a tree of moves with the current game position at the root of the tree. The algorithm is recursive with exponential time complexity and can have a very high branching factor after the first move there are 225 1 224 possible moves. Because it is not feasible to evaluate all possible games to completion Minimax calculation is usually limited to a fixed depth after which the algorithm evaluates terminal leaf nodes using the gameover function and the static evaluator. After each human move known as plies Minimax assigns a score to each of the possible reply moves. By convention the AI will score favorable moves with a positive score and unfavorable moves with a negative score. The move corresponding to the highest score is then selected. In other words the AI is called the maximizer . Likewise the human is known as the minimizer. To determine the score of each possible move the minimax algorithm will recursively either maximize or minimize the possible moves available. After a given depth the evaluation will stop and return either an infinite value for an AI win for human win or a finite evaluation of the state of the board. This static evaluation can be rather expensive but luckily even a rough approximation is effective. In practice in addition to a depth limitation this minimax algorithm also reduces the branching factor by limiting the squares which will be evaluated to those which are adjacent to squares which have been played. Given the fact that a disconnected island square cannot immediately lead to a win this seems to be a reasonable simplification. At the leaf nodes of the tree either the game is over the human has won or the computer has won or the board needs to be evaluated with regards to who is winning. Alpha beta pruning Alpha beta pruning is an improvement on the minimax algorithm reducing the number of branches and leaf nodes which need to be evaluated. This is achieved by pruning unnecessary branches ignoring them because the parent minimizer maximizer would never choose it. For a maximizer whose parent is a minimizer this will occur if the parent minimizer has already seen a lower evaluation than a number the maximizing child sees. Static evaluator This function is used to evaluate a board position with regards to which player is winning and by how much. The MiniMax algorithm will then choose the highest value for itself while minimizing the options for its opponent. For gomoku it was important to derive an evaluation function which could be calculated quickly and which builds towards the final desired result of 5 squares in a row. Note that such a function would necessarily be isomorphic in four directions vertical horizontal and on both diagonals. My initial thought was that this would be extremely computationally expensive. There are many permutations of selected squares which can lead to a win and many which do not. For example XX OOO XX with O to move will lead to a win for O but with X to move will not. However I convinced myself that any static evaluation which built towards a win would find winning nodes at sufficient depth so finding extremely detailed evaluation was less important than a general approximation. Building from this thought I decided to count the number of 4 in a rows 4s and give them a high score along with the 3s and 2s. Each in a row would be given an exponentially increasing reward so that 4s scores much higher than 2 2s. For example the payout function might be f n 2 N for 2 3 and 4 so that f 4 16 and 2 f 2 8. This ensures the desired result that the optimal configuration of N squares is Ns. Eventually I determined that it was sufficient to simply count 2s with overlaps since allowing double counts would still favor longer sequences of squares but would not require separate checks for each length. Therefore if 2s was rewarded 1 then XXX would be rewarded 2 and XXXX would be rewarded 3. This means that 4s is still more the most efficient configuration of four squares since XX XX only evaluates to 2. Gameover function This function simply needs to return true if the game is over and a player has won. After the simplifications to the static evaluator the gameover function behaves almost identically. Instead of counting 2s we check for the presence of 5s. Bitmasks Here is where the fun begins. I realized that a very efficient way of representing a game board was with a sequence of bits where 1 represented an occupied square and 0 represented an unoccupied square. A game state would therefore only require a bit sequence for each player the game engine would prevent overlapping bits . For a 15 15 225 square board each player s occupied squares could be represented with a number 225 bits in length. Although Javascript Numbers are only 53 bits long Javascript has a newer primitive BigInt which can store numbers of arbitrary length. The biggest benefit of representing the game board this way is that it facilitates bitwise operations which drastically reduces the time complexity for the static evaluator and gameover functions. Here the mask is shown in white and the actual squares occupied are shown in orange. With each step in the bitmask check the board and the mask are bitwise ANDed together a very fast operation which reduces the computational complexity required in the static evaluator and gameover function. About BigInt The BigInt primitive is a newer built in type in Javascript and as such is unsupported in some browsers. In particular Internet Explorer and Safari do not have BigInt as a primitive. Although there are polyfills available for BigInt they do not have the same performance as the native type. I decided that as a demonstration of the Minimax algorithm supporting all browsers was not a priority. Web Workers Most people know that Javascript is single threaded. It is except when it isn t. Web Workers are a way of multithreading in the browser which in this context is pretty important because it helps to avoid freezing the user interface. In this game the board state is handed off to a Web Worker thread which computes the best move and returns it to the main thread. Progress is reported back periodically to the main thread as well which is shown in a progress bar underneath the Gomoku2049 logo. Theoretically I could have taken further advantage of multithreading when creating this game. Each branch in the decision tree can be parallelized allowing for simultaneous computation of each node s value. For example a new thread could be used to evaluate each of the AI s possible moves. Unfortunately the number of possible moves for the AI can be quite high later in the game and browsers limit the number of Web Workers allowed Chrome allows 60 Firefox allows 20 etc. so instead of spawning a new worker for each top level branch threads would need to be spawned from a shared thread pool. The full source code for this game can be found here.", "tags": "ai game theory javascript minimax", "time": "2020-05-18", "name": "gomoku2049"}, "how-does-hnsw-work": {"title": "How does HNSW work?", "text": "How does HNSW work Suppose we have a vector database with a billion items in it the haystack . And suppose we are looking for K vectors the needles which maximize some similarity function. In the case of cosine similarity or euclidean distance we may be maximizing 1 distance x y . And also suppose that we d like to do this quickly. Naive and semi naive approaches One approach might be to compare every vector and take the argmax. In that case for vectors of length D our runtime will be 1 billion x D. def search needle haystack arg k 1 normalize needle and haystack vectors needle np.linalg.norm needle haystack np.linalg.norm haystack axis 1 keepdims True compute cosine similarities similarities np.dot haystack needle get the indexes of the top k elements idx topk np.argpartition a k k return haystack vector with highest cosine similarity return haystack idx topk In practice there are some optimizations we can try which can increase the practical speed of this operation We can parallelize computations on device with SIMD or BLAS. We can parallelize computations on device with GPU. We can parallelize across cores for machines with multiple cores. We can parallelize across devices. For example partitioning the database across 10 clusters finding a winner from each cluster and then repeating the search across the 10 winners. There are also some relatively simple but destructive techniques we can also try We can try pruning the dataset in some way beforehand. For example maybe we don t need to consider old or unpopular vectors. We can try reducing the dimensionality of the vectors with something like PCA. We can try quantizing the vectors e.g. f64 f16 reduces memory requirements 4x. Usually this requires quantization aware training. Hierarchical Navigable Small Worlds HNSW Sometimes linear time is still too slow. In that case we can consider an approximate solution where some acceptable percent of vectors returned are actually not in the top K nearest neighbors. This is called approximate nearest neighbors ANN . If we have a large system it s possible to use another more precise model to rerank the vectors results anyways. In that case we just need to set K large enough to reliably include the vectors we really need. ANN algorithms typically come in three flavors graph based approaches space partitioning based approaches and hash based approaches. If your dataset is large enough you might add compression e.g. product quantization on top of it. Intuition HNSW is a bit of a Six Degrees of Kevin Bacon approach. I ll give what I think this is a fairly good intuition for how it works. Suppose you have 1 000 friends and you d like to know which five of them live closest to a landmark like the Golden Gate Bridge. You don t have everyone s exact address but each friend keeps a list of their close friends addresses. You might start with a friend who lives in the US. From there you check their friends to find who lives closest to the target and then search among their friends for someone even closer. By using a hierarchical approach you don t need to search through everyone just those likely to be closest. In the analogy the Golden Gate Bridge is the needle and the friends are the haystack . This is a pretty efficient method but of course it is possible to miss a node if their friend wasn t linked to an earlier friend. HNSW uses a multi layered graph for efficient nearest neighbor search. Searching Searching is fairly straightforward. Start from an entry point maybe a random point in the top layer. For each of the neighbors of the entry point compute the distance between the query and that point. If any of the neighbors are closer to the query hop to that neighbor and repeat the process. Select the closest point and go to the next layer down. We keep searching until we reach layer 0. To compute the K nearest neighbors during search with K 1 we simply maintain a min heap and add all nodes considered during the search. The number of nodes returned in this search K is also known as ef search in the HNSW paper. Addition To add a vector X we start by computing the highest layer L the node will appear in. L is randomly selected using an exponentially decaying probability. All nodes appear in level 0 but can have skip links to higher levels. Next starting from the top layer we traverse the graph structure as if we were searching for X. Once we reach layer L from earlier we begin to make connections. Connections can be made with previously found nearest neighbors or nearest neighbors on the current layer. The number of nearest neighbors to consider connections with is controlled by the ef construction parameter. The number of links to actually create is controlled by the parameter M. Performance considerations HNSW can require a large amount of memory. This can be reduced with product quantization a lossy vector compression method. Adding new elements is slow. There is a tradeoff between index quality and time required to build the index controlled by ef construction. More information about HNSW parameters can be found here.", "tags": "machine learning retrieval vectors", "time": "2024-05-20", "name": "how-does-hnsw-work"}, "grover-paper-summary": {"title": "Paper Summary: Defending Against Neural Fake News", "text": "Paper Summary Defending Against Neural Fake News Defending Against Neural Fake News by Zellers et al. presents a model for controllable text generation called Grover. This model can be used to create highly believable computer generated news articles. The authors present this paper as a method of detecting and preventing the spread of fake news. They claim their model is 92 accurate at detecting fake news stories partially due to artifacts that generators include in the generated text. Grover functions in an adversarial manner an adversary generates synthetic stories a discriminator identifies fake stories Current generative models are fairly good at creating realistic looking text but largely lack the ability to be controlled via tunable parameters. In contrast Grover models news stories as distributions like domain date authors headline body to be sampled from. Adversaries specify domain date and headline and the generator creates the body and author as well as a more appropriate headline. Grover samples from a joint distribution of the parts of a news article. Grover is built using a transformer architecture similar to BERT and GPT. Depending on the model size the number of parameters varies from similar to GPT to on par with GPT 2. The authors use the RealNews corpus resulting in 120 gigabytes of total file size. Training took 2 weeks on 256 TPU v3 cores. Grover is actually better at writing synthetic articles than humans are according to authors. The results of the authors subjective experiments show that humans have a hard time identifying Grover written propaganda. Using Mechanical Turk articles were rated according to stylistic consistency content sensibility and overall trustworthiness. Grover on the other hand turns out to be a fairly good discriminator. The authors also tested GPT2 BERT and FastText on the task of classifying news as human or synthetic. The researchers set up the experiment in an unpaired setting give a human synthetic classification for a single article and a paired setting determine which is the human and which is the synthetic between 2 articles . Unsurprisingly the paired setting was far easier. Also unsurprisingly the larger models perform better at discriminating when paired with smaller generators. For generators of the same size discrimination accuracy was around 90 91 . When comparing generator discriminator pairs with the same numbers of parameters classification accuracy was around 90 92 . Grover tends to leave artifacts when generating text and this fact may be part of the reason discriminators are so good at identifying synthetic text. For one the authors identify exposure bias as one of the reasons. The fact that Grover is never trained on generated text only on human authored articles seems to contribute to this. Perplexity also tends to vary over the length of the generated article and depending on the sampling variance may fall out of the distribution of human language.", "tags": "nlp gpt2", "time": "2021-09-19", "name": "grover-paper-summary"}, "python-format-string": {"title": "Python: Formatting a string", "text": "Python Formatting a string There are three main ways to format strings in python name Luke food pizza old style My name is s and I like s. name food str.format My name is 0 and I like 1 . .format name food f strings f My name is name and I like food .", "tags": "python quick reference", "time": "2021-02-24", "name": "python-format-string"}, "managing-python-environments": {"title": "Managing Python Environments", "text": "Managing Python Environments Need to switch between python versions often Use pyenv. Installing pyenv install pyenv curl https pyenv.run bash check pyenv install location which pyenv Install another python version see a list of available python versions pyenv install list check installed python versions pyenv versions installs python 3.7.5 pyenv install 3.7.5 Switch python versions use python 3.7.5 everywhere on your machine pyenv global 3.7.5 use python 3.7.5 in current directory pyenv local 3.7.5 use python 3.7.5 in current shell session pyenv shell 3.7.5", "tags": "python quick reference javascript minimax", "time": "2020-10-24", "name": "managing-python-environments"}, "sparse-autoencoder": {"title": "What are Sparse Autoencoders?", "text": "What are Sparse Autoencoders TLDR A sparse autoencoder is just a regular autoencoder that encourages sparsity with an L1 penalty or KL divergence loss rather than using a low dimensional bottleneck. If you understood all of those words above you may be interested in the OpenAI paper which used sparse autoencoders to interpret features from GPT 4. If not I ll try to break it down. What is an autoencoder An autoencoder is a machine learning architecture which contains two functions an encoder and a decoder. The encoder learns to create an efficient representation of the input similar to lossy file compression. The decoder learns to reconstruct the original input from the efficient representation. I have a couple of other pretty good explanations on this. In the first post I described how to build an autoencoder. Later I discussed their potential applications in vector retrieval. What is sparsity Sparsity just means that most of the values in a vector are zero. What is an L1 penalty L1 stands for penalizing the L 1 norm of a vector. In other words the sum of the absolute values of the values in the vector text L1 norm sum i 1 N vert x i vert What is KL divergence KL divergence is a measure of the distance one probability distribution has from another sum x in X P x log left frac P x Q x right Note that KL divergence is not symmetrical so distance isn t an entirely accurate description. If you re looking for a symmetrical measure of distribution similarity use Jensen Shannon divergence. Additionally X doesn t have to be a continuous variable although it often is. In the demo below you can see that as the probability distributions overlap more and more the KL divergence decreases. mean 40.0 variance 20.0 What is a KL divergence loss KL divergence loss tries to force a model s predicted distribution to match a target distribution. Common applications of KL divergence loss are t SNE dimensionality reduction and generative models like variational autoencoders and generative adversarial networks.", "tags": "deep learning llm", "time": "2024-06-06", "name": "sparse-autoencoder"}, "siamese-nn-video": {"title": "Siamese Neural Networks (Video)", "text": "Siamese Neural Networks Video The following is a transcript of the above video In this paper the authors present a novel neural network architecture to enable audio search via sounds humans are able to make for example humming and whistling. This is an important capability when searching through audio for a specific sound. Motivation Imagine you have hundreds of unlabeled sound effects on your computer and you are looking for a specific one. It could be very tedious to listen to every single one until you can find the right sound. Even if the sounds do have some kind of word labels it could be hard to pinpoint exactly which words to search for. A lot of sounds don t exactly lend themselves to text descriptors so finding the right sound can be difficult with a text search. This paper contains three main contributions First it introduces a new neural network architecture for matching imitated sounds with a sound corpus the semi siamese convolutional neural network. Second the researchers built a second architecture which utilizes transfer learning from other audio tasks in an attempt for better performance. Third the researchers visualized and sonified input patterns which excited the neurons in different layers. Both neural networks outperform the state of the art systems as we will see later on. Data To train the siamese model the authors used a dataset called VocalSketch which contains sounds from 4 broad categories acoustic instruments commercial synthesizers everyday and single synthesizer notes. The dataset also contains a number of human vocal imitations of each of the sounds. For each of the 4 categories in VocalSketch the researchers selected half of the sounds in each category along with corresponding vocal imitations as the training and validation set and the other half for their test set. Since each of the categories other than Everyday contained 40 sounds 20 sounds would be for training and 20 for test and validation. The everyday category contained 120 sounds so that category had 60 in training and 60 in test. Each sound in the dataset has 10 corresponding human vocal imitations. In the training set 7 of them for each sound were selected for training and the remaining 3 were used for validation. Overall the researchers used this dataset to create 840 positive and 840 negative pairs for training and 360 positive and negative pairs for validation. I thought it was interesting that the researchers opted not to use balanced categories of 20 sounds for each category. There isn t a comment in the paper about the reasoning behind this but it may be due to the difficulty of categorizing this class of sounds. The transfer learning model required pretraining the two towers on two additional datasets before training the full network on VocalSketch. For the vocal imitation tower they used a dataset called VoxForge. From this dataset the researchers selected 8 thousand samples for each of 7 different languages. They used a 70 30 split for training and testing and achieved a 69.8 accuracy which seems pretty good for a 7 class classification task. For the second transfer learning tower the environmental sound classification tower they used a dataset called UrbanSound8k. This dataset contains 8732 sound samples in 10 different classes things like car horns jackhammers and street music. The researchers used 10 fold cross validation when pretraining on this dataset and achieved a 70.2 accuracy over the dataset. Note that for pre training the two towers the researchers used a slightly modified neural network architecture appending two fully connected layers to categorize the results into the necessary number of classes. So how are sounds fed into the neural networks You re probably familiar with the way that convolutional neural networks work with images. Typically the input for each image is of a shape width by height by color depth. Creating an input when working with sound is similar. The width of the audio file in this case is time and the height is the frequencies during that time step. This is similar to the output of a spectrogram. In order to be fed into the network audio must first undergo a preprocessing step. The specific preprocessing involved varied between the networks but generally involves downsampling the audio and splitting it by frequency band. This resulted in an input which resembles a spectrogram image. Methodology The heart of this problem is an architecture the researchers dubbed siamese style or semi siamese neural networks. A true siamese neural network consists of two identical neural network towers with the same weights and is used similar to the way that hashing is used to match similar inputs. A siamese style network is similar to a siamese neural network but may have different weights in one of its towers. The first network the researchers called IMINET which included a true siamese neural network as one of its configurations. This network consisted of two convolutional network towers a concatenation step and a fully connected network with three more layers to compute a similarity score between 0 and 1. The convolutional neural networks in IMINET had the same structure even though in some configurations their weights were not the same. They consisted of four convolutional layers with pool layers following convolutional layers 1 and 2. The researchers detailed the parameters they used for each of the convolutional layers specifically the number of filters and the receptive field size. Each of the filters in a layer learns to detect various characteristics of the input and the receptive field is the part of the input that the filter is able to see. The max pooling layers output the maximum value among all of their inputs reducing the number of inputs for the next layer in the network. The researchers experimented with three different configurations for the convolutional network towers a tied configuration where both towers would share the exact same weights and biases an untied configuration where the two towers were required to share no weights and biases at all and a partially tied configuration where weights and biases were shared for convolutional layers 3 and 4 but not 1 and 2. Because the untied and partially tied configurations are not truly siamese neural networks the researchers called these configurations semi siamese. After both inputs pass through the convolutional towers they are concatenated together into one input vector and fed into a fully connected network with three layers. This network s job is to compute a similarity score between 0 and 1 for the two inputs. Both of the first two layers used ReLU activation. The final layer was a single neuron which used sigmoid activation to squash its input into an output between 0 and 1. This architecture achieved state of the art results in sound retrieval which I will discuss in a moment along with the rest of the findings of this paper. The second neural network developed by researchers was called TL IMINET. This network was very similar to the first network but this time the researchers tried using transfer learning to achieve better performance. The researchers hypothesized that the vocal imitation task shares many characteristics with language identification and that sound recognition shares many similarities with environmental sound classification. This would allow networks which were pre trained on these tasks to require only fine tuning to be adapted to this task. The network architectures for the language identification and environmental classification tasks were slightly different from those used in IMINET and are shown here. Note that the towers are also different in architecture from each other. The researchers also experimented with fusion strategies between different models. For IMINET the similarity likelihoods for all three configurations were multiplied together to achieve a combined score. They also experimented with combining IMINET with the previous state of the art model. Since that model computes a cosine distance between the input sound and a candidate sound this output was converted into a likelihood using softmax and that softmax was multiplied by the output of IMINET. The transfer learning model TL IMINET was also combined with the state of the art model in a similar way by computing the softmax and multiplying by the output of TL IMINET. These fusion strategies ended up improving the performance of each of the models quite a bit. Experiments and Findings To measure the performance of these networks recall that the output of the networks was a number between 0 and 1 indicating how similar the network believed the two inputs were to each other. For example two very similar inputs might have a similarity rating of 0.9 while two dissimilar inputs might have a similarity rating of 0.1. After gathering the similarity of the human vocalization sound to each of its potential matches the matches were ranked according to their similarity score. The authors then used a metric called mean reciprocal rank which is a number between 0 and 1 indicating how well the algorithm ranked the sounds. For example a mean reciprocal rank of 0.5 suggests that the target sound is ranked second among all possible matches on average. Here are the performances of the various network configurations when measured by mean reciprocal rank. The researchers highlighted several insights which could be drawn from their results. First it seems that tied configurations performed the best among all configurations of IMINET. This runs contrary to the researchers expectations that untied configurations would outperform tied configurations. Second the tied configuration outperformed the previous state of the art benchmark in two categories of sounds commercial synthesizers and everyday. It performed worse than the state of the art for acoustic instruments and was about the same performance for the single synthesizer category. Third IMINET achieved even better performance in most categories by using an ensemble of different configurations. Fourth even without pretraining the TL IMINET model performed better than the untied configuration of IMINET for all categories except commercial synthesizers. This is interesting because the only difference between these two models is the network structure of the convolutional towers. And finally the pre trained TL IMINET model outperformed the previous state of the art model by quite a bit in all categories but the best performing configuration overall was TL IMINET fused with the previous state of the art model. One of the most interesting experiments was the visualization of the input patterns which activate neurons in each of the layers. This was done by performing gradient ascent of the neuron activation with respect to the input from a random initialization state. Visualizing the convolutional layers showed that the first layer tends to learn local features like edges intermediate layer neurons learn more complex features like texture and direction and the deepest layer recognizes concentrations of frequency ranges. The visualizations also helped to confirm that pretraining indeed helped the networks to learn more detail. The patterns from the pretrained vocal imitation tower were sharper than those in the naive IMINET towers. Takeaways There are a few key takeaways from this research. The first is that the transfer learners had much better performance than the naive non transfer learner as evidenced by the fact that TL IMINET performed better than IMINET for most categories even though neither model was pretrained. The research also showed that ensemble methods can outperform any single model on its own. IMINET performed better when used in combination with its different configurations and combining it with the state of the art model performed better than either model on its own. Finally visualizing the inputs can help to confirm that the network is learning the correct things and helps to provide insights as to what types of sound properties are important.", "tags": "neural networks audio video siamese neural network", "time": "2020-12-17", "name": "siamese-nn-video"}, "connect-jupyter-to-remote": {"title": "Connect Jupyter to Remote", "text": "Connect Jupyter to Remote Here s how to connect to a remote Jupyter notebook. Create an ssh tunnel to your remote machine ssh L 8080 localhost 8080 user 12.34.56.78 or use a .pem file to connect to ec2 ssh L 8080 localhost 8080 i aws.pem ec2 user ec2 12 34 56 78.compute 1.amazonaws.com Start Jupyter on that machine in headless mode jupyter notebook no browser port 8080 Use a browser to open one of the urls that Jupyter presents http localhost 8080 token xyz", "tags": "python jupyter quick reference", "time": "2021-09-07", "name": "connect-jupyter-to-remote"}, "running-simple-language-model": {"title": "How to Train and Run a Simple Language Model", "text": "How to Train and Run a Simple Language Model This article will show how to run a simple language model KenLM. It s not as powerful as transformer based models like BERT or GPT 3 but depending on what you re trying to accomplish it may be more than enough. This tutorial should take you about 15 minutes including the time to run the scripts. Let s work backwards from where we re trying to get to. When you ve finished you should be able to run the following script import kenlm path path to model.arpa lm kenlm.LanguageModel path sentence I am not superstitious but I am a little stitious print model.score sentence The first step will be to build KenLM. Then we will build the ARPA file which KenLM uses to evaluate. Building KenLM First clone this repository git clone git github.com kpu kenlm.git Now we need to build the KenLM toolkit. Run the following to build mkdir p build cd build cmake .. make j 4 Now we just need to provide the model with a .arpa ngram language model file. So let s get one. Building an ngram language model from Wikitext 2 First let s clone a repository which will build an ARPA file. This repository builds the ngram file from Wikitext 2 a common dataset used in natural language processing. git clone git github.com daandouwe ngram lm.git cd ngram lm mkdir data . get data.sh mkdir arpa . main.py order 3 interpolate save arpa name wiki interpolate Once that has finished you ll have new .arpa in the arpa directory you created. This script took the longest to run on my machine. Be patient your computer is busy reading all of Wikipedia. All Together Now Now we re finally ready to evaluate a sentence with the language model. import kenlm path path to model.arpa lm kenlm.LanguageModel path sentence I am not superstitious but I am a little stitious print model.score sentence Which prints something like 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 24.47921371459961 Now if you re interested in a bit more information about what s going on you can add this bit at the bottom words s sentence.split s for i prob length oov in enumerate lm.full scores sentence print f prob length .join words i 2 length i 2 if oov print f t words i 1 is an OOV for w in words if not w in lm print f w is an OOV Which adds this to your output 3.1138248443603516 2 s I 1.1560251712799072 3 s I am 1.1645264625549316 3 I am not 4.912360191345215 1 superstitious 4.504511833190918 1 but 2.2214112281799316 2 but I 1.1531075239181519 3 but I am 1.2614283561706543 3 I am a 0.9001830816268921 3 am a little 1.2325057983398438 3 a little stitious stitious is an OOV 2.8593297004699707 2 stitious s stitious is an OOV To the left of each term is the log base 10 probability of each term occurring. For the first term s I means start of sentence followed by I which the model has assigned a log probability of 3.11. That s around 0.00078. You might think it s strange that a sentence beginning with I is so unlikely but we are using Wikitext 2. Wikitext 2 is Wikipedia articles. Not a lot of sentences on Wikipedia begin with I . Notice that stitious is an OOV out of vocabulary term here. Clearly the language model doesn t appreciate humor. We ll have to tackle that next time.", "tags": "nlp", "time": "2021-04-16", "name": "running-simple-language-model"}, "connect-to-colab": {"title": "Colab: Connect to Google Drive", "text": "Colab Connect to Google Drive Here s how to connect your Google Colab notebook to your Drive directory from google.colab import drive drive.mount content gdrive Follow the prompts from there. That is all.", "tags": "python quick reference", "time": "2021-06-30", "name": "connect-to-colab"}, "read-write-json": {"title": "Python: Read & Write Json", "text": "Python Read Write Json Often it is useful to save python data to json files. The following code will demonstrate how that can be done. God bless JSON a soon to be famous programmer import json data a 1 b hello c False filename awesome data.json write data to file with open filename w as f json.dump data f read json from file with open filename r as f data json.load f print data prints a 1 b hello c False", "tags": "python json quick reference", "time": "2021-03-07", "name": "read-write-json"}, "what-is-temperature": {"title": "What is Temperature in NLP?\ud83d\udc2d", "text": "What is Temperature in NLP Temperature is a parameter used in natural language processing models to increase or decrease the confidence a model has in its most likely response. In my opinion the most intuitive way of understanding how temperature affects model outputs is to play with it yourself. If you re interested in the mathematical details I ve included them below but I won t be offended if you just want to play around with the slider . Temperature 25.0 What s going on Suppose we have a language model which predicts the last word in the sentence The mouse ate the . Given the previous words in the sentence and its prior training our language model will try to fill in the blank with a reasonable final token. Suppose those raw outputs are as follows token logit cat 3 cheese 70 pizza 40 cookie 65 fondue 55 banana 10 baguette 15 cake 12 These outputs make sense. A mouse probably eats cheese but mice are also known to eat cookies. A mouse probably wouldn t eat a baguette unless it was a French mouse. Since these are the raw outputs of the model they won t sum to 100. To normalize these values we typically use softmax z i e z i over sum j 0 N e z j When modulating with temperature we introduce an additional temperature variable which affects the softmax distribution. A higher temperature excites previously low probability outputs. A lower temperature lowers the smaller outputs relative to the largest outputs. To accomplish this we replace each zi in the formula above with the quotient zi z i e z i over over sum j 0 N e z j over Higher temperatures make the model more creative which can be useful when generating prose for example. Lower temperatures make the model more confident which can be useful in applications like question answering.", "tags": "nlp", "time": "2021-04-01", "name": "what-is-temperature"}, "how-does-convolution-work": {"title": "How Does Convolution Work?", "text": "How Does Convolution Work Convolutional neural networks have had breakthrough success in image recognition natural language processing and even board games like Chess and Go. But what s really going on during convolution Well I think the easiest way to explain is with an interactive demo. Feel free to play around with the parameters below to see for yourself number four three eight padding kernel size stride speed You can use the settings above to control the hyperparameters of the convolutional layer. As you ve probably figured out the left side shows the input a 20x20 handwritten digit and the right side shows the output as it is being drawn. A convolutional layer will pass a filter over the input selecting only a small group of pixels at a time. Not shown once the filter selects a window of pixels it will compute a dot product with the pixel values and its own weights. In this demo the filter weights are assumed to be 1 for simplicity. More on the hyperparameters Sometimes the settings will result in an invalid convolution. This happens if the kernel extends beyond the activation map. We can write a python function to check whether it will be valid def valid convolution input size kernel size padding stride if input size 2 padding kernel size stride 0 return True else return False Padding Increasing the padding will add extra pixels typically zeros around the edges of the input. This is done in order to ensure that the layer parameters will be valid. Additionally it causes pixels which would have been on the edges to be more to the middle of the activation map. Kernel Size This refers to the sliding window which passes over the input. Choosing a larger kernel tends to increase the smudging which happens in the output while a smaller kernel preserves more information for deeper layers. Sometimes kernel size is referred to as filter size. Stride Stride refers to the distance the sliding window will move between steps. A stride of 1 moves the window one pixel to the right while a stride of 2 moves the window 2 pixels. A larger stride can reduce output layer size at the expense of granularity.", "tags": "computer vision", "time": "2021-06-14", "name": "how-does-convolution-work"}, "alphabet-chess": {"title": "Alphabet Chess", "text": "Alphabet Chess TLDR Alphabet chess is a chess variant that allows handicapping by mixing in a bit of poker into the beginning of the game. Moves must be played according to a secret word at the beginning of the game. Chess has been played in different forms since the seventh century and in its modern form since the nineteenth century. Opening theory i.e. the study of the best moves to begin the game with has been developing since then. The EGG Opening I was inspired the other day after watching a video by Eric Rosen called Quadruple Egg . Chess boards are usually notated from left to right with the letters A through H. These columns are called files . The egg opening therefore involves moving the E pawn then the G pawn twice spelling the word egg . It s an extremely unconventional opening but how bad is it compared with unrestricted openings What are the best i.e. least bad alphabetic openings We can call the Egg Opening one version of an alphabetical chess opening an opening where the piece moved must start on a file corresponding to the next letter in a given word. In this definition the pieces don t have to be pawns. This raises an interesting question what are the best and worst alphabetical openings Under engine evaluation they will all be losing because the opponent is not limited in which piece they can move. However as we will see some are much worse than others. Four letters English doesn t have an official dictionary so I chose this list of four letter words. From there we need to eliminate all words with letters after H. This leaves us with 37 words. Don t ask me what they all mean abbe abed aced ache aged agha baba babe bach bade bead beef cafe cage ceca cede chad chef dace dada dead deaf deed each edda edge egad face fade feed gaff gaga gage geed ghee head heed A simple way of measuring how bad each of these is would be to evaluate them against Stockfish to see how badly we re losing after playing these moves. In this simulation we play as white each time. This measurement approach has a problem though Stockfish is assuming that white can make any move and playing black s moves accordingly. In reality white s moves are extremely restricted. If black knew about this restriction he would be much more aggressive. Further if black were to discover which word white is playing he could punish white. And even if black doesn t know exactly which word white is playing black could eliminate moves which definitely can t form English words. For example if white s first move is H the next move will definitely be E. Alphabetical Chess What if both players were required to play their first four moves from dictionary words And what if those words were pre assigned but secret to each other For example if each player drew a card from a deck. In this game if a player is unable to make a move with a piece of the correct letter they lose. There are a few interesting features of this chess variant Your starting word will have a big impact on how the game will turn out. Figuring out your opponent s word will give you a big advantage. Checks and other attacks can be devastating. You may not be able to respond to the attack and checks may end the game immediately. In that case it isn t necessarily true that the best and worst words from above will still be the best and worst when pitted against another randomly chosen word. It s a rock paper scissors situation depending on which word A you happen to get there is another word B which best counters your word and another word C which your word best counters. Here the y axis shows the word which was played with the white pieces and the x axis shows the word played with black. Cool Things About Alphabet Chess There are a few interesting benefits to this variant Handicaps are quite easily built in. The longer your secret word is the more restricted your play is and the more of a handicap you will have. An element of gamesmanship and imperfect information. Players can win on the board with good moves but also off board by figuring out their opponent s secret word. Prevents good players from using memorized openings. Play Alphabet Chess If you d like to try it out with a friend and a chessboard here s an online tool that ll pick a random word https alphachess.surge.sh", "tags": "chess game theory statistics", "time": "2022-06-10", "name": "alphabet-chess"}, "monte-carlo": {"title": "Estimating Pi with a Monte Carlo Simulation", "text": "Estimating Pi with a Monte Carlo Simulation A Monte Carlo simulation is a method of estimating events or quantities which are difficult or computationally infeasible to derive a closed form solution to. The value of the mathematical constant Pi is a good example of this although it is possible to calculate the exact value of Pi a good estimate is easily demonstrated with just a few lines of code. Monte Carlo simulations work when the input can be drawn from a random probability distribution and the outcome can be derived deterministically from the input. In the case of calculating Pi this can be modeled geometrically. The random distribution is all points within the square and the outcome is whether a selected point lies within the circle inside of the square. We know that for a square circumscribed about a circle Area circle pi r 2 A square 2r 2 4r 2 frac Area circle Area square frac pi r 2 4r 2 frac pi 4 4 frac Area circle Area square pi If we notice that the probability that a randomly placed dot will fall within the circle is the same as the ratio of their areas i.e. the circle takes up about 78 of the area of the square so a random dot has about a 78 chance of landing inside the circle then multiplying that probability by 4 gives Pi. By placing dots randomly we play out that probability in real time. As to whether a given dot lies within the circle we simply use the Pythagorean theorem to calculate its distance from the origin sqrt x 2 y 2 1 In other words all dots greater than 1 unit from the origin are outside the circle. Below is a simulation of the derivation of the value of Pi. Click start simulation to see for yourself. Warning this simulation may become slow once many dots are drawn on the screen. start simulation stop simulation Number of points 0 inside circle 0 Approximate value of pi 0 Next I was interested in the way that this simulation would play out once the number of points became large. Intuitively this should approach Pi but doing so requires that the random numbers generated by browsers be evenly distributed. start simulation stop simulation Number of points 0 Approximate value of pi 0 error 0", "tags": "monte carlo statistics geometry javascript", "time": "2020-07-09", "name": "monte-carlo"}, "dual-encoders-ranking": {"title": "Paper Summary: Dual-Encoders in Ranking", "text": "Paper Summary Dual Encoders in Ranking In Defense of Dual Encoders for Neural Ranking by Menon et. al. discusses the question of why dual encoder DE models also called Bi Encoders elsewhere don t match the performance of cross attention CA models. The authors investigate what is actually going on and demonstrate some improved performance over baseline DE models with a new model distillation method. Background Search requires an automatic way to find the most relevant documents to a query. There are bag of word approaches to this task for example BM25 and neural approaches. An example of a bag of words approach might simply be to count the number of similar words between the query and each document and return the document with the highest number of similar words. There are word stuffing issues with this idea but the larger issue is that a bag of words strategy can t account for synonyms. If I search for bad guy I will never find villain without some additional logic to account for this. A neural network implicitly understands the relationship between words and avoids the fragile logic of simple word counts. The idea of a neural encoding approach is pretty simple. For each document in your corpus pass the query a document into a function which will return a similarity score between 0 and 1. Then just sort the documents by that score. There are two main architectures for doing this dual encoders and cross attention models. Dual encoder architectures can precompute document embeddings but tend to be less accurate than cross attention models. The great thing about DE models is that document embeddings can be computed ahead of time. When users enter a query only that query embedding needs to be calculated and then compared with the embeddings already calculated for each of the documents. It s a lot faster. So much faster in fact that CA is generally not used for initial retrieval only for reranking afterwards. However DE models tend to be less accurate than CA models. It would be great if it was possible to transfer some of the benefits of CA models to DE models. The Problem It s unclear whether the shortcomings of DEs are due to the DE model s capacity or because of its training procedure. DEs may be overfitting. DE models can match CA model training performance but are often lower during evaluation. Dual encoders can also be improved by distillation of which there are two kinds Logit matching. Try to match embeddings between teacher and student. Probability matching. Try to match the softmax probabilities between teacher and student. CA models have better separation between positive and negative examples strongly predicting negative examples. DE models have more overlap between positive and negative predictions. Normalizing the margins between positive and negative predictions higher is better CA models clearly have better performance. The distilled model is slightly better than the DE model. Part of the cause of this discrepancy may be the fact that DE models have noisier updates. DE models may have difficulty modeling negative scores since updating their weights on positive q d pairs can inadvertently increase scores for negative q d pairs. Dropout also doesn t seem to mitigate overfitting. The evolution of scores for five positive and five negative documents for a fixed query. Scores from the CA model separate much more smoothly than in the DE model. Solutions Previous work has tried to improve training procedures in several ways Adjusting the scoring layer. Usually embeddings are scored with a simple dot product but a more sophisticated scoring function may be able to capture more information at the cost of inference speed. Distilling predictions from CA models. Model distillation uses a teacher student framework where the smaller student model attempts to mirror the teacher . This paper explores a new approach to distillation. The authors introduce multi margin MSE loss M3SE M3SE loss attempts to match the margins of score differences between teacher and student. For performance reasons however rather than matching each margin it only encourages the student to be less than or equal to the teacher s highest negative score. M3SE can be seen as an extension of Margin MSE loss where instead of matching logits it matches raw scores. It can also be seen as a smooth approximation of softmax cross entropy loss. The authors also highlight parallels between M3SE and RankDistil. Results Apart from TREC M3SE distillation appears to nearly close the gap with cross attention models. Distilled models are 6 layer BERT models with embedding size 768.", "tags": "machine learning search", "time": "2022-12-17", "name": "dual-encoders-ranking"}, "chess-engine-history": {"title": "The Chess Engine\u2019s Final Horizon", "text": "The Chess Engine s Final Horizon This is part 1 of a paper I wrote for Ken Forbus Qualitative Reasoning course adapted for this blog. You can find a printable version of the paper here and part 2 here. Computers that play chess otherwise known as chess engines have existed since at least the late 1940s. Because the game was said to require the perfect combination of planning strategy psychology and calculation chess was once thought to be an activity directly correlated with intelligence and that only a truly intelligent computer should be able to defeat humans. However as a recent chess.com report explains computers are now far stronger than humans Human chess and computer chess are different even at the highest levels. The best humans play at an Elo rating of 2800. Stockfish the most powerful chess engine has an estimated rating of more than 3500. In a theoretical match between World Champion Magnus Carlsen vs. Stockfish we estimate that it is most likely that Magnus Carlsen would lose every single game no wins and no draws. The supremacy of Machine over Man should be marked as a great triumph in artificial intelligence and in a broad sense it is. However I would argue that playing skill should not be the only goal and a truly useful engine should be interpretable as well. We ve already solved the problem of playing chess at an extremely high level. Yet for historical reasons interpretability was sacrificed for the sake of speed and playing strength. However these tradeoffs may not make as much sense today. Part 2 discusses opportunities and clear benefits for reevaluating those decisions moving forward. Background The general algorithm for performing search in zero sum perfect information games like chess is known as minimax. The algorithm attempts to find the move which minimizes the opponent s maximum i.e. best move. Minimax visualizes the possible future state of the game as a tree and the value of each node in the tree as a function of the nodes following from it. If you could compute the full game tree in simple games like tik tac toe this is possible you would be able to enumerate all possible terminal nodes and propagate the result 1 for a win 1 for a loss 0 for a draw back up the tree. That s not possible in chess. The game tree is too big. Instead the value of many of the nodes needs to be estimated using heuristics so that they can be propagated upwards. Therefore a fast and accurate static evaluation function is critical. However since the problem is recursive the problem of creating a perfectly accurate static evaluator is as hard as evaluating that node s entire associated game tree. Minimax is often augmented with alpha beta pruning to reduce the number of positions which will be evaluated. This effectively cuts the computational complexity exponent in half by removing from consideration those branches which cannot affect the final result. Other challenges in minimax evaluation exist as well. In particular a phenomenon dubbed the Horizon Effect is a peculiar failure mode of minimax searches. The Horizon Effect was first described by grandmaster and computer scientist Hans Berliner in 1975. His illustration of the problem his Figure 1.3 is reproduced in Figure 1. Algorithms that do not account for the Horizon Effect will try to push bad outcomes of their search beyond their search horizon instead opting to make hopeless moves which only serve to delay the inevitable. As humans we intuitively understand this illusion. Figure 1. White to move. Here white s bishop on a4 is doomed attacked by black s pawn on b5. White could delay the inevitable by moving his bishop to b3 but then black simply seals the bishop s fate with pawn to c4. In that position white does not have time to save his bishop and it will be captured no matter what on the next move by the pawn on c4. Due to the Horizon Effect at a limited depth white will not recognize this and will play hopeless moves like pawn from e4 to e5 temporarily attacking the knight but easily parried by capturing with the pawn on d6. This phenomenon is deemed the Horizon Effect because by pushing negative outcomes beyond the horizon of calculation depth the engine is able to trick itself into believing that the problem doesn t exist. A true case of see no evil . Branching Factor In any given position there may be many possible moves. By some estimates chess has an average branching factor of around 30 with other estimates putting the number around 40. It isn t easy to find a concrete value for the branching factor of chess. One source claims without citation that the branching factor may be up to 40 see pg. 117 . However a more recent statistical analysis of 2.5 million chess games put the real number closer to 31. The branching factor depends on the stage of the game middle games have far more available moves than end games. Because not all games are the same length shorter games will tend to have higher average branching factors than longer ones. Barnes graphic is reproduced above. In any case it is possible to dramatically reduce the branching factor by employing selective search which means excluding nodes from recursive tree search altogether. Reducing the number of possible moves that will be explored in any given position promises dramatic computational speedups allowing the tree to be searched deeper at the expense of width. Given the slow speeds by contemporary standards of early computers the allure of such a technique should be clear. It is also a more psychologically plausible way of playing since most players quickly rule out ridiculous seeming moves and focus on the most promising ones. Early Chess Engines An early attempt to narrow the search tree was proposed by Berliner who in 1975 devised CAPS II which utilized a tree search algorithm with five total reference levels including ALPHA and BETA. His paper also was cognizant of the work of Chase and Simon from two years prior recognizing the need for a bottom up board representation. Unfortunately his board representation was largely geometrical and included little in the way of qualitative relationships between pieces. Nevertheless the resulting program was able to solve tactical puzzles in a quite impressive manner for the time. Another example of the use of such a narrowed search tree is PARADISE PAttern Recognition Applied to DIrecting SEarch . This system used a knowledge base containing 200 production rules to match board patterns and determine the best move at any time. An example production rule is shown in Figure 2. Figure 2. A sample production rule from the PARADISE knowledge base. This production rule detects and acts upon opponent s trapped pieces. A trapped piece is identified as a non pawn defensive piece which cannot move to another square without being captured and is not already attacked. Finally the production rule describes the threat of this action winning the piece and how likely it is to succeed. Because the tree search was narrower the system was able to search to higher depths. Still because of hardware limitations of the time PARADISE was extremely slow generating only 109 nodes in 20 minutes of search time. Current chess engines evaluate millions of nodes per second. PARADISE executes static analysis on a position using its many production rules with the ultimate goal of creating and executing a plan. Matched production rules post concepts to the database in addition to information about the intentions of the concept and its likelihood of success. The program is then able to use this information to form a plan which is a set of actions for the side to move along with corresponding plans for each defensive alternative. Because there may be many potential alternatives at each move this plan includes many branches. Figure 3. White to move PARADISE has produced a plan which involves checking the black king by moving the knight to g5 then checking the black king by moving the rook to d7. Depending on black s next move white will then try to either capture the queen on d4 or the rook on d7. An example of such a plan is reproduced in Figure 3 above. An interesting sidebar about this position is that based upon Stockfish analysis this position results in inescapable checkmate for black within 15 moves. That is even with optimal play black will be checkmated in 15 or fewer moves. The winning sequence begins with white moving his queen to e5 exploiting the pin on the f6 pawn from the f1 rook. Searching only selective lines is more difficult to implement than full width search. Further selective searches may miss important continuations of a position causing the computer to select an incorrect move. It was for this reason that Berliner himself an original proponent for the application of strict logical rules in chess decided to seek brute force search methods instead. Modern Chess Engines Stockfish is currently the 1 ranked chess engine in the world. Stockfish is open source and performs a full width search on the game tree. Leaf nodes are evaluated using either a classical hand crafted evaluation heuristic or more recently a neural network evaluator called NNUE. The classical evaluation function uses a set of around 30 factors weighted empirically using a dedicated testing framework called Fishtest. Altogether the project has around 200 contributors. Optimized for speed Stockfish can evaluate around millions of nodes per second on a typical 4 core computer. Stockfish analyzing the starting position on my laptop at around 1 million nodes per second. Neural networks may also be used more directly. Leela Chess Zero also known as Lc0 is another open source chess engine modeled after DeepMind s AlphaZero chess engine. Lc0 uses Predictor Upper Confidence Bound tree search PUCT to search its game tree. Leela evaluates new nodes by iteratively choosing moves from a probability distribution until it reaches an unexplored node. At that point Leela s neural network estimates the node s value and propagates that value back up the tree. PUCT is very similar to Monte Carlo Tree Search but with game rollouts replaced by neural network evaluations. One other notable engine is Maia Chess. Maia is interesting for two reasons. Unlike most other engines Maia s objective is to model human behavior rather than to perform optimally. This is interesting because Maia is in many cases trained on suboptimal data. The other notable feature about Maia is the unusual manner in which moves are selected instead of performing any type of tree search at all the engine simply returns the neural network s static evaluation of the current board position. Suffice it to say however that despite computation no longer being a significant limitation prior qualitative approaches still have garnered relatively little attention. Conclusion Engineers have historically sought to build the most powerful chess engine possible. With the singular goal of defeating human and now computer opponents explainability was an expedient but understandable tradeoff to make. In the present day however there is no longer a question of whether humans or computers are superior chess players. It seems quite clear that the time has come to revisit some of the tradeoffs made in the past. In part 2 I will discuss some clear benefits of a more interpretable chess engine and some possible routes of getting there.", "tags": "chess ai explainability", "time": "2022-10-07", "name": "chess-engine-history"}, "best-antimaia-games": {"title": "My Favorite Antimaia Games", "text": "My Favorite Antimaia Games This is a follow up to When Suboptimal Minimax is Better. After running 400 simulations I can conclusively say that opponent modeling is pretty cool. The TLDR on opponent modeling is that if we have a pretty good idea of what the opponent might do we can beat them faster by playing moves which aren t objectively optimal as far as minimax is concerned. Here Maia 1900 is a model of a relatively high level chess player. Antimaia 1900 is specifically designed to counter Maia 1900. White Black Games White win Average half moves per game Antimaia 1900 Maia 1900 100 100 37 Maia 1900 Antimaia 1900 100 0 43 Maia 1900 Stockfish 10 100 0 67 Stockfish 10 Maia 1900 100 100 70 Maia 1900 uses Maia Chess weights trained on games from players rated near 1900. Antimaia 1900 uses Maia 1900 weights and Stockfish 10. Stockfish 10 is Stockfish 14.1 evaluated at depth 10. As you can see Antimaia finishes its games much more quickly 12 16 moves faster . This demonstrates that Antimaia is exploiting weaknesses in Maia s policy as measured by Stockfish and playing high risk high reward moves which usually pay off. Selected Games I had to keep reminding myself that even though these games look ridiculous Maia plays poorly because Antimaia is finding moves where Maia is most likely to mess up. Antimaia vs Maia Game 15 Smothered mate comes out of nowhere. Game 24 Antimaia lays a poison pawn trap then waits patiently for 2 moves before Maia falls for it blundering a queen. Game 32 The shortest game. Another nice checkmate delivered by the knight. Game 59 Maia blunders a piece and then falls for Antimaia s poison rook to get checkmated. Game 75 Promoting to a knight not because it s the best move but because it doesn t matter. Game 88 Antimaia makes the crazy move Bh7 but Maia blunders into checkmate by taking with the knight. Maia vs. Antimaia Game 2 Queen takes e3 is objectively a horrible move but Antimaia gambles correctly that Maia won t know how to respond. Game 7 Antimaia baits Maia into blundering a queen and mate in 3. Game 51 Antimaia tricks Maia with a poison pawn. Game 59 Maia manages to lose in 5 moves. Game 69 Antimaia swindles a queen. The rest of the games I added the rest of the games to github https github.com lukesalamone antimaia games", "tags": "chess ai game theory", "time": "2022-11-26", "name": "best-antimaia-games"}, "self-attention": {"title": "A Few Notes on the Transformer", "text": "A Few Notes on the Transformer A self attention block depicted as a neural network. In this post I will describe the attention mechanism commonly used in transformers a popular neural language architecture. Most of the most well known large language models of late are based on the transformer architecture. Attention was first described in Attention is All You Need by Vaswani et al. What is attention At a high level attention is a mechanism for neural networks to boost portions of an input which are relevant and ignore those which aren t. In language models attention is used as a way for the model to learn which portions of a sentence are relevant to each word. What is attention for Let s use an example I am sitting at the library with my friend. It should be pretty clear that not all words in this sentence are equally important. What words are relevant to I Probably sitting library and friend . Likewise the might only be relevant to library . Attention provides a way for a model to increase and decrease the importance of each word. Since the value of each token in the sequence is dependent on other tokens this method of generating word embeddings is very different from more classical methods like Word2Vec and GloVe. There is no one fixed vector for a given word. And this makes sense. Many words in English are homonyms and have identical spellings for distinct meanings. For example rock is a genre of music but also can mean a stone. The word run has 645 meanings and has recently replaced set as the word with the most definitions. It would not make sense for all of these homonyms to have the same vector. An interactive example You can hover over each word to see the relative importances of each word in the sentence to the hovered word. How does self attention work The Vaswani paper describes scaled dot product attention which involves normalizing by the square root of the input dimension. This is the part where Vaswani delves into a database analogy with keys queries and values. Most online resources try to salvage this analogy. Personally I always found this a bit confusing. What you need to know is that keys values and queries correspond to 3 matrices Mk Mq and Mv which are used in a dot product with the original input vectors. In linear algebra terms this means multiplying the 1xd input vector by a matrix of size dxd. In neural network terms this means passing the input vector through a full connected layer. After Mk and Mq are multiplied they are normalized by the square root of dk a constant representing the dimension of the input vector. Can we attend to multiple parts of a sentence Multi headed attention means performing attention n times in parallel inside of an encoder block. Yes that is called multi headed attention. Its architecture is very similar using additional Mk Mq and Mv matrices for each additional attention head . In the Vaswani paper they used 8 heads. How do transformers compare with other architectures e.g. RNN CNN When the input sequence length n is lower than the input dimensionality d self attention is faster than recurrent neural networks. Self attention is also easily parallelizable. Generally speaking RNNs are able to memorize but not parallelize and CNNs are able to parallelize but not memorize. Transformers are able to do both. The Vaswani paper outlines three main benefits Computational complexity per layer. Self attention layers are faster than recurrent layers when the input sequence length is smaller than the input vector dimensionality. The opportunity to parallelize calculations. Each head in multi headed attention can be computed separately in an encoder layer. Easier to learn long range dependencies. For many English sentences especially fairly complex ones found in more scientific writings the full context of a word cannot be learned from its immediate neighbors. Sometimes it can t even be found in the same sentence. However even though most language models prior to the transformer had theoretically infinite input sequence lengths in practice it was quite difficult for them to learn long range dependencies. Because a transformer sees its whole input simultaneously Vaswani argues it is able to more easily learn those dependencies. Is that all No. In part two I will describe the encoder and decoder blocks as well as the self supervised training process.", "tags": "transformer attention", "time": "2022-11-16", "name": "self-attention"}, "chess-blunders": {"title": "What is a blunder in chess?", "text": "What is a blunder in chess What is a blunder in chess The tension between the qualitative and quantitative answers to this question is at the heart of different approaches towards chess and more broadly how quantitative metrics may lack context but qualitative metrics lack precision. Qualitative answer There are many qualitative answers to this question especially when comparing blunders and mistakes a move that negatively affects their position in a significant way chess.com severely worsens the player s situation by allowing a loss of material checkmate or anything similar Wikipedia Blunders tend to be immediately refutable while mistakes require planning to capitalize on. r chess An issue with these qualitative answers is that while their words may be correct smart people may still disagree with their applicability at the margins. For a suboptimal move to have a significant negative effect it requires that the opponent notices and takes advantage of it. Quantitative answer The quantitative answer considers a move which causes a significant drop in probability of winning to be blunder. What is significant A change 14 or greater. How is winning probability in a chess game calculated Objectively since there are only three possible outcomes in a game win draw loss by definition any real advantage will lead to a win with perfect play. But objectively humans aren t perfect. Even grandmasters can let an advantage slip. If Magnus Carlsen doesn t capitalize on your blunder was it really a blunder From a machine learning perspective we can view winning probability as a logistic regression problem where centipawn evaluation is a feature and game outcome is a label. If we further limit data points to 2300 rated games this is what Lichess uses. Of course this isn t perfect and there s an argument to be made that outcomes for 2300 elo players may not be representative of lower rated players. It also doesn t take time pressure into consideration. But there is a tradeoff between accuracy of our metric and generalization power of the model. One other important objection to this line of inquery is that the centipawn evaluation of a position is not a constant. The evaluation varies by search depth and between engines. So if Stockfish 15 evaluates a position at 1 and Stockfish 16 evaluates the same position at 1.5 white s actual winning chances haven t changed at all. The evaluation is not anchored to any real value especially with the introduction of NNUE eval functions. References https lichess.org page accuracy", "tags": "chess evaluation statistics explainability", "time": "2023-09-25", "name": "chess-blunders"}, "learning-the-haystack": {"title": "Learning the Haystack", "text": "Learning the Haystack Embeddings or vector representations of a document which could be a piece of text image sound etc. can be extremely useful for making sense of large datasets. They transform information into a vector space such that their distance corresponds to their similarity. Enterprising readers might be asking themselves how to get these vectors also known as embeddings in the first place. One way is to simply pay for them. This isn t ideal for a couple of reasons It can be expensive. You ll need to pay once to embed each document and separately for each query. If you have a large corpus it can be cost prohibitive. At the time of writing OpenAI charges about 130 per million tokens around 1000 paragraphs for their largest model. Similarity may mean different things depending on your use case. For example suppose we are retrieving documents for customer support. For the embedding model to learn that two documents share user behavior characteristics i.e. two documents were opened by support agents in the same session that information needs to be available in the training process. Below is an overview of three of the main training regimes I have used for creating embeddings. For more information and in depth examples I highly recommend the loss overview page of the Sentence Transformers library. Generally speaking there are three categories of methods unsupervised methods contrastive learning methods positive negative labels and regression methods floating point labels . Autoencoders An autoencoder uses a bottleneck to reduce the dimensionality of the input. One approach for vectorizing a document image or other blob of information is to simply use an autoencoder. An autoencoder is a function which learns a lossy compression function. It can be considered an unsupervised method since each item is its own label. However although they are conceptually pretty simple autoencoders aren t always best. They may learn an efficient representation of your document but it may be wasting a lot of space on things you don t care about. For example if it was reproducing a picture which contained a TV showing static you may not care exactly what the static looks like just that it has static. So it s probably a waste of space to try to reproduce every pixel of static. Likewise if it were encoding people it might be really important to you that the people have 5 fingers. Unfortunately the autoencoder wasted too much space encoding the TV and not on boring details like numbers of fingers. See also the noisy TV problem Fine tuning with Sentence Transformers The applicability of the next few methods will depend on the format of your data and labels. Here is a simple example for creating text embeddings with sentence transformers using ContrastiveLoss from sentence transformers import SentenceTransformer SentenceTransformerTrainer from sentence transformers.losses import ContrastiveLoss from datasets import Dataset model SentenceTransformer all MiniLM L6 v2 train dataset Dataset.from dict sentence1 I feel the need... Life is like a box of chocolates. sentence2 ...the need for speed similar Here s Johnny dissimilar label 1 0 trainer SentenceTransformerTrainer model model train dataset train dataset loss ContrastiveLoss model trainer.train And to generate the embedding embedding model.encode There s no crying in baseball print embedding.shape 1 384 Contrastive Learning Triplet loss Triplet loss explicitly learns embeddings to be used in cosine similarity or euclidean distance L2 norm comparison. It uses triplets of the form A P N where A is an anchor P is a positive example which is similar to A and N is a negative example which is dissimilar to A. For example if we wanted to learn embeddings for songs we might say a positive example is a spectrogram clip from the same artist as the anchor and a negative sample is from a different artist. text L sum i 1 N text max left 0 cos f a i f p i cos f a i f n i m right for cosine similarity or for euclidean distiance text L sum i 1 N text max left 0 lVert f a i f p i rVert 2 lVert f a i f n i rVert 2 m right In other words we enforce that the distance between the anchor and the negative sample is greater than the distance between the anchor and the positive sample plus some margin. For cosine similarity what this would look like is that your vectors appear as points on the perimeter of a high dimensional clock face i.e. the surface of an N dimensional hypersphere . Positive samples grow closer to the anchor while the negative samples are pushed away. The higher the margin the closer positive samples will be pushed together. Learning to differentiate Bangarang and Clair de lune. Triplet loss pushes positive examples closer to the anchor and negatives farther away. Some say Claude DeBussy was the Skrillex of 1890. Contrastive learning Contrastive loss If you have binary labels you can use contrastive loss. It works with positive pairs anchor positive and negative pairs anchor negative denoted with a binary label. In the original paper y in 1 0 is the label indicating the desired distance between the points not whether they are similar . Like triplet loss contrastive loss also includes a margin parameter. For dissimilar pairs a loss is incurred only if their predicted distance falls within the margin s radius. text loss frac 1 y i D W i 2 y i left text max 0 m D W i right 2 2 Where D W i is euclidean distance between the two embeddings of sentences a i and b i D W i lVert f a i f b i rVert 2 Note that triplet loss is often confused with contrastive loss but they are not the same. Regression Cosine similarity loss If you already have some numerical labels for the similarity of two pieces of text you can compute mean squared error between their predicted cosine similarity and the actual similarity. For pairs of sentences ai and bi and label yi text loss frac 1 N sum i 1 N left cos left f a i f b i right y i right 2 Regression CoSENT loss CoSENT loss is an improved version of cosine similarity learning which makes better use of in batch values. For a batch with size N it makes up to Nx N 1 2 predictions. I.e. for a batch size of 10 we make up to 45 predictions rather than the 10 in Cosine Similarity Loss. It is essentially a ranking loss taking advantage of the knowledge of the relative similarities within the batch. text loss log left 1 sum text sim i j text sim k l e lambda cos k l cos i j right Experiments have shown improved performance over Cosine Similarity loss.", "tags": "vector loss", "time": "2024-03-27", "name": "learning-the-haystack"}, "game-of-life-3d": {"title": "A 3D Game of Life", "text": "A 3D Game of Life Conway s Game of Life is a simulation developed in 1970 describing a grid of binary cells and transition rules for each cell which depend on the state of the cell s neighbors. It s capable of creating some pretty cool patterns. This variant of the Game of Life uses three overlapping channels so instead of just one simulation there are three simultaneous simulations. I visualize these in the three color channels red green and blue. Two or more channels active on the same cell are represented with additive color mixing. These additional channels enable additional interaction between channels which is pretty neat. I ve added a rule which revives a dead cell if the sum of its active neighbors from other channels is 9. One nice result of this inter channel interaction is that stagnant colors can be reactivated by other colors that pass by. Finding parameters that don t cause the simulation to blow up or die out is a bit of a trial and error. I made an empirical choice which results in a lively pond after the chaotic initial state. I m sure there are other nice combinations as well. Other variants I experimented with briefly were to include more than three channels and to consider more complex relationships between the channels. However these ran slower and or were harder to balance between chaotic and desolate tendencies. start game pause game", "tags": "simulation javascript", "time": "2023-08-23", "name": "game-of-life-3d"}, "suboptimal-minimax": {"title": "When Suboptimal Minimax is Better", "text": "When Suboptimal Minimax is Better Minimax solves for optimal opponent play minimizing the best move an opponent could make. But what if we knew the opponent wouldn t make the best move What if we knew what the opponent would do ahead of time In that case we could beat them faster by playing moves which take advantage of this fact even if our move isn t objectively the best move. Don t play the game play the man. For the purposes of this problem let s assume it is a chess algorithm but this concept can be generalized to any algorithm which can use minimax. Anyone who has played a game against another person understands the idea of downloading your opponent playing against a person enough that you feel like you know what they will do. Hikaru Nakamura does. It seems like this type of player would have many possible applications including training for specific skill levels and even trainers personalized to an individual s playing style. Prior work There has been previous work in terms of opponent modeling in chess. Maia chess has been trained to model several Elo levels of opponent play which makes it ideal to be used as a backbone in my experiments. The Policy Function We can think of the policy function as a model of the opponent. We will therefore be optimizing an algorithm to play against this model rather than the optimal opponent. The specifics of the policy function are as follows. The function takes two parameters a board position B and an opponent skill level L. The parameter L is important because the nature of the opponent s suboptimal play is important. All levels of chess players play sub optimally but the types of mistakes a beginner or intermediate level player will make are different from a grandmaster. The algorithm should exploit this fact. In practice this algorithm should play in a much more trappy way than the methodical slow grind of a typical chess engine. Formally at move i there will be n i possible moves. For each of the n i moves we should calculate an evaluation weighted by the opponent s probability of playing responses to that move e i m i p L M i 1 cdot r M i 1 Where M i 1 is the set of possible moves the opponent can make in response to move m i p L is the probability distribution of each move in M i being made and r is some real evaluation of the board position. Let s use an example. Suppose that at some stage in the game the algorithm must choose between 2 possible moves. For simplicity the opponent will have 4 possible replies after each of the two possible moves. After the algorithm makes the the first possible move the probabilities look like this Opponent move distribution at position 1 Opponent Move Probability of play Resulting position evaluation 1 0.6 5 2 0.3 1 3 0.1 0.5 4 0.1 1 begin bmatrix 0.6 0.3 0.1 0.1 end bmatrix cdot begin bmatrix 5 1 0.5 1 end bmatrix 3.25 Opponent move distribution at position 2 And after the second possible move the opponent move distribution looks like this Opponent Move Probability of play Resulting position evaluation 1 0.9 4 2 0.05 0.5 3 0.03 0 4 0.02 2 begin bmatrix 0.9 0.05 0.03 0.02 end bmatrix cdot begin bmatrix 4 0.5 0 2 end bmatrix 3.585 In these tables positions are evaluated from the algorithm s point of view so higher evaluation better. Assume the evaluation function is a standard evaluation from a chess engine e.g. Stockfish. In this example it is clear that the algorithm should choose move 2 because it is highly likely that after making this move the algorithm will have a decisive advantage. Checkmates All of this works quite well in the middle game where position evaluations are all within a reasonable range. But what happens when one of the opponent s moves leads to checkmate How much value should we give to the possibility that an opponent might blunder into checkmate The above calculation only works for finite numbers. The evaluation of a checkmate is infinity. Instead let s choose a large finite number for checkmate. The behavior of the algorithm depends heavily on our choice for this checkmate evaluation the CHECKMATE WEIGHT. If the number is too high the algorithm will favor positions where the opponent might get checkmated playing risky moves. If the number is too low the algorithm will choose other branches which win less quickly. After a bit of trial and error I settled on 10 for checkmate. This is enough for the algorithm to seek out these types of positions. A further question is what to do once checkmate is inevitable. That is what should the algorithm do once it will definitely win One option is to continue to sample from the distribution continuing to play the move which maximizes winning chances. At the beginning this was my intuition. Unfortunately this leads to many draws by repetition. Instead once the game is in a forced win situation mate in N moves the algorithm will stop sampling from the distribution and simply play the moves which lead to checkmate. While this will inevitably lead to a win it changes the character of the algorithm somewhat. Therefore I have limited the lookahead to checkmates in the immediate next few moves specifically mate in 5 or fewer moves . Since the original goal was to create an algorithm which is used for training this seems reasonable. Games I simulated 400 matches testing this opponent modeling approach which I wrote about here.", "tags": "chess ai game theory", "time": "2022-07-02", "name": "suboptimal-minimax"}, "evolutionary-antenna-design": {"title": "Paper Summary: Antenna Design with Evolutionary Algorithms", "text": "Paper Summary Antenna Design with Evolutionary Algorithms This is a summary of Automated Antenna Design with Evolutionary Algorithms a 2006 paper by Hornby et al. As large language models become more and more synonymous with AI it is interesting to see how researchers solved problems in the past. Typically antennas are designed and built by hand by domain experts. This is a very time consuming process however so researchers have been investigating evolutionary algorithms since the 1990s. Inspired by natural evolution an evolutionary algorithm is based on small random changes and an evaluation metric. In this paper the authors describe the use of an evolutionary algorithm to design an antenna for a small satellite weighing only 25 kilograms called ST5. The researchers then describe the technical specifications which the antenna would need to satisfy. The antenna needed to weigh under 165 grams and have a height and diameter of around 15 centimeters. In representing the antenna the researchers used a tree structure with each node capable of representing one of several operations forward rotate x rotate y and rotate z. The forward operation adds a length of wire with a given radius. The rotate x operation rotates the current state about the x axis. The rotate y and rotate z operations do the same for the y and z axes. The researchers used a fitness function which was the product of VSWR a gain error term and a gain smoothness term. The gain error term is similar to the least squares error function. The gain smoothness term describes how uniform the gain pattern was since the satellite would be spinning. All three of these factors were multiplied together to calculate an overall fitness score against which all candidate antennas were measured. Due to a change in the orbit of the satellite the specifications for the antenna were updated and the researchers needed to modify their evolutionary algorithm. This was completed within one month. The evolved antenna consumed less power took less time to build was less complex and performed better than traditionally designed antennas. Since the satellite had 2 antennas the researchers measured the combined performance. The evolved antennas were 93 efficient while the designed antennas were only 38 efficient. Additionally the evolved antennas were much faster to design and fabricate. The researchers then designed another antenna for NASA s TDRS C satellite. This design used an evolutionary algorithm in combination with a stochastic hill climbing algorithm. They used a loss function which attended to the standing wave ratio and gain at several frequencies relevant to the satellite. In performing the evolutionary algorithm 150 algorithm processes were run for 50 000 iterations after randomizing many parameters. After this first stage the best antenna from the previous 150 was optimized using stochastic hill climbing with random mutations. From this second stage the 23 best antennas were selected and run through another stochastic hill climbing step for 100 000 iterations. Of the 23 finalists one of the evolved antennas exceeded the project specifications and was optimized further using more accurate software.", "tags": "ai paper summary", "time": "2023-04-17", "name": "evolutionary-antenna-design"}, "what-are-attention-masks": {"title": "What Are Attention Masks?", "text": "What Are Attention Masks TLDR Attention masks allow us to send a batch into the transformer even when the examples in the batch have varying lengths. We do this by padding all sequences to the same length then using the attention mask tensor to identify which tokens are padding. Here we use a batch with three samples padded from the left since we want to predict the next token on the right. Padding on the right would probably predict another pad. If you want to perform inference with transformers one sequence at a time you can ignore attention masks. The slow way will be sufficient for your needs. The slow way We can perform inference with GPT 2 using sequences one at a time but it s slow from transformers import GPT2LMHeadModel GPT2Tokenizer tokenizer GPT2Tokenizer.from pretrained gpt2 gpt2 GPT2LMHeadModel.from pretrained gpt2 context tokenizer It will rain in the return tensors pt prediction gpt2.generate context max length 10 tokenizer.decode prediction 0 prints It will rain in the morning and the rain It s way faster to batch the inputs which means adding their token vectors to the context and performing inference only once. The un slow way The cool way to perform inference on many samples is with batching. It s much faster but it s also slightly more complicated. tokenizer.padding side left tokenizer.pad token tokenizer.eos token sentences It will rain in the I want to eat a big bowl of My dog is inputs tokenizer sentences return tensors pt padding True output sequences gpt2.generate inputs for seq in output sequences print tokenizer.decode seq What s happening here And what does this have to do with attention masks First let s explain padding then take a look at the code line by line. We feed tokens into transformer based language models like GPT 2 and BERT for inference as tensors. A tensor is like a python list but with a few extra features and restrictions. Specifically for a tensor of dimension 2 all vectors in that dimension need to be the same length. For example from torch import tensor tensor 1 2 3 4 ok tensor 1 2 3 error When we tokenize an input it it will be turned into a tensor containing sequence of integers each corresponding to an item in the transformer s vocabulary. Here is an example tokenization in GPT 2 String Token ID It 1026 will 481 rain 6290 in 287 the 262 Suppose we wanted to include a second sequence in our input String Token ID My 3666 dog 3290 is 318 Because these two sequences have different lengths we can t just combine them in one tensor. Instead we have to pad the shorter sequences with dummy tokens so that each sequence is the same length. And because we want the model to continue to add to the right side of our sequence we will pad the left side of shorter sequences. String Token ID pad 50256 My 3666 dog 3290 is 318 This is where the attention mask comes in. The attention mask simply shows the transformer which tokens are padding placing 0s in the positions of padding tokens and 1s in the positions of actual tokens. Now that we understand that let s look at the code line by line. tokenizer.padding side left This line tells the tokenizer to begin padding from the left default is right because the logits of the rightmost token will be used to predict future tokens. tokenizer.pad token tokenizer.eos token This line specifies which token we will use for padding. It doesn t matter which one you choose but here we re choosing the end of sequence token. sentences It will rain in the I want to eat a big bowl of My dog is These three sequences all have different lengths when tokenized so should be a good test of our padding method. inputs tokenizer sentences return tensors pt padding True Now we tokenize. We re passing in the sentences from above telling the tokenizer to use PyTorch tensors rather than Tensorflow and telling the tokenizer to add padding for us. We can print inputs here to confirm that yes tokenization is working as we thought input ids tensor 50256 50256 50256 1026 481 6290 287 262 40 765 284 4483 257 1263 9396 286 50256 50256 50256 50256 50256 3666 3290 318 attention mask tensor 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 As you can see the first and third sequence include padding at the beginning and the attention mask parameter marks the position of this padding. Now let s actually pass this input into the model to generate new text output sequences gpt2.generate inputs If you re unfamiliar with kwargs syntax for function calls this passes in the inputs dict as named parameters using the keys as the parameter names and the values as the corresponding argument values. Check the docs for more info. Finally we just need to loop through each of the generated sequences and print out the result in human readable form using the decode function to convert token IDs to strings. for seq in output sequences print tokenizer.decode seq", "tags": "nlp python transformers gpt2 bert pytorch huggingface", "time": "2021-06-15", "name": "what-are-attention-masks"}}